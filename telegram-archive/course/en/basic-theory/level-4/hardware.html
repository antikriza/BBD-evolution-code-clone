<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Hardware Basics - Basic Theory</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background:#0a0a0f; color:#e4e6eb; }
a { color:#6ab2f2; text-decoration:none; }
a:hover { text-decoration:underline; }
.header { background:#111119; padding:16px 24px; border-bottom:1px solid #1e1e2e; display:flex; align-items:center; gap:16px; position:sticky; top:0; z-index:10; }
.header h1 { font-size:18px; color:#6ab2f2; }
.header .back { color:#8696a4; font-size:14px; }
.header .lang-switch { margin-left:auto; font-size:13px; color:#8696a4; }
.container { max-width:900px; margin:0 auto; padding:24px; }
.breadcrumb { color:#8696a4; font-size:13px; margin-bottom:20px; }
.breadcrumb a { color:#6ab2f2; }
.level-badge { display:inline-block; padding:4px 12px; border-radius:20px; font-size:12px; font-weight:600; margin-bottom:16px; }
.level-1 { background:rgba(74,222,128,0.15); color:#4ade80; }
.level-2 { background:rgba(96,165,250,0.15); color:#60a5fa; }
.level-3 { background:rgba(245,158,11,0.15); color:#f59e0b; }
.level-4 { background:rgba(239,68,68,0.15); color:#ef4444; }
.level-5 { background:rgba(168,85,247,0.15); color:#a855f7; }
h2 { font-size:28px; margin-bottom:8px; }
.desc { color:#8696a4; font-size:15px; line-height:1.7; margin-bottom:24px; }
.detail-list { list-style:none; padding:0; }
.detail-list li { padding:10px 16px; margin-bottom:6px; border-radius:8px; background:#111119; border-left:3px solid #2b5278; font-size:14px; line-height:1.5; color:#e4e6eb; }
.detail-list li::before { content:'\2192 '; color:#6ab2f2; }
.section-title { font-size:16px; color:#8696a4; margin:24px 0 12px; text-transform:uppercase; letter-spacing:1px; }
.related-topics { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
.related-topics a { display:inline-block; background:#111119; border:1px solid #1e1e2e; padding:6px 14px; border-radius:8px; font-size:13px; color:#6ab2f2; }
.related-topics a:hover { background:#1e1e2e; text-decoration:none; }
.nav-links { display:flex; justify-content:space-between; margin-top:30px; padding-top:20px; border-top:1px solid #1e1e2e; }
.nav-links a { color:#6ab2f2; font-size:14px; }
.nav-links .disabled { color:#333; }
.topic-grid { display:grid; grid-template-columns:repeat(auto-fill, minmax(260px, 1fr)); gap:12px; margin-top:16px; }
.topic-card { background:#111119; border-radius:12px; padding:18px; border-left:3px solid #2b5278; display:block; transition:background 0.15s; }
.topic-card:hover { background:#1a1a2e; text-decoration:none; }
.topic-card h3 { color:#e4e6eb; font-size:15px; margin-bottom:4px; }
.topic-card .sub { color:#8696a4; font-size:13px; }
.footer { text-align:center; padding:30px; color:#8696a4; font-size:12px; margin-top:30px; border-top:1px solid #1e1e2e; }
.overview { margin-bottom:28px; }
.overview p { color:#c4c6cb; font-size:15px; line-height:1.8; margin-bottom:14px; }
.terms-grid { display:grid; gap:10px; margin-top:12px; }
.term-card { background:#111119; border:1px solid #1e1e2e; border-radius:10px; padding:14px 18px; }
.term-card strong { color:#6ab2f2; font-size:14px; }
.term-card span { color:#c4c6cb; font-size:13px; display:block; margin-top:4px; line-height:1.5; }
.tip-box { background:linear-gradient(135deg, rgba(106,178,242,0.06), rgba(106,178,242,0.02)); border:1px solid rgba(106,178,242,0.15); border-radius:10px; padding:16px 20px; margin-top:12px; }
.tip-box li { color:#c4c6cb; font-size:14px; line-height:1.6; margin-bottom:8px; list-style:none; }
.tip-box li::before { content:'üí° '; }
.detail-card { background:#111119; border-radius:10px; padding:16px 18px; margin-bottom:8px; border-left:3px solid #2b5278; }
.detail-card .dt { color:#e4e6eb; font-size:14px; font-weight:600; margin-bottom:4px; }
.detail-card .dd { color:#9ca3af; font-size:13px; line-height:1.6; }
.detail-card .dl { margin-top:6px; }
.detail-card .dl a { font-size:12px; color:#6ab2f2; background:rgba(106,178,242,0.08); padding:3px 10px; border-radius:6px; display:inline-block; margin-right:6px; margin-top:4px; }
.detail-card .dl a:hover { background:rgba(106,178,242,0.15); text-decoration:none; }
</style>
</head>
<body>
<div class="header">
  <a href="../../index.html" class="back">&larr; Back to Course</a>
  <h1>Basic Theory</h1>
  <a href="../../../uk/basic-theory/level-4/hardware.html" class="lang-switch">üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</a>
</div>
<div class="container">
  <div class="breadcrumb"><a href="../../index.html">Course</a> / <a href="../index.html">Basic Theory</a> / <a href="index.html">Level 4</a> / Hardware Basics</div>
  <span class="level-badge level-4">üöÄ Level 4 ‚Äî Master</span>
  <h2>Hardware Basics</h2>
  <p class="desc">Hardware requirements for running AI models locally.</p>

  <div class="overview">
    <p>Running AI models locally requires understanding the hardware constraints. The key bottleneck is memory ‚Äî specifically GPU VRAM for fast inference. A 7B parameter model needs about 4-6GB VRAM (quantized), while a 70B model needs 35-48GB. Your hardware determines which models you can run and how fast they generate tokens.</p>
    <p>The hardware landscape has democratized significantly. Apple Silicon Macs with unified memory can run surprisingly large models. Consumer NVIDIA GPUs (RTX 4090 with 24GB VRAM) handle 13B-34B models well. For larger models, cloud GPU providers offer pay-per-hour access. Understanding these options helps you choose the right balance of cost, speed, and capability.</p>
  </div>
  <div class="section-title">Key Topics Covered</div>
  <div class="detail-cards">
    <div class="detail-card"><div class="dt">GPU vs CPU Inference</div><div class="dd">GPUs are 10-50x faster than CPUs for AI inference due to massive parallelism. CPUs work for small models or when using GGUF with CPU offloading. For any serious local AI work, a GPU is essential.</div></div>
    <div class="detail-card"><div class="dt">VRAM Requirements</div><div class="dd">7B model: ~4GB (Q4), ~8GB (FP16). 13B: ~8GB (Q4). 34B: ~20GB (Q4). 70B: ~40GB (Q4). Rule of thumb: model size in GB at Q4 is roughly half the parameter count in billions.</div><div class="dl"><a href="model-formats.html">Model Formats</a></div></div>
    <div class="detail-card"><div class="dt">NVIDIA Consumer GPUs</div><div class="dd">RTX 4090 (24GB, $1600) ‚Äî king of local LLMs. RTX 4080 (16GB). RTX 3090 (24GB, used ~$800) ‚Äî best value. RTX 4060 Ti 16GB ‚Äî budget option. VRAM matters more than compute speed for LLMs.</div></div>
    <div class="detail-card"><div class="dt">Apple Silicon</div><div class="dd">M1/M2/M3/M4 Macs with unified memory can run large models. M2 Ultra (192GB) can run 70B+ models. M3 Max (128GB) handles 34B well. Slower than NVIDIA but memory bandwidth is excellent.</div></div>
    <div class="detail-card"><div class="dt">Data Center GPUs</div><div class="dd">A100 (80GB), H100 (80GB), H200 (141GB) ‚Äî the hardware powering AI labs. 10-20x more expensive than consumer GPUs. Available through cloud providers for hourly rental.</div></div>
    <div class="detail-card"><div class="dt">Cloud GPU Providers</div><div class="dd">RunPod, Vast.ai, Lambda Labs ‚Äî rent GPUs by the hour ($0.50-$4/hr for A100). Good for occasional use or running models too large for local hardware. No upfront investment.</div><div class="dl"><a href="api-providers.html">API Providers</a></div></div>
    <div class="detail-card"><div class="dt">Multi-GPU Setups</div><div class="dd">Split large models across multiple GPUs. NVLink provides fast GPU-to-GPU communication on matching NVIDIA cards. Consumer GPUs can use PCIe with slower but functional model sharding.</div></div>
    <div class="detail-card"><div class="dt">Inference Engines</div><div class="dd">llama.cpp (CPU+GPU, versatile), vLLM (high-throughput GPU serving), Ollama (easy local setup), LM Studio (GUI), TGI (HuggingFace serving). Each optimizes for different use cases.</div><div class="dl"><a href="model-formats.html">Model Formats</a></div></div>
    <div class="detail-card"><div class="dt">RAM and Storage</div><div class="dd">System RAM matters for CPU inference and model loading. 32GB minimum, 64GB+ recommended. NVMe SSD dramatically speeds up model loading times (30-70B models are 20-40GB files).</div></div>
    <div class="detail-card"><div class="dt">Budget Configurations</div><div class="dd">Entry ($500): used RTX 3060 12GB ‚Äî runs 7B models. Mid ($1500): RTX 4090 24GB ‚Äî runs up to 34B. High ($3000+): Mac Studio M2 Ultra or dual GPU. Budget: use cloud APIs instead of local hardware.</div></div>
  </div>

  <div class="section-title">Key Terms</div>
  <div class="terms-grid">
    <div class="term-card"><strong>VRAM</strong><span>Video RAM on the GPU ‚Äî the primary constraint for which AI models can run locally.</span></div>
    <div class="term-card"><strong>Unified Memory</strong><span>Apple Silicon architecture where CPU and GPU share the same memory pool, enabling larger models on Mac.</span></div>
    <div class="term-card"><strong>Model Sharding</strong><span>Splitting a model across multiple GPUs when it is too large to fit in a single GPU's VRAM.</span></div>
    <div class="term-card"><strong>Inference Engine</strong><span>Software that loads and runs AI models ‚Äî llama.cpp, vLLM, Ollama, TensorRT-LLM each optimize for different scenarios.</span></div>
  </div>

  <div class="section-title">Practical Tips</div>
  <div class="tip-box"><ul>
    <li>VRAM is the most important spec ‚Äî an RTX 3090 with 24GB VRAM often beats an RTX 4080 with 16GB for LLM work</li>
    <li>Start with Ollama or LM Studio for the easiest local setup experience before diving into raw llama.cpp</li>
    <li>If you only need occasional access to large models, cloud GPU rental ($1-2/hour) is cheaper than buying hardware</li>
  </ul></div>

    <div class="section-title">Related Community Discussions</div>
    <div class="related-topics">
      <a href="#">Models</a>
    </div>

  <div class="nav-links">
    <a href="ai-protocols.html">&larr; AI Protocols</a>
    <a href="api-providers.html">API Providers &rarr;</a>
  </div>
</div>
<div class="footer"><a href="../../index.html">&larr; Back to Course</a></div>
</body>
</html>