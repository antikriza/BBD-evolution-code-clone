<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Model Formats - Basic Theory</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background:#0a0a0f; color:#e4e6eb; }
a { color:#6ab2f2; text-decoration:none; }
a:hover { text-decoration:underline; }
.header { background:#111119; padding:16px 24px; border-bottom:1px solid #1e1e2e; display:flex; align-items:center; gap:16px; position:sticky; top:0; z-index:10; }
.header h1 { font-size:18px; color:#6ab2f2; }
.header .back { color:#8696a4; font-size:14px; }
.header .lang-switch { margin-left:auto; font-size:13px; color:#8696a4; }
.container { max-width:900px; margin:0 auto; padding:24px; }
.breadcrumb { color:#8696a4; font-size:13px; margin-bottom:20px; }
.breadcrumb a { color:#6ab2f2; }
.level-badge { display:inline-block; padding:4px 12px; border-radius:20px; font-size:12px; font-weight:600; margin-bottom:16px; }
.level-1 { background:rgba(74,222,128,0.15); color:#4ade80; }
.level-2 { background:rgba(96,165,250,0.15); color:#60a5fa; }
.level-3 { background:rgba(245,158,11,0.15); color:#f59e0b; }
.level-4 { background:rgba(239,68,68,0.15); color:#ef4444; }
.level-5 { background:rgba(168,85,247,0.15); color:#a855f7; }
h2 { font-size:28px; margin-bottom:8px; }
.desc { color:#8696a4; font-size:15px; line-height:1.7; margin-bottom:24px; }
.detail-list { list-style:none; padding:0; }
.detail-list li { padding:10px 16px; margin-bottom:6px; border-radius:8px; background:#111119; border-left:3px solid #2b5278; font-size:14px; line-height:1.5; color:#e4e6eb; }
.detail-list li::before { content:'\2192 '; color:#6ab2f2; }
.section-title { font-size:16px; color:#8696a4; margin:24px 0 12px; text-transform:uppercase; letter-spacing:1px; }
.related-topics { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
.related-topics a { display:inline-block; background:#111119; border:1px solid #1e1e2e; padding:6px 14px; border-radius:8px; font-size:13px; color:#6ab2f2; }
.related-topics a:hover { background:#1e1e2e; text-decoration:none; }
.nav-links { display:flex; justify-content:space-between; margin-top:30px; padding-top:20px; border-top:1px solid #1e1e2e; }
.nav-links a { color:#6ab2f2; font-size:14px; }
.nav-links .disabled { color:#333; }
.topic-grid { display:grid; grid-template-columns:repeat(auto-fill, minmax(260px, 1fr)); gap:12px; margin-top:16px; }
.topic-card { background:#111119; border-radius:12px; padding:18px; border-left:3px solid #2b5278; display:block; transition:background 0.15s; }
.topic-card:hover { background:#1a1a2e; text-decoration:none; }
.topic-card h3 { color:#e4e6eb; font-size:15px; margin-bottom:4px; }
.topic-card .sub { color:#8696a4; font-size:13px; }
.footer { text-align:center; padding:30px; color:#8696a4; font-size:12px; margin-top:30px; border-top:1px solid #1e1e2e; }
.overview { margin-bottom:28px; }
.overview p { color:#c4c6cb; font-size:15px; line-height:1.8; margin-bottom:14px; }
.terms-grid { display:grid; gap:10px; margin-top:12px; }
.term-card { background:#111119; border:1px solid #1e1e2e; border-radius:10px; padding:14px 18px; }
.term-card strong { color:#6ab2f2; font-size:14px; }
.term-card span { color:#c4c6cb; font-size:13px; display:block; margin-top:4px; line-height:1.5; }
.tip-box { background:linear-gradient(135deg, rgba(106,178,242,0.06), rgba(106,178,242,0.02)); border:1px solid rgba(106,178,242,0.15); border-radius:10px; padding:16px 20px; margin-top:12px; }
.tip-box li { color:#c4c6cb; font-size:14px; line-height:1.6; margin-bottom:8px; list-style:none; }
.tip-box li::before { content:'üí° '; }
.detail-card { background:#111119; border-radius:10px; padding:16px 18px; margin-bottom:8px; border-left:3px solid #2b5278; }
.detail-card .dt { color:#e4e6eb; font-size:14px; font-weight:600; margin-bottom:4px; }
.detail-card .dd { color:#9ca3af; font-size:13px; line-height:1.6; }
.detail-card .dl { margin-top:6px; }
.detail-card .dl a { font-size:12px; color:#6ab2f2; background:rgba(106,178,242,0.08); padding:3px 10px; border-radius:6px; display:inline-block; margin-right:6px; margin-top:4px; }
.detail-card .dl a:hover { background:rgba(106,178,242,0.15); text-decoration:none; }
</style>
</head>
<body>
<div class="header">
  <a href="../../index.html" class="back">&larr; Back to Course</a>
  <h1>Basic Theory</h1>
  <a href="../../../uk/basic-theory/level-4/model-formats.html" class="lang-switch">üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</a>
</div>
<div class="container">
  <div class="breadcrumb"><a href="../../index.html">Course</a> / <a href="../index.html">Basic Theory</a> / <a href="index.html">Level 4</a> / Model Formats</div>
  <span class="level-badge level-4">üöÄ Level 4 ‚Äî Master</span>
  <h2>Model Formats</h2>
  <p class="desc">Understanding different model distribution and execution formats.</p>

  <div class="overview">
    <p>AI models need to be serialized into files for distribution and loading. Different formats optimize for different goals: GGUF prioritizes running on consumer hardware with CPU+GPU splitting, GPTQ and AWQ are GPU-optimized for maximum throughput, SafeTensors ensures safe loading without code execution risks, and ONNX provides cross-platform compatibility.</p>
    <p>Understanding model formats is essential for local AI deployment. The format you choose determines which inference engine you can use (llama.cpp, vLLM, TensorRT), what hardware it runs on, how much memory it needs, and how fast it generates tokens. Most models on Hugging Face are available in multiple formats.</p>
  </div>
  <div class="section-title">Key Topics Covered</div>
  <div class="detail-cards">
    <div class="detail-card"><div class="dt">GGUF (llama.cpp)</div><div class="dd">The most versatile format for local inference. Supports CPU, GPU, and mixed CPU+GPU execution. Single-file distribution with embedded metadata. The go-to format for running models on consumer hardware.</div><div class="dl"><a href="hardware.html">Hardware Basics</a></div></div>
    <div class="detail-card"><div class="dt">GPTQ</div><div class="dd">GPU-optimized post-training quantization format. Models are quantized to 4-bit or 8-bit with calibration data. Faster than GGUF on pure GPU but requires the full model to fit in VRAM.</div><div class="dl"><a href="../level-3/model-optimization.html">Model Optimization</a></div></div>
    <div class="detail-card"><div class="dt">AWQ (Activation-Aware Quantization)</div><div class="dd">Advanced GPU quantization that preserves important weights based on activation patterns. Generally better quality than GPTQ at the same bit width. Supported by vLLM and TensorRT-LLM.</div></div>
    <div class="detail-card"><div class="dt">SafeTensors</div><div class="dd">Hugging Face safe serialization format that prevents arbitrary code execution on load, supports memory-mapping for fast loading, and is now the default format on Hugging Face Hub.</div></div>
    <div class="detail-card"><div class="dt">ONNX (Open Neural Network Exchange)</div><div class="dd">Cross-platform format supported by Microsoft, Google, and others. Enables running models on different hardware (CPU, GPU, NPU) through ONNX Runtime. Used for edge deployment and mobile inference.</div></div>
    <div class="detail-card"><div class="dt">ExLlamaV2 and EXL2</div><div class="dd">Highly optimized GPU inference with variable quantization ‚Äî different layers can use different bit widths. Achieves the best perplexity-per-bit among quantized formats. Popular for enthusiast setups.</div></div>
    <div class="detail-card"><div class="dt">TensorRT-LLM</div><div class="dd">NVIDIA high-performance inference engine. Compiles models into optimized execution plans for NVIDIA GPUs. Maximum throughput for production serving but requires NVIDIA hardware and compilation step.</div><div class="dl"><a href="hardware.html">Hardware Basics</a></div></div>
    <div class="detail-card"><div class="dt">Quantization Levels</div><div class="dd">Q2 (smallest, lowest quality) through Q8 (largest, highest quality). Q4_K_M is the sweet spot for GGUF ‚Äî good quality with reasonable size. Q5+ recommended for reasoning-heavy tasks.</div><div class="dl"><a href="../level-3/model-optimization.html">Model Optimization</a></div></div>
    <div class="detail-card"><div class="dt">Model Distribution</div><div class="dd">Hugging Face is the primary hub. Models are uploaded in multiple formats by quantization specialists (TheBloke, bartowski). Ollama and LM Studio download GGUF models with one-click setup.</div></div>
    <div class="detail-card"><div class="dt">Choosing the Right Format</div><div class="dd">Consumer GPU: GGUF or EXL2. Production GPU server: AWQ or TensorRT-LLM. CPU only: GGUF. Cross-platform: ONNX. Mobile/edge: ONNX or CoreML. When in doubt, start with GGUF Q4_K_M.</div></div>
  </div>

  <div class="section-title">Key Terms</div>
  <div class="terms-grid">
    <div class="term-card"><strong>GGUF</strong><span>llama.cpp model format supporting CPU+GPU inference ‚Äî the most popular format for running models on consumer hardware.</span></div>
    <div class="term-card"><strong>SafeTensors</strong><span>Secure model serialization format that prevents code execution attacks during model loading.</span></div>
    <div class="term-card"><strong>Quantization Level</strong><span>Bit precision of model weights (Q2-Q8) ‚Äî lower bits mean smaller files but reduced quality.</span></div>
    <div class="term-card"><strong>VRAM</strong><span>Video RAM on GPU ‚Äî the primary constraint determining which model sizes and formats can run on your hardware.</span></div>
  </div>

  <div class="section-title">Practical Tips</div>
  <div class="tip-box"><ul>
    <li>Start with GGUF Q4_K_M for any new model ‚Äî it is the best balance of quality and size for most consumer setups</li>
    <li>If you have a dedicated NVIDIA GPU with enough VRAM, try AWQ or EXL2 for noticeably faster inference than GGUF</li>
    <li>Always use SafeTensors when available ‚Äî never load untrusted serialized model files that could execute arbitrary code</li>
  </ul></div>

    <div class="section-title">Related Community Discussions</div>
    <div class="related-topics">
      <a href="#">Models</a>
    </div>

  <div class="nav-links">
    <a href="frameworks.html">&larr; Applied Frameworks</a>
    <a href="ai-protocols.html">AI Protocols &rarr;</a>
  </div>
</div>
<div class="footer"><a href="../../index.html">&larr; Back to Course</a></div>
</body>
</html>