<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Techno-Pessimists - Basic Theory</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background:#0a0a0f; color:#e4e6eb; }
a { color:#6ab2f2; text-decoration:none; }
a:hover { text-decoration:underline; }
.header { background:#111119; padding:16px 24px; border-bottom:1px solid #1e1e2e; display:flex; align-items:center; gap:16px; position:sticky; top:0; z-index:10; }
.header h1 { font-size:18px; color:#6ab2f2; }
.header .back { color:#8696a4; font-size:14px; }
.header .lang-switch { margin-left:auto; font-size:13px; color:#8696a4; }
.container { max-width:900px; margin:0 auto; padding:24px; }
.breadcrumb { color:#8696a4; font-size:13px; margin-bottom:20px; }
.breadcrumb a { color:#6ab2f2; }
.level-badge { display:inline-block; padding:4px 12px; border-radius:20px; font-size:12px; font-weight:600; margin-bottom:16px; }
.level-1 { background:rgba(74,222,128,0.15); color:#4ade80; }
.level-2 { background:rgba(96,165,250,0.15); color:#60a5fa; }
.level-3 { background:rgba(245,158,11,0.15); color:#f59e0b; }
.level-4 { background:rgba(239,68,68,0.15); color:#ef4444; }
.level-5 { background:rgba(168,85,247,0.15); color:#a855f7; }
h2 { font-size:28px; margin-bottom:8px; }
.desc { color:#8696a4; font-size:15px; line-height:1.7; margin-bottom:24px; }
.detail-list { list-style:none; padding:0; }
.detail-list li { padding:10px 16px; margin-bottom:6px; border-radius:8px; background:#111119; border-left:3px solid #2b5278; font-size:14px; line-height:1.5; color:#e4e6eb; }
.detail-list li::before { content:'\2192 '; color:#6ab2f2; }
.section-title { font-size:16px; color:#8696a4; margin:24px 0 12px; text-transform:uppercase; letter-spacing:1px; }
.related-topics { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
.related-topics a { display:inline-block; background:#111119; border:1px solid #1e1e2e; padding:6px 14px; border-radius:8px; font-size:13px; color:#6ab2f2; }
.related-topics a:hover { background:#1e1e2e; text-decoration:none; }
.nav-links { display:flex; justify-content:space-between; margin-top:30px; padding-top:20px; border-top:1px solid #1e1e2e; }
.nav-links a { color:#6ab2f2; font-size:14px; }
.nav-links .disabled { color:#333; }
.topic-grid { display:grid; grid-template-columns:repeat(auto-fill, minmax(260px, 1fr)); gap:12px; margin-top:16px; }
.topic-card { background:#111119; border-radius:12px; padding:18px; border-left:3px solid #2b5278; display:block; transition:background 0.15s; }
.topic-card:hover { background:#1a1a2e; text-decoration:none; }
.topic-card h3 { color:#e4e6eb; font-size:15px; margin-bottom:4px; }
.topic-card .sub { color:#8696a4; font-size:13px; }
.footer { text-align:center; padding:30px; color:#8696a4; font-size:12px; margin-top:30px; border-top:1px solid #1e1e2e; }
.overview { margin-bottom:28px; }
.overview p { color:#c4c6cb; font-size:15px; line-height:1.8; margin-bottom:14px; }
.terms-grid { display:grid; gap:10px; margin-top:12px; }
.term-card { background:#111119; border:1px solid #1e1e2e; border-radius:10px; padding:14px 18px; }
.term-card strong { color:#6ab2f2; font-size:14px; }
.term-card span { color:#c4c6cb; font-size:13px; display:block; margin-top:4px; line-height:1.5; }
.tip-box { background:linear-gradient(135deg, rgba(106,178,242,0.06), rgba(106,178,242,0.02)); border:1px solid rgba(106,178,242,0.15); border-radius:10px; padding:16px 20px; margin-top:12px; }
.tip-box li { color:#c4c6cb; font-size:14px; line-height:1.6; margin-bottom:8px; list-style:none; }
.tip-box li::before { content:'üí° '; }
.detail-card { background:#111119; border-radius:10px; padding:16px 18px; margin-bottom:8px; border-left:3px solid #2b5278; }
.detail-card .dt { color:#e4e6eb; font-size:14px; font-weight:600; margin-bottom:4px; }
.detail-card .dd { color:#9ca3af; font-size:13px; line-height:1.6; }
.detail-card .dl { margin-top:6px; }
.detail-card .dl a { font-size:12px; color:#6ab2f2; background:rgba(106,178,242,0.08); padding:3px 10px; border-radius:6px; display:inline-block; margin-right:6px; margin-top:4px; }
.detail-card .dl a:hover { background:rgba(106,178,242,0.15); text-decoration:none; }
</style>
</head>
<body>
<div class="header">
  <a href="../../index.html" class="back">&larr; Back to Course</a>
  <h1>Basic Theory</h1>
  <a href="../../../uk/basic-theory/level-5/doomers.html" class="lang-switch">üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</a>
</div>
<div class="container">
  <div class="breadcrumb"><a href="../../index.html">Course</a> / <a href="../index.html">Basic Theory</a> / <a href="index.html">Level 5</a> / Techno-Pessimists</div>
  <span class="level-badge level-5">üåå Level 5 ‚Äî Horizons</span>
  <h2>Techno-Pessimists</h2>
  <p class="desc">Concerns about existential risk from advanced AI.</p>

  <div class="overview">
    <p>AI risk researchers and techno-pessimists (&quot;doomers&quot;) argue that advanced AI poses existential risks to humanity that demand immediate, serious action. Their concerns range from near-term harms (deepfakes, job displacement, surveillance) to long-term catastrophic risks (misaligned superintelligence, loss of human control). Key figures include Eliezer Yudkowsky, Stuart Russell, and organizations like MIRI and the Pause AI movement.</p>
    <p>The doomer perspective is often mischaracterized as simply &quot;anti-technology.&quot; In reality, most AI safety researchers are deeply technical people who understand AI capabilities and see specific, well-reasoned dangers. Their arguments have influenced the creation of safety teams at every major AI lab and have shaped governmental AI policy worldwide.</p>
  </div>
  <div class="section-title">Key Topics Covered</div>
  <div class="detail-cards">
    <div class="detail-card"><div class="dt">The Core Argument</div><div class="dd">We are building increasingly powerful AI systems without understanding how to control them. The alignment problem is unsolved. Deploying superintelligent AI before solving alignment could be catastrophic and irreversible. The stakes are too high for &quot;move fast and break things.&quot;</div><div class="dl"><a href="alignment.html">AI Alignment</a></div></div>
    <div class="detail-card"><div class="dt">Eliezer Yudkowsky's Position</div><div class="dd">The most prominent &quot;doomer&quot; argues AI poses extinction-level risk, current alignment approaches are insufficient, and we may need to halt development of frontier AI. His writings at LessWrong have shaped the entire AI safety field.</div></div>
    <div class="detail-card"><div class="dt">Stuart Russell's Framework</div><div class="dd">&quot;Human Compatible&quot; (2019) argues the standard AI model (optimize a given objective) is fundamentally flawed. Instead, AI should be uncertain about human preferences and defer to humans. A more moderate but influential critique.</div></div>
    <div class="detail-card"><div class="dt">MIRI and AI Risk Research</div><div class="dd">Machine Intelligence Research Institute has studied AI alignment since 2000. Pioneered many concepts: corrigibility, goal stability, decision theory for AI. Their pessimism about current approaches has been influential.</div></div>
    <div class="detail-card"><div class="dt">Pause AI Movement</div><div class="dd">Calling for a moratorium on training models more powerful than GPT-4 until alignment is solved. The open letter (signed by Musk, Wozniak, and others) requested a 6-month pause. Critics call it impractical and counterproductive.</div></div>
    <div class="detail-card"><div class="dt">Near-Term AI Risks</div><div class="dd">Not just far-future concerns: deepfake misinformation, AI-powered cyberattacks, autonomous weapons, mass surveillance, algorithmic discrimination, and economic disruption from rapid automation. These risks are current and measurable.</div></div>
    <div class="detail-card"><div class="dt">The Alignment Tax</div><div class="dd">Safety research costs time and money but produces no direct revenue. Companies face competitive pressure to skip safety work. Without regulation or cultural norms, the incentive structure favors speed over safety.</div><div class="dl"><a href="ai-safety.html">AI Safety</a></div></div>
    <div class="detail-card"><div class="dt">Regulatory Approaches</div><div class="dd">EU AI Act (risk-based regulation), US Executive Order on AI Safety, UK AI Safety Institute, China AI regulations. Different countries taking different approaches ‚Äî a global framework remains elusive.</div></div>
    <div class="detail-card"><div class="dt">Counterarguments</div><div class="dd">Critics argue: pausing is unenforceable internationally, slowing AI has real costs (delayed benefits), current AI is not close to dangerous superintelligence, and safety research progresses alongside capabilities.</div><div class="dl"><a href="accelerationists.html">Techno-Optimists</a></div></div>
    <div class="detail-card"><div class="dt">The Productive Middle</div><div class="dd">Many researchers occupy a middle ground: AI development should continue but with mandatory safety evaluations, alignment research investment proportional to capability, and international coordination on the most dangerous capabilities.</div></div>
  </div>

  <div class="section-title">Key Terms</div>
  <div class="terms-grid">
    <div class="term-card"><strong>AI Doomer</strong><span>Person who believes advanced AI poses serious existential risk to humanity and advocates for caution or pause.</span></div>
    <div class="term-card"><strong>Pause AI</strong><span>Movement calling for a moratorium on training AI systems more powerful than current frontier models.</span></div>
    <div class="term-card"><strong>X-Risk</strong><span>Existential risk ‚Äî any event that could cause human extinction or permanent, drastic reduction in human potential.</span></div>
    <div class="term-card"><strong>AI Governance</strong><span>Policy frameworks, regulations, and international agreements for managing AI development and deployment.</span></div>
  </div>

  <div class="section-title">Practical Tips</div>
  <div class="tip-box"><ul>
    <li>Read both doomer and accelerationist perspectives ‚Äî the truth likely involves elements of both, and strong opinions on either side often miss nuance</li>
    <li>Follow the EU AI Act implementation for the most concrete example of AI regulation in practice</li>
    <li>Near-term AI risks (deepfakes, surveillance) are worth taking seriously regardless of your view on long-term existential risk</li>
  </ul></div>

    <div class="section-title">Related Community Discussions</div>
    <div class="related-topics">
      <a href="#">Feed</a>
      <a href="#">Video Content</a>
    </div>

  <div class="nav-links">
    <a href="accelerationists.html">&larr; Techno-Optimists</a>
    <a href="ai-safety.html">AI Safety &rarr;</a>
  </div>
</div>
<div class="footer"><a href="../../index.html">&larr; Back to Course</a></div>
</body>
</html>