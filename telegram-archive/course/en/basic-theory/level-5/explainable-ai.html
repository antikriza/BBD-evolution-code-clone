<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Explainable &amp; Constitutional AI - Basic Theory</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background:#0a0a0f; color:#e4e6eb; }
a { color:#6ab2f2; text-decoration:none; }
a:hover { text-decoration:underline; }
.header { background:#111119; padding:16px 24px; border-bottom:1px solid #1e1e2e; display:flex; align-items:center; gap:16px; position:sticky; top:0; z-index:10; }
.header h1 { font-size:18px; color:#6ab2f2; }
.header .back { color:#8696a4; font-size:14px; }
.header .lang-switch { margin-left:auto; font-size:13px; color:#8696a4; }
.container { max-width:900px; margin:0 auto; padding:24px; }
.breadcrumb { color:#8696a4; font-size:13px; margin-bottom:20px; }
.breadcrumb a { color:#6ab2f2; }
.level-badge { display:inline-block; padding:4px 12px; border-radius:20px; font-size:12px; font-weight:600; margin-bottom:16px; }
.level-1 { background:rgba(74,222,128,0.15); color:#4ade80; }
.level-2 { background:rgba(96,165,250,0.15); color:#60a5fa; }
.level-3 { background:rgba(245,158,11,0.15); color:#f59e0b; }
.level-4 { background:rgba(239,68,68,0.15); color:#ef4444; }
.level-5 { background:rgba(168,85,247,0.15); color:#a855f7; }
h2 { font-size:28px; margin-bottom:8px; }
.desc { color:#8696a4; font-size:15px; line-height:1.7; margin-bottom:24px; }
.detail-list { list-style:none; padding:0; }
.detail-list li { padding:10px 16px; margin-bottom:6px; border-radius:8px; background:#111119; border-left:3px solid #2b5278; font-size:14px; line-height:1.5; color:#e4e6eb; }
.detail-list li::before { content:'\2192 '; color:#6ab2f2; }
.section-title { font-size:16px; color:#8696a4; margin:24px 0 12px; text-transform:uppercase; letter-spacing:1px; }
.related-topics { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
.related-topics a { display:inline-block; background:#111119; border:1px solid #1e1e2e; padding:6px 14px; border-radius:8px; font-size:13px; color:#6ab2f2; }
.related-topics a:hover { background:#1e1e2e; text-decoration:none; }
.nav-links { display:flex; justify-content:space-between; margin-top:30px; padding-top:20px; border-top:1px solid #1e1e2e; }
.nav-links a { color:#6ab2f2; font-size:14px; }
.nav-links .disabled { color:#333; }
.topic-grid { display:grid; grid-template-columns:repeat(auto-fill, minmax(260px, 1fr)); gap:12px; margin-top:16px; }
.topic-card { background:#111119; border-radius:12px; padding:18px; border-left:3px solid #2b5278; display:block; transition:background 0.15s; }
.topic-card:hover { background:#1a1a2e; text-decoration:none; }
.topic-card h3 { color:#e4e6eb; font-size:15px; margin-bottom:4px; }
.topic-card .sub { color:#8696a4; font-size:13px; }
.footer { text-align:center; padding:30px; color:#8696a4; font-size:12px; margin-top:30px; border-top:1px solid #1e1e2e; }
.overview { margin-bottom:28px; }
.overview p { color:#c4c6cb; font-size:15px; line-height:1.8; margin-bottom:14px; }
.terms-grid { display:grid; gap:10px; margin-top:12px; }
.term-card { background:#111119; border:1px solid #1e1e2e; border-radius:10px; padding:14px 18px; }
.term-card strong { color:#6ab2f2; font-size:14px; }
.term-card span { color:#c4c6cb; font-size:13px; display:block; margin-top:4px; line-height:1.5; }
.tip-box { background:linear-gradient(135deg, rgba(106,178,242,0.06), rgba(106,178,242,0.02)); border:1px solid rgba(106,178,242,0.15); border-radius:10px; padding:16px 20px; margin-top:12px; }
.tip-box li { color:#c4c6cb; font-size:14px; line-height:1.6; margin-bottom:8px; list-style:none; }
.tip-box li::before { content:'üí° '; }
.detail-card { background:#111119; border-radius:10px; padding:16px 18px; margin-bottom:8px; border-left:3px solid #2b5278; }
.detail-card .dt { color:#e4e6eb; font-size:14px; font-weight:600; margin-bottom:4px; }
.detail-card .dd { color:#9ca3af; font-size:13px; line-height:1.6; }
.detail-card .dl { margin-top:6px; }
.detail-card .dl a { font-size:12px; color:#6ab2f2; background:rgba(106,178,242,0.08); padding:3px 10px; border-radius:6px; display:inline-block; margin-right:6px; margin-top:4px; }
.detail-card .dl a:hover { background:rgba(106,178,242,0.15); text-decoration:none; }
</style>
</head>
<body>
<div class="header">
  <a href="../../index.html" class="back">&larr; Back to Course</a>
  <h1>Basic Theory</h1>
  <a href="../../../uk/basic-theory/level-5/explainable-ai.html" class="lang-switch">üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</a>
</div>
<div class="container">
  <div class="breadcrumb"><a href="../../index.html">Course</a> / <a href="../index.html">Basic Theory</a> / <a href="index.html">Level 5</a> / Explainable &amp; Constitutional AI</div>
  <span class="level-badge level-5">üåå Level 5 ‚Äî Horizons</span>
  <h2>Explainable &amp; Constitutional AI</h2>
  <p class="desc">Making AI decisions transparent and principled.</p>

  <div class="overview">
    <p>Explainable AI (XAI) is the field dedicated to making AI decisions understandable to humans. As AI systems make increasingly important decisions (medical diagnoses, loan approvals, legal recommendations), the ability to explain &quot;why&quot; becomes critical for trust, debugging, accountability, and regulatory compliance.</p>
    <p>The field spans from post-hoc explanation methods (LIME, SHAP) that explain individual predictions, to inherently interpretable architectures, to the frontier of mechanistic interpretability ‚Äî reverse-engineering what happens inside neural networks at the circuit level. The EU AI Act and similar regulations now mandate explainability for high-risk AI systems.</p>
  </div>
  <div class="section-title">Key Topics Covered</div>
  <div class="detail-cards">
    <div class="detail-card"><div class="dt">Why Explainability Matters</div><div class="dd">Trust (would you accept a cancer diagnosis from a black box?), debugging (finding model errors), accountability (who is responsible when AI is wrong?), regulation (EU AI Act requires explanations), and fairness (detecting bias in decisions).</div></div>
    <div class="detail-card"><div class="dt">LIME</div><div class="dd">Local Interpretable Model-agnostic Explanations ‚Äî explains individual predictions by approximating the model locally with a simpler, interpretable model. Works with any model. &quot;For this patient, age and blood pressure were the key factors.&quot;</div></div>
    <div class="detail-card"><div class="dt">SHAP</div><div class="dd">SHapley Additive exPlanations ‚Äî uses game theory (Shapley values) to assign each feature its fair contribution to a prediction. Mathematically rigorous. Provides both local (per-prediction) and global (overall model) explanations.</div></div>
    <div class="detail-card"><div class="dt">Attention Visualization</div><div class="dd">Visualizing which input tokens/regions a transformer model focuses on when generating output. Informative but can be misleading ‚Äî attention patterns don't always reveal the true reasoning process.</div><div class="dl"><a href="../level-1/reasoning.html">Reasoning</a></div></div>
    <div class="detail-card"><div class="dt">Mechanistic Interpretability</div><div class="dd">The frontier: reverse-engineering neural networks to understand the actual algorithms they implement. Anthropic's research identified specific circuits for math, language, and factual recall inside Claude. This is the deepest form of explainability.</div><div class="dl"><a href="alignment.html">AI Alignment</a></div></div>
    <div class="detail-card"><div class="dt">Constitutional AI and Principles</div><div class="dd">Making AI behavior principled and transparent by training with explicit value statements. The model can articulate why it refuses or adjusts certain responses. Principles provide a human-readable &quot;source code&quot; for AI behavior.</div><div class="dl"><a href="ai-safety.html">AI Safety</a></div></div>
    <div class="detail-card"><div class="dt">Inherently Interpretable Models</div><div class="dd">Decision trees, linear models, and rule-based systems are interpretable by design. For high-stakes applications, some argue we should prefer interpretable models even at a cost to accuracy. Trade-off between capability and transparency.</div></div>
    <div class="detail-card"><div class="dt">Chain-of-Thought as Explanation</div><div class="dd">LLMs can explain their reasoning step by step. But are these explanations faithful to the actual internal process, or post-hoc rationalizations? Research suggests they are partially faithful but not fully reliable.</div><div class="dl"><a href="../level-4/prompting-techniques.html">Prompting Techniques</a></div></div>
    <div class="detail-card"><div class="dt">Regulatory Landscape</div><div class="dd">EU AI Act requires explainability for high-risk AI systems (healthcare, law enforcement, credit). US is developing sector-specific guidelines. The &quot;right to explanation&quot; may become a fundamental right in AI-affected decisions.</div></div>
    <div class="detail-card"><div class="dt">Challenges and Limitations</div><div class="dd">Some models may be too complex to explain faithfully. Explanations can be gamed (providing plausible but incorrect reasons). Balancing accuracy with interpretability remains an open challenge. Perfect explainability may be impossible for the most capable systems.</div></div>
  </div>

  <div class="section-title">Key Terms</div>
  <div class="terms-grid">
    <div class="term-card"><strong>XAI</strong><span>Explainable AI ‚Äî field dedicated to making AI decisions understandable and transparent to humans.</span></div>
    <div class="term-card"><strong>SHAP</strong><span>Shapley Additive Explanations ‚Äî game theory-based method for explaining individual model predictions.</span></div>
    <div class="term-card"><strong>Mechanistic Interpretability</strong><span>Reverse-engineering neural networks to understand the actual algorithms and circuits they implement.</span></div>
    <div class="term-card"><strong>Right to Explanation</strong><span>Emerging legal concept that people affected by AI decisions have the right to understand how those decisions were made.</span></div>
  </div>

  <div class="section-title">Practical Tips</div>
  <div class="tip-box"><ul>
    <li>Use SHAP for production ML systems that need explainability ‚Äî it is the most rigorous and widely accepted method</li>
    <li>Follow Anthropic's mechanistic interpretability research for cutting-edge work on understanding what happens inside neural networks</li>
    <li>If you build AI products for regulated industries (finance, healthcare), plan for explainability requirements from the start</li>
  </ul></div>

    <div class="section-title">Related Community Discussions</div>
    <div class="related-topics">
      <a href="#">Feed</a>
    </div>

  <div class="nav-links">
    <a href="alignment.html">&larr; AI Alignment</a>
    <a href="decentralized-ai.html">Decentralized AI &rarr;</a>
  </div>
</div>
<div class="footer"><a href="../../index.html">&larr; Back to Course</a></div>
</body>
</html>