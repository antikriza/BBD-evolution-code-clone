<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI Safety - Basic Theory</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background:#0a0a0f; color:#e4e6eb; }
a { color:#6ab2f2; text-decoration:none; }
a:hover { text-decoration:underline; }
.header { background:#111119; padding:16px 24px; border-bottom:1px solid #1e1e2e; display:flex; align-items:center; gap:16px; position:sticky; top:0; z-index:10; }
.header h1 { font-size:18px; color:#6ab2f2; }
.header .back { color:#8696a4; font-size:14px; }
.header .lang-switch { margin-left:auto; font-size:13px; color:#8696a4; }
.container { max-width:900px; margin:0 auto; padding:24px; }
.breadcrumb { color:#8696a4; font-size:13px; margin-bottom:20px; }
.breadcrumb a { color:#6ab2f2; }
.level-badge { display:inline-block; padding:4px 12px; border-radius:20px; font-size:12px; font-weight:600; margin-bottom:16px; }
.level-1 { background:rgba(74,222,128,0.15); color:#4ade80; }
.level-2 { background:rgba(96,165,250,0.15); color:#60a5fa; }
.level-3 { background:rgba(245,158,11,0.15); color:#f59e0b; }
.level-4 { background:rgba(239,68,68,0.15); color:#ef4444; }
.level-5 { background:rgba(168,85,247,0.15); color:#a855f7; }
h2 { font-size:28px; margin-bottom:8px; }
.desc { color:#8696a4; font-size:15px; line-height:1.7; margin-bottom:24px; }
.detail-list { list-style:none; padding:0; }
.detail-list li { padding:10px 16px; margin-bottom:6px; border-radius:8px; background:#111119; border-left:3px solid #2b5278; font-size:14px; line-height:1.5; color:#e4e6eb; }
.detail-list li::before { content:'\2192 '; color:#6ab2f2; }
.section-title { font-size:16px; color:#8696a4; margin:24px 0 12px; text-transform:uppercase; letter-spacing:1px; }
.related-topics { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
.related-topics a { display:inline-block; background:#111119; border:1px solid #1e1e2e; padding:6px 14px; border-radius:8px; font-size:13px; color:#6ab2f2; }
.related-topics a:hover { background:#1e1e2e; text-decoration:none; }
.nav-links { display:flex; justify-content:space-between; margin-top:30px; padding-top:20px; border-top:1px solid #1e1e2e; }
.nav-links a { color:#6ab2f2; font-size:14px; }
.nav-links .disabled { color:#333; }
.topic-grid { display:grid; grid-template-columns:repeat(auto-fill, minmax(260px, 1fr)); gap:12px; margin-top:16px; }
.topic-card { background:#111119; border-radius:12px; padding:18px; border-left:3px solid #2b5278; display:block; transition:background 0.15s; }
.topic-card:hover { background:#1a1a2e; text-decoration:none; }
.topic-card h3 { color:#e4e6eb; font-size:15px; margin-bottom:4px; }
.topic-card .sub { color:#8696a4; font-size:13px; }
.footer { text-align:center; padding:30px; color:#8696a4; font-size:12px; margin-top:30px; border-top:1px solid #1e1e2e; }
.overview { margin-bottom:28px; }
.overview p { color:#c4c6cb; font-size:15px; line-height:1.8; margin-bottom:14px; }
.terms-grid { display:grid; gap:10px; margin-top:12px; }
.term-card { background:#111119; border:1px solid #1e1e2e; border-radius:10px; padding:14px 18px; }
.term-card strong { color:#6ab2f2; font-size:14px; }
.term-card span { color:#c4c6cb; font-size:13px; display:block; margin-top:4px; line-height:1.5; }
.tip-box { background:linear-gradient(135deg, rgba(106,178,242,0.06), rgba(106,178,242,0.02)); border:1px solid rgba(106,178,242,0.15); border-radius:10px; padding:16px 20px; margin-top:12px; }
.tip-box li { color:#c4c6cb; font-size:14px; line-height:1.6; margin-bottom:8px; list-style:none; }
.tip-box li::before { content:'üí° '; }
.detail-card { background:#111119; border-radius:10px; padding:16px 18px; margin-bottom:8px; border-left:3px solid #2b5278; }
.detail-card .dt { color:#e4e6eb; font-size:14px; font-weight:600; margin-bottom:4px; }
.detail-card .dd { color:#9ca3af; font-size:13px; line-height:1.6; }
.detail-card .dl { margin-top:6px; }
.detail-card .dl a { font-size:12px; color:#6ab2f2; background:rgba(106,178,242,0.08); padding:3px 10px; border-radius:6px; display:inline-block; margin-right:6px; margin-top:4px; }
.detail-card .dl a:hover { background:rgba(106,178,242,0.15); text-decoration:none; }
</style>
</head>
<body>
<div class="header">
  <a href="../../index.html" class="back">&larr; Back to Course</a>
  <h1>Basic Theory</h1>
  <a href="../../../uk/basic-theory/level-5/ai-safety.html" class="lang-switch">üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</a>
</div>
<div class="container">
  <div class="breadcrumb"><a href="../../index.html">Course</a> / <a href="../index.html">Basic Theory</a> / <a href="index.html">Level 5</a> / AI Safety</div>
  <span class="level-badge level-5">üåå Level 5 ‚Äî Horizons</span>
  <h2>AI Safety</h2>
  <p class="desc">Research and practices for building safe AI systems.</p>

  <div class="overview">
    <p>AI safety is the field of research and engineering dedicated to ensuring AI systems behave as intended, remain under human control, and do not cause harm. It spans from near-term practical concerns (preventing bias, ensuring robustness, avoiding misuse) to long-term challenges (alignment, containment, value learning). Every major AI lab now has a dedicated safety team.</p>
    <p>Safety is not the opposite of capability ‚Äî it is what enables capability to be deployed responsibly. Just as aviation safety enabled air travel to become the safest form of transportation, AI safety research aims to make increasingly powerful AI systems trustworthy enough for high-stakes applications.</p>
  </div>
  <div class="section-title">Key Topics Covered</div>
  <div class="detail-cards">
    <div class="detail-card"><div class="dt">Categories of AI Risk</div><div class="dd">Misuse (intentional harm: deepfakes, cyberweapons), Misalignment (unintended behavior from flawed objectives), Accidents (bugs and failures in AI systems), Structural risks (concentration of power, economic disruption).</div></div>
    <div class="detail-card"><div class="dt">Red-Teaming</div><div class="dd">Adversarial testing of AI systems to find harmful behaviors before deployment. Teams try to make the AI produce dangerous content, leak information, or behave unpredictably. Now standard practice at all major labs.</div></div>
    <div class="detail-card"><div class="dt">Safety Evaluations</div><div class="dd">Standardized tests for dangerous capabilities: CBRN knowledge (chemical/biological/radiological/nuclear), cyber offense, persuasion, autonomous replication. Anthropic, OpenAI, and DeepMind all publish safety evaluation results.</div></div>
    <div class="detail-card"><div class="dt">Constitutional AI</div><div class="dd">Anthropic's approach: train AI with a set of principles (a &quot;constitution&quot;) and have it self-evaluate against those principles. Reduces the need for human feedback while maintaining safety properties.</div><div class="dl"><a href="alignment.html">AI Alignment</a></div></div>
    <div class="detail-card"><div class="dt">Containment and Monitoring</div><div class="dd">Strategies for controlling AI systems: sandboxing (limited environment access), human-in-the-loop (approval for high-stakes actions), output filtering, and continuous monitoring for anomalous behavior.</div><div class="dl"><a href="../level-4/agents.html">Agents</a></div></div>
    <div class="detail-card"><div class="dt">Responsible Scaling</div><div class="dd">Anthropic's Responsible Scaling Policy and similar frameworks: assess safety before increasing capabilities. If safety is not demonstrated at a capability level, don't scale further until it is.</div></div>
    <div class="detail-card"><div class="dt">AI Safety Organizations</div><div class="dd">Anthropic (safety-focused lab), OpenAI Safety team, Google DeepMind Safety, MIRI, Center for AI Safety (CAIS), AI Safety Institute (UK/US), Alignment Research Center (ARC). Growing ecosystem of safety-focused research.</div></div>
    <div class="detail-card"><div class="dt">Practical Safety Engineering</div><div class="dd">Input validation, output filtering, rate limiting, abuse detection, prompt injection defense, and secure tool use. The engineering side of safety that every AI application developer should implement.</div><div class="dl"><a href="../level-4/tool-use.html">Tool Use</a></div></div>
    <div class="detail-card"><div class="dt">Dual-Use Concerns</div><div class="dd">Many AI capabilities are dual-use: code generation helps developers but also creates malware. Biology knowledge helps research but enables bioweapons. Managing this tension is a core safety challenge.</div></div>
    <div class="detail-card"><div class="dt">The Safety Culture Shift</div><div class="dd">AI safety has moved from niche concern to mainstream requirement. Major AI conferences have safety tracks, companies hire safety researchers, and governments create safety institutes. The culture is shifting toward taking safety seriously.</div></div>
  </div>

  <div class="section-title">Key Terms</div>
  <div class="terms-grid">
    <div class="term-card"><strong>Red-Teaming</strong><span>Adversarial testing to find harmful AI behaviors before deployment by simulating attacks and misuse.</span></div>
    <div class="term-card"><strong>Constitutional AI</strong><span>Training method where AI evaluates its own outputs against a set of safety principles.</span></div>
    <div class="term-card"><strong>Responsible Scaling</strong><span>Framework requiring safety demonstrations before increasing AI model capabilities.</span></div>
    <div class="term-card"><strong>Dual-Use</strong><span>AI capabilities that have both beneficial and harmful applications, creating tension between access and safety.</span></div>
  </div>

  <div class="section-title">Practical Tips</div>
  <div class="tip-box"><ul>
    <li>If you build AI applications, implement practical safety measures (input validation, output filtering, rate limiting) from day one</li>
    <li>Follow Anthropic's research blog for the most accessible writing on frontier AI safety techniques</li>
    <li>AI safety is a growing career field with strong demand ‚Äî consider contributing regardless of your background</li>
  </ul></div>

    <div class="section-title">Related Community Discussions</div>
    <div class="related-topics">
      <a href="#">Feed</a>
      <a href="#">Video Content</a>
    </div>

  <div class="nav-links">
    <a href="doomers.html">&larr; Techno-Pessimists</a>
    <a href="alignment.html">AI Alignment &rarr;</a>
  </div>
</div>
<div class="footer"><a href="../../index.html">&larr; Back to Course</a></div>
</body>
</html>