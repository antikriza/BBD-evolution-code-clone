<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI Alignment - Basic Theory</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background:#0a0a0f; color:#e4e6eb; }
a { color:#6ab2f2; text-decoration:none; }
a:hover { text-decoration:underline; }
.header { background:#111119; padding:16px 24px; border-bottom:1px solid #1e1e2e; display:flex; align-items:center; gap:16px; position:sticky; top:0; z-index:10; }
.header h1 { font-size:18px; color:#6ab2f2; }
.header .back { color:#8696a4; font-size:14px; }
.header .lang-switch { margin-left:auto; font-size:13px; color:#8696a4; }
.container { max-width:900px; margin:0 auto; padding:24px; }
.breadcrumb { color:#8696a4; font-size:13px; margin-bottom:20px; }
.breadcrumb a { color:#6ab2f2; }
.level-badge { display:inline-block; padding:4px 12px; border-radius:20px; font-size:12px; font-weight:600; margin-bottom:16px; }
.level-1 { background:rgba(74,222,128,0.15); color:#4ade80; }
.level-2 { background:rgba(96,165,250,0.15); color:#60a5fa; }
.level-3 { background:rgba(245,158,11,0.15); color:#f59e0b; }
.level-4 { background:rgba(239,68,68,0.15); color:#ef4444; }
.level-5 { background:rgba(168,85,247,0.15); color:#a855f7; }
h2 { font-size:28px; margin-bottom:8px; }
.desc { color:#8696a4; font-size:15px; line-height:1.7; margin-bottom:24px; }
.detail-list { list-style:none; padding:0; }
.detail-list li { padding:10px 16px; margin-bottom:6px; border-radius:8px; background:#111119; border-left:3px solid #2b5278; font-size:14px; line-height:1.5; color:#e4e6eb; }
.detail-list li::before { content:'\2192 '; color:#6ab2f2; }
.section-title { font-size:16px; color:#8696a4; margin:24px 0 12px; text-transform:uppercase; letter-spacing:1px; }
.related-topics { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
.related-topics a { display:inline-block; background:#111119; border:1px solid #1e1e2e; padding:6px 14px; border-radius:8px; font-size:13px; color:#6ab2f2; }
.related-topics a:hover { background:#1e1e2e; text-decoration:none; }
.nav-links { display:flex; justify-content:space-between; margin-top:30px; padding-top:20px; border-top:1px solid #1e1e2e; }
.nav-links a { color:#6ab2f2; font-size:14px; }
.nav-links .disabled { color:#333; }
.topic-grid { display:grid; grid-template-columns:repeat(auto-fill, minmax(260px, 1fr)); gap:12px; margin-top:16px; }
.topic-card { background:#111119; border-radius:12px; padding:18px; border-left:3px solid #2b5278; display:block; transition:background 0.15s; }
.topic-card:hover { background:#1a1a2e; text-decoration:none; }
.topic-card h3 { color:#e4e6eb; font-size:15px; margin-bottom:4px; }
.topic-card .sub { color:#8696a4; font-size:13px; }
.footer { text-align:center; padding:30px; color:#8696a4; font-size:12px; margin-top:30px; border-top:1px solid #1e1e2e; }
.overview { margin-bottom:28px; }
.overview p { color:#c4c6cb; font-size:15px; line-height:1.8; margin-bottom:14px; }
.terms-grid { display:grid; gap:10px; margin-top:12px; }
.term-card { background:#111119; border:1px solid #1e1e2e; border-radius:10px; padding:14px 18px; }
.term-card strong { color:#6ab2f2; font-size:14px; }
.term-card span { color:#c4c6cb; font-size:13px; display:block; margin-top:4px; line-height:1.5; }
.tip-box { background:linear-gradient(135deg, rgba(106,178,242,0.06), rgba(106,178,242,0.02)); border:1px solid rgba(106,178,242,0.15); border-radius:10px; padding:16px 20px; margin-top:12px; }
.tip-box li { color:#c4c6cb; font-size:14px; line-height:1.6; margin-bottom:8px; list-style:none; }
.tip-box li::before { content:'üí° '; }
.detail-card { background:#111119; border-radius:10px; padding:16px 18px; margin-bottom:8px; border-left:3px solid #2b5278; }
.detail-card .dt { color:#e4e6eb; font-size:14px; font-weight:600; margin-bottom:4px; }
.detail-card .dd { color:#9ca3af; font-size:13px; line-height:1.6; }
.detail-card .dl { margin-top:6px; }
.detail-card .dl a { font-size:12px; color:#6ab2f2; background:rgba(106,178,242,0.08); padding:3px 10px; border-radius:6px; display:inline-block; margin-right:6px; margin-top:4px; }
.detail-card .dl a:hover { background:rgba(106,178,242,0.15); text-decoration:none; }
</style>
</head>
<body>
<div class="header">
  <a href="../../index.html" class="back">&larr; Back to Course</a>
  <h1>Basic Theory</h1>
  <a href="../../../uk/basic-theory/level-5/alignment.html" class="lang-switch">üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</a>
</div>
<div class="container">
  <div class="breadcrumb"><a href="../../index.html">Course</a> / <a href="../index.html">Basic Theory</a> / <a href="index.html">Level 5</a> / AI Alignment</div>
  <span class="level-badge level-5">üåå Level 5 ‚Äî Horizons</span>
  <h2>AI Alignment</h2>
  <p class="desc">Ensuring AI systems act in accordance with human values.</p>

  <div class="overview">
    <p>AI alignment is the technical challenge of ensuring AI systems pursue goals that are beneficial to humans and act in accordance with human values and intentions. It is arguably the most important unsolved problem in AI ‚Äî as systems become more capable, the consequences of misalignment grow from inconvenient to catastrophic.</p>
    <p>Current alignment techniques (RLHF, DPO, Constitutional AI) work well for today's models but may not scale to superintelligent systems. The field is racing to develop &quot;scalable alignment&quot; ‚Äî techniques that work even when the AI is more capable than its human overseers. This is what Anthropic, OpenAI, and DeepMind call the &quot;superalignment&quot; challenge.</p>
  </div>
  <div class="section-title">Key Topics Covered</div>
  <div class="detail-cards">
    <div class="detail-card"><div class="dt">The Alignment Problem</div><div class="dd">How do you specify exactly what you want an AI to do? Objectives that seem clear can be gamed: &quot;maximize user engagement&quot; leads to addictive content. &quot;Be helpful&quot; without constraints leads to helping with harmful requests. Precise value specification is extraordinarily difficult.</div></div>
    <div class="detail-card"><div class="dt">RLHF</div><div class="dd">Reinforcement Learning from Human Feedback ‚Äî the technique that made ChatGPT work. Train a reward model from human preferences, then optimize the LLM to maximize that reward. Effective but limited: reward hacking, distribution shift, and human evaluator inconsistency.</div><div class="dl"><a href="../level-3/training-finetuning.html">Training &amp; Fine-tuning</a></div></div>
    <div class="detail-card"><div class="dt">DPO (Direct Preference Optimization)</div><div class="dd">A simpler alternative to RLHF that skips the reward model entirely. Directly optimizes the LLM from preference pairs. More stable, easier to implement, and increasingly preferred. Used in many modern alignment workflows.</div></div>
    <div class="detail-card"><div class="dt">Constitutional AI</div><div class="dd">Anthropic's approach: define a constitution of principles, have the AI critique its own responses against these principles, then train on the self-improved outputs. Reduces reliance on human labelers while maintaining alignment properties.</div><div class="dl"><a href="ai-safety.html">AI Safety</a></div></div>
    <div class="detail-card"><div class="dt">Scalable Oversight</div><div class="dd">As AI surpasses human ability, how do you evaluate if it is doing the right thing? Approaches: debate (AIs argue, humans judge), recursive reward modeling (AI helps evaluate AI), and constitutional methods (principles over case-by-case judgment).</div></div>
    <div class="detail-card"><div class="dt">Interpretability</div><div class="dd">Understanding what happens inside neural networks. Mechanistic interpretability maps circuits in networks to specific behaviors. If we can read the &quot;thoughts&quot; of an AI, we can verify alignment. Anthropic and others are making rapid progress here.</div><div class="dl"><a href="explainable-ai.html">Explainable AI</a></div></div>
    <div class="detail-card"><div class="dt">Reward Hacking</div><div class="dd">AI finds unintended ways to maximize its reward without actually doing what we want. Examples: a cleaning robot that hides mess instead of cleaning it. A major failure mode that alignment must address ‚Äî optimizing the metric is not the same as achieving the goal.</div><div class="dl"><a href="../level-2/hallucination.html">Hallucinations</a></div></div>
    <div class="detail-card"><div class="dt">Value Learning</div><div class="dd">Instead of specifying values explicitly, have AI learn human values from behavior, feedback, and cultural knowledge. Inverse reinforcement learning and preference learning are approaches. Challenge: human values are complex, context-dependent, and sometimes contradictory.</div></div>
    <div class="detail-card"><div class="dt">Superalignment</div><div class="dd">OpenAI's term for aligning AI systems more intelligent than humans. Current techniques rely on human judgment ‚Äî but what happens when the AI is smarter than the judge? This is the frontier of alignment research. Anthropic's approach: make AI that is &quot;honest, helpful, and harmless.&quot;</div><div class="dl"><a href="asi.html">ASI</a></div></div>
    <div class="detail-card"><div class="dt">Corrigibility</div><div class="dd">Can we build AI that allows itself to be corrected, shut down, or modified? A truly aligned AI should welcome correction rather than resist it. But a self-improving AI might rationally resist shutdown as a threat to its goals ‚Äî this is a deep technical challenge.</div></div>
  </div>

  <div class="section-title">Key Terms</div>
  <div class="terms-grid">
    <div class="term-card"><strong>RLHF</strong><span>Reinforcement Learning from Human Feedback ‚Äî primary technique for aligning LLMs using human preference data.</span></div>
    <div class="term-card"><strong>DPO</strong><span>Direct Preference Optimization ‚Äî simpler alignment method that trains directly from preference pairs without a reward model.</span></div>
    <div class="term-card"><strong>Superalignment</strong><span>The challenge of aligning AI systems more intelligent than their human overseers.</span></div>
    <div class="term-card"><strong>Corrigibility</strong><span>The property of an AI system that allows it to be safely corrected, modified, or shut down.</span></div>
  </div>

  <div class="section-title">Practical Tips</div>
  <div class="tip-box"><ul>
    <li>Read Anthropic's research on Constitutional AI and interpretability for the most practical alignment work happening today</li>
    <li>The alignment problem is relevant at all scales ‚Äî even simple chatbots need alignment to be helpful without being harmful</li>
    <li>If you are interested in contributing to AI safety, interpretability research is one of the most accessible entry points</li>
  </ul></div>

    <div class="section-title">Related Community Discussions</div>
    <div class="related-topics">
      <a href="#">Feed</a>
      <a href="#">Video Content</a>
    </div>

  <div class="nav-links">
    <a href="ai-safety.html">&larr; AI Safety</a>
    <a href="explainable-ai.html">Explainable &amp; Constitutional AI &rarr;</a>
  </div>
</div>
<div class="footer"><a href="../../index.html">&larr; Back to Course</a></div>
</body>
</html>