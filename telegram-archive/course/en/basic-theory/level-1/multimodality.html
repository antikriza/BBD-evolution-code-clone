<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Multimodality - Basic Theory</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background:#0a0a0f; color:#e4e6eb; }
a { color:#6ab2f2; text-decoration:none; }
a:hover { text-decoration:underline; }
.header { background:#111119; padding:16px 24px; border-bottom:1px solid #1e1e2e; display:flex; align-items:center; gap:16px; position:sticky; top:0; z-index:10; }
.header h1 { font-size:18px; color:#6ab2f2; }
.header .back { color:#8696a4; font-size:14px; }
.header .lang-switch { margin-left:auto; font-size:13px; color:#8696a4; }
.container { max-width:900px; margin:0 auto; padding:24px; }
.breadcrumb { color:#8696a4; font-size:13px; margin-bottom:20px; }
.breadcrumb a { color:#6ab2f2; }
.level-badge { display:inline-block; padding:4px 12px; border-radius:20px; font-size:12px; font-weight:600; margin-bottom:16px; }
.level-1 { background:rgba(74,222,128,0.15); color:#4ade80; }
.level-2 { background:rgba(96,165,250,0.15); color:#60a5fa; }
.level-3 { background:rgba(245,158,11,0.15); color:#f59e0b; }
.level-4 { background:rgba(239,68,68,0.15); color:#ef4444; }
.level-5 { background:rgba(168,85,247,0.15); color:#a855f7; }
h2 { font-size:28px; margin-bottom:8px; }
.desc { color:#8696a4; font-size:15px; line-height:1.7; margin-bottom:24px; }
.detail-list { list-style:none; padding:0; }
.detail-list li { padding:10px 16px; margin-bottom:6px; border-radius:8px; background:#111119; border-left:3px solid #2b5278; font-size:14px; line-height:1.5; color:#e4e6eb; }
.detail-list li::before { content:'\2192 '; color:#6ab2f2; }
.section-title { font-size:16px; color:#8696a4; margin:24px 0 12px; text-transform:uppercase; letter-spacing:1px; }
.related-topics { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
.related-topics a { display:inline-block; background:#111119; border:1px solid #1e1e2e; padding:6px 14px; border-radius:8px; font-size:13px; color:#6ab2f2; }
.related-topics a:hover { background:#1e1e2e; text-decoration:none; }
.nav-links { display:flex; justify-content:space-between; margin-top:30px; padding-top:20px; border-top:1px solid #1e1e2e; }
.nav-links a { color:#6ab2f2; font-size:14px; }
.nav-links .disabled { color:#333; }
.topic-grid { display:grid; grid-template-columns:repeat(auto-fill, minmax(260px, 1fr)); gap:12px; margin-top:16px; }
.topic-card { background:#111119; border-radius:12px; padding:18px; border-left:3px solid #2b5278; display:block; transition:background 0.15s; }
.topic-card:hover { background:#1a1a2e; text-decoration:none; }
.topic-card h3 { color:#e4e6eb; font-size:15px; margin-bottom:4px; }
.topic-card .sub { color:#8696a4; font-size:13px; }
.footer { text-align:center; padding:30px; color:#8696a4; font-size:12px; margin-top:30px; border-top:1px solid #1e1e2e; }
.overview { margin-bottom:28px; }
.overview p { color:#c4c6cb; font-size:15px; line-height:1.8; margin-bottom:14px; }
.terms-grid { display:grid; gap:10px; margin-top:12px; }
.term-card { background:#111119; border:1px solid #1e1e2e; border-radius:10px; padding:14px 18px; }
.term-card strong { color:#6ab2f2; font-size:14px; }
.term-card span { color:#c4c6cb; font-size:13px; display:block; margin-top:4px; line-height:1.5; }
.tip-box { background:linear-gradient(135deg, rgba(106,178,242,0.06), rgba(106,178,242,0.02)); border:1px solid rgba(106,178,242,0.15); border-radius:10px; padding:16px 20px; margin-top:12px; }
.tip-box li { color:#c4c6cb; font-size:14px; line-height:1.6; margin-bottom:8px; list-style:none; }
.tip-box li::before { content:'üí° '; }
.detail-card { background:#111119; border-radius:10px; padding:16px 18px; margin-bottom:8px; border-left:3px solid #2b5278; }
.detail-card .dt { color:#e4e6eb; font-size:14px; font-weight:600; margin-bottom:4px; }
.detail-card .dd { color:#9ca3af; font-size:13px; line-height:1.6; }
.detail-card .dl { margin-top:6px; }
.detail-card .dl a { font-size:12px; color:#6ab2f2; background:rgba(106,178,242,0.08); padding:3px 10px; border-radius:6px; display:inline-block; margin-right:6px; margin-top:4px; }
.detail-card .dl a:hover { background:rgba(106,178,242,0.15); text-decoration:none; }
</style>
</head>
<body>
<div class="header">
  <a href="../../index.html" class="back">&larr; Back to Course</a>
  <h1>Basic Theory</h1>
  <a href="../../../uk/basic-theory/level-1/multimodality.html" class="lang-switch">üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</a>
</div>
<div class="container">
  <div class="breadcrumb"><a href="../../index.html">Course</a> / <a href="../index.html">Basic Theory</a> / <a href="index.html">Level 1</a> / Multimodality</div>
  <span class="level-badge level-1">üå± Level 1 ‚Äî Beginner</span>
  <h2>Multimodality</h2>
  <p class="desc">AI models that work with multiple data types simultaneously.</p>

  <div class="overview">
    <p>Multimodal AI refers to models that can process and generate multiple types of data ‚Äî text, images, audio, video ‚Äî within a single system. Rather than separate models for each data type, modern multimodal models understand the relationships between modalities, enabling powerful capabilities like describing images, answering questions about documents, or generating images from text.</p>
    <p>The trend toward multimodality is accelerating. GPT-4V, Claude Vision, and Gemini can all analyze images alongside text. Gemini processes audio and video natively. This convergence means that a single model can increasingly handle tasks that previously required specialized pipelines.</p>
  </div>
  <div class="section-title">Key Topics Covered</div>
  <div class="detail-cards">
    <div class="detail-card"><div class="dt">What is Multimodality</div><div class="dd">A single model processing and generating multiple data types ‚Äî text, images, audio, video ‚Äî within one unified system. Moving beyond text-only AI.</div><div class="dl"><a href="data-classification.html">Data Type Classification</a></div></div>
    <div class="detail-card"><div class="dt">Vision-Language Models</div><div class="dd">GPT-4V, Claude Vision (Sonnet/Opus), Gemini Pro Vision. These models &quot;see&quot; images and reason about them using natural language ‚Äî describing scenes, reading text, analyzing charts.</div><div class="dl"><a href="big-players.html">The Big Players</a></div></div>
    <div class="detail-card"><div class="dt">Image Understanding</div><div class="dd">Scene description, OCR (reading text from images), chart/diagram analysis, visual question answering. Models can analyze screenshots, photos, documents, and diagrams.</div></div>
    <div class="detail-card"><div class="dt">Audio Understanding &amp; Generation</div><div class="dd">Whisper (OpenAI) transcribes speech to text. TTS (text-to-speech) models synthesize natural voice. Voice cloning reproduces a specific person's voice from samples.</div></div>
    <div class="detail-card"><div class="dt">Document Understanding</div><div class="dd">Parsing complex layouts ‚Äî PDFs, invoices, handwritten text, multi-column documents. Combines OCR with language understanding for intelligent data extraction.</div></div>
    <div class="detail-card"><div class="dt">Cross-Modal Generation</div><div class="dd">Text-to-image (DALL-E, Midjourney), image-to-text (captioning), text-to-audio (TTS), audio-to-text (transcription). Converting between data types seamlessly.</div><div class="dl"><a href="diffusion-models.html">Diffusion Models</a></div></div>
    <div class="detail-card"><div class="dt">Video Understanding</div><div class="dd">Temporal analysis, action recognition, video QA ‚Äî understanding what happens across frames over time. More complex than single-image analysis.</div></div>
    <div class="detail-card"><div class="dt">Native vs Adapter Multimodality</div><div class="dd">Some models (Gemini) are natively multimodal from pre-training. Others bolt vision adapters onto existing text models. Native tends to be more capable and efficient.</div><div class="dl"><a href="foundation-models.html">Foundation Models</a></div></div>
    <div class="detail-card"><div class="dt">Gemini Approach</div><div class="dd">Google's Gemini was pre-trained natively on text + images + audio + video simultaneously. This gives deeper cross-modal understanding compared to adapter-based approaches.</div><div class="dl"><a href="big-players.html">The Big Players</a></div></div>
    <div class="detail-card"><div class="dt">Real-World Applications</div><div class="dd">Accessibility tools (image descriptions for blind users), content moderation (detecting harmful images), medical imaging, autonomous driving, document processing pipelines.</div></div>
  </div>

  <div class="section-title">Key Terms</div>
  <div class="terms-grid">
    <div class="term-card"><strong>Modality</strong><span>A type of data input/output: text, image, audio, video, or code.</span></div>
    <div class="term-card"><strong>Vision-Language Model</strong><span>Model that can understand and reason about images alongside text.</span></div>
    <div class="term-card"><strong>OCR</strong><span>Optical Character Recognition ‚Äî extracting text from images of documents or screens.</span></div>
    <div class="term-card"><strong>Cross-Modal</strong><span>Converting between data types, e.g., generating an image from text description.</span></div>
  </div>

    <div class="section-title">Related Community Discussions</div>
    <div class="related-topics">
      <a href="#">Models</a>
      <a href="#">Feed</a>
    </div>

  <div class="nav-links">
    <a href="diffusion-models.html">&larr; Diffusion Models</a>
    <a href="reasoning.html">Reasoning &rarr;</a>
  </div>
</div>
<div class="footer"><a href="../../index.html">&larr; Back to Course</a></div>
</body>
</html>