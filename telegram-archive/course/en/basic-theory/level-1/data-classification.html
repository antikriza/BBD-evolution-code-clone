<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Data Type Classification - Basic Theory</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background:#0a0a0f; color:#e4e6eb; }
a { color:#6ab2f2; text-decoration:none; }
a:hover { text-decoration:underline; }
.header { background:#111119; padding:16px 24px; border-bottom:1px solid #1e1e2e; display:flex; align-items:center; gap:16px; position:sticky; top:0; z-index:10; }
.header h1 { font-size:18px; color:#6ab2f2; }
.header .back { color:#8696a4; font-size:14px; }
.header .lang-switch { margin-left:auto; font-size:13px; color:#8696a4; }
.container { max-width:900px; margin:0 auto; padding:24px; }
.breadcrumb { color:#8696a4; font-size:13px; margin-bottom:20px; }
.breadcrumb a { color:#6ab2f2; }
.level-badge { display:inline-block; padding:4px 12px; border-radius:20px; font-size:12px; font-weight:600; margin-bottom:16px; }
.level-1 { background:rgba(74,222,128,0.15); color:#4ade80; }
.level-2 { background:rgba(96,165,250,0.15); color:#60a5fa; }
.level-3 { background:rgba(245,158,11,0.15); color:#f59e0b; }
.level-4 { background:rgba(239,68,68,0.15); color:#ef4444; }
.level-5 { background:rgba(168,85,247,0.15); color:#a855f7; }
h2 { font-size:28px; margin-bottom:8px; }
.desc { color:#8696a4; font-size:15px; line-height:1.7; margin-bottom:24px; }
.detail-list { list-style:none; padding:0; }
.detail-list li { padding:10px 16px; margin-bottom:6px; border-radius:8px; background:#111119; border-left:3px solid #2b5278; font-size:14px; line-height:1.5; color:#e4e6eb; }
.detail-list li::before { content:'\2192 '; color:#6ab2f2; }
.section-title { font-size:16px; color:#8696a4; margin:24px 0 12px; text-transform:uppercase; letter-spacing:1px; }
.related-topics { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
.related-topics a { display:inline-block; background:#111119; border:1px solid #1e1e2e; padding:6px 14px; border-radius:8px; font-size:13px; color:#6ab2f2; }
.related-topics a:hover { background:#1e1e2e; text-decoration:none; }
.nav-links { display:flex; justify-content:space-between; margin-top:30px; padding-top:20px; border-top:1px solid #1e1e2e; }
.nav-links a { color:#6ab2f2; font-size:14px; }
.nav-links .disabled { color:#333; }
.topic-grid { display:grid; grid-template-columns:repeat(auto-fill, minmax(260px, 1fr)); gap:12px; margin-top:16px; }
.topic-card { background:#111119; border-radius:12px; padding:18px; border-left:3px solid #2b5278; display:block; transition:background 0.15s; }
.topic-card:hover { background:#1a1a2e; text-decoration:none; }
.topic-card h3 { color:#e4e6eb; font-size:15px; margin-bottom:4px; }
.topic-card .sub { color:#8696a4; font-size:13px; }
.footer { text-align:center; padding:30px; color:#8696a4; font-size:12px; margin-top:30px; border-top:1px solid #1e1e2e; }
.overview { margin-bottom:28px; }
.overview p { color:#c4c6cb; font-size:15px; line-height:1.8; margin-bottom:14px; }
.terms-grid { display:grid; gap:10px; margin-top:12px; }
.term-card { background:#111119; border:1px solid #1e1e2e; border-radius:10px; padding:14px 18px; }
.term-card strong { color:#6ab2f2; font-size:14px; }
.term-card span { color:#c4c6cb; font-size:13px; display:block; margin-top:4px; line-height:1.5; }
.tip-box { background:linear-gradient(135deg, rgba(106,178,242,0.06), rgba(106,178,242,0.02)); border:1px solid rgba(106,178,242,0.15); border-radius:10px; padding:16px 20px; margin-top:12px; }
.tip-box li { color:#c4c6cb; font-size:14px; line-height:1.6; margin-bottom:8px; list-style:none; }
.tip-box li::before { content:'üí° '; }
.detail-card { background:#111119; border-radius:10px; padding:16px 18px; margin-bottom:8px; border-left:3px solid #2b5278; }
.detail-card .dt { color:#e4e6eb; font-size:14px; font-weight:600; margin-bottom:4px; }
.detail-card .dd { color:#9ca3af; font-size:13px; line-height:1.6; }
.detail-card .dl { margin-top:6px; }
.detail-card .dl a { font-size:12px; color:#6ab2f2; background:rgba(106,178,242,0.08); padding:3px 10px; border-radius:6px; display:inline-block; margin-right:6px; margin-top:4px; }
.detail-card .dl a:hover { background:rgba(106,178,242,0.15); text-decoration:none; }
</style>
</head>
<body>
<div class="header">
  <a href="../../index.html" class="back">&larr; Back to Course</a>
  <h1>Basic Theory</h1>
  <a href="../../../uk/basic-theory/level-1/data-classification.html" class="lang-switch">üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</a>
</div>
<div class="container">
  <div class="breadcrumb"><a href="../../index.html">Course</a> / <a href="../index.html">Basic Theory</a> / <a href="index.html">Level 1</a> / Data Type Classification</div>
  <span class="level-badge level-1">üå± Level 1 ‚Äî Beginner</span>
  <h2>Data Type Classification</h2>
  <p class="desc">Categorizing AI models by what data types they handle as input and output.</p>

  <div class="overview">
    <p>AI models can be categorized by the types of data they process as inputs and produce as outputs. Understanding this classification helps you choose the right model for each task. A text-to-text model (LLM) handles different tasks than an image-to-text model (captioning) or a text-to-image model (diffusion).</p>
    <p>Modern models increasingly blur these boundaries ‚Äî multimodal foundation models can handle multiple data types in a single conversation. But understanding the underlying classification helps you design effective AI pipelines and choose appropriate APIs.</p>
  </div>
  <div class="section-title">Key Topics Covered</div>
  <div class="detail-cards">
    <div class="detail-card"><div class="dt">Text ‚Üí Text (LLMs)</div><div class="dd">Chat, writing, analysis, translation, summarization. Models: GPT-4, Claude, Gemini, Llama. The most mature and widely used category of generative AI.</div><div class="dl"><a href="llm-and-gpt.html">LLM and GPT</a></div></div>
    <div class="detail-card"><div class="dt">Text ‚Üí Image (Diffusion)</div><div class="dd">Generate images from text descriptions. Models: DALL-E 3, Midjourney, Stable Diffusion, Flux. Quality has improved from abstract art to photorealistic outputs in just 2 years.</div><div class="dl"><a href="diffusion-models.html">Diffusion Models</a></div></div>
    <div class="detail-card"><div class="dt">Image ‚Üí Text (Vision)</div><div class="dd">Captioning, OCR, visual question-answering. Models: GPT-4V, Claude Vision, Gemini Pro Vision. Enables AI to &quot;see&quot; and reason about images, screenshots, documents.</div><div class="dl"><a href="multimodality.html">Multimodality</a></div></div>
    <div class="detail-card"><div class="dt">Text ‚Üí Audio (TTS)</div><div class="dd">Voice synthesis from text. Models: ElevenLabs, OpenAI TTS, Bark. Modern TTS produces near-human quality speech with emotion, accents, and multiple languages.</div></div>
    <div class="detail-card"><div class="dt">Audio ‚Üí Text (Speech Recognition)</div><div class="dd">Transcription and speech-to-text. Models: Whisper, AssemblyAI, Deepgram. Enables voice interfaces, meeting transcription, and accessibility features.</div></div>
    <div class="detail-card"><div class="dt">Text ‚Üí Video</div><div class="dd">Generate video clips from text descriptions. Models: Sora, Runway Gen-3, Kling, Pika. The newest frontier ‚Äî quality is improving rapidly but still limited to short clips.</div></div>
    <div class="detail-card"><div class="dt">Text ‚Üí Code</div><div class="dd">Code generation and completion from natural language. Models: GPT-4, Claude, Codex, StarCoder. Powers tools like GitHub Copilot, Cursor, and Claude Code.</div><div class="dl"><a href="../level-2/vibecoding.html">Vibecoding</a></div></div>
    <div class="detail-card"><div class="dt">Code ‚Üí Text</div><div class="dd">Code explanation, documentation generation, and review. All major LLMs excel at reading and explaining code, making it one of the highest-value AI applications.</div></div>
    <div class="detail-card"><div class="dt">Image ‚Üí Image</div><div class="dd">Image editing, style transfer, super-resolution, inpainting. Models: ControlNet, Instruct-Pix2Pix. Transform existing images rather than generating from scratch.</div><div class="dl"><a href="diffusion-models.html">Diffusion Models</a></div></div>
    <div class="detail-card"><div class="dt">Audio ‚Üí Audio</div><div class="dd">Voice conversion, music remixing, noise removal, audio enhancement. Specialized models that transform audio inputs without going through text as an intermediate.</div></div>
  </div>

  <div class="section-title">Key Terms</div>
  <div class="terms-grid">
    <div class="term-card"><strong>Modality</strong><span>The type of data a model works with: text, image, audio, video, or code.</span></div>
    <div class="term-card"><strong>Pipeline</strong><span>A chain of models processing data, e.g., audio‚Üítext‚Üítext‚Üíaudio for a voice chatbot.</span></div>
    <div class="term-card"><strong>Embedding</strong><span>A numerical representation of data (text, image) in a vector space, enabling semantic search.</span></div>
  </div>

    <div class="section-title">Related Community Discussions</div>
    <div class="related-topics">
      <a href="#">Models</a>
    </div>

  <div class="nav-links">
    <a href="foundation-models.html">&larr; Foundation Models</a>
    <a href="sota.html">State of the Art (SOTA) &rarr;</a>
  </div>
</div>
<div class="footer"><a href="../../index.html">&larr; Back to Course</a></div>
</body>
</html>