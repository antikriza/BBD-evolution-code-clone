<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Context - Basic Theory</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background:#0a0a0f; color:#e4e6eb; }
a { color:#6ab2f2; text-decoration:none; }
a:hover { text-decoration:underline; }
.header { background:#111119; padding:16px 24px; border-bottom:1px solid #1e1e2e; display:flex; align-items:center; gap:16px; position:sticky; top:0; z-index:10; }
.header h1 { font-size:18px; color:#6ab2f2; }
.header .back { color:#8696a4; font-size:14px; }
.header .lang-switch { margin-left:auto; font-size:13px; color:#8696a4; }
.container { max-width:900px; margin:0 auto; padding:24px; }
.breadcrumb { color:#8696a4; font-size:13px; margin-bottom:20px; }
.breadcrumb a { color:#6ab2f2; }
.level-badge { display:inline-block; padding:4px 12px; border-radius:20px; font-size:12px; font-weight:600; margin-bottom:16px; }
.level-1 { background:rgba(74,222,128,0.15); color:#4ade80; }
.level-2 { background:rgba(96,165,250,0.15); color:#60a5fa; }
.level-3 { background:rgba(245,158,11,0.15); color:#f59e0b; }
.level-4 { background:rgba(239,68,68,0.15); color:#ef4444; }
.level-5 { background:rgba(168,85,247,0.15); color:#a855f7; }
h2 { font-size:28px; margin-bottom:8px; }
.desc { color:#8696a4; font-size:15px; line-height:1.7; margin-bottom:24px; }
.detail-list { list-style:none; padding:0; }
.detail-list li { padding:10px 16px; margin-bottom:6px; border-radius:8px; background:#111119; border-left:3px solid #2b5278; font-size:14px; line-height:1.5; color:#e4e6eb; }
.detail-list li::before { content:'\2192 '; color:#6ab2f2; }
.section-title { font-size:16px; color:#8696a4; margin:24px 0 12px; text-transform:uppercase; letter-spacing:1px; }
.related-topics { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
.related-topics a { display:inline-block; background:#111119; border:1px solid #1e1e2e; padding:6px 14px; border-radius:8px; font-size:13px; color:#6ab2f2; }
.related-topics a:hover { background:#1e1e2e; text-decoration:none; }
.nav-links { display:flex; justify-content:space-between; margin-top:30px; padding-top:20px; border-top:1px solid #1e1e2e; }
.nav-links a { color:#6ab2f2; font-size:14px; }
.nav-links .disabled { color:#333; }
.topic-grid { display:grid; grid-template-columns:repeat(auto-fill, minmax(260px, 1fr)); gap:12px; margin-top:16px; }
.topic-card { background:#111119; border-radius:12px; padding:18px; border-left:3px solid #2b5278; display:block; transition:background 0.15s; }
.topic-card:hover { background:#1a1a2e; text-decoration:none; }
.topic-card h3 { color:#e4e6eb; font-size:15px; margin-bottom:4px; }
.topic-card .sub { color:#8696a4; font-size:13px; }
.footer { text-align:center; padding:30px; color:#8696a4; font-size:12px; margin-top:30px; border-top:1px solid #1e1e2e; }
.overview { margin-bottom:28px; }
.overview p { color:#c4c6cb; font-size:15px; line-height:1.8; margin-bottom:14px; }
.terms-grid { display:grid; gap:10px; margin-top:12px; }
.term-card { background:#111119; border:1px solid #1e1e2e; border-radius:10px; padding:14px 18px; }
.term-card strong { color:#6ab2f2; font-size:14px; }
.term-card span { color:#c4c6cb; font-size:13px; display:block; margin-top:4px; line-height:1.5; }
.tip-box { background:linear-gradient(135deg, rgba(106,178,242,0.06), rgba(106,178,242,0.02)); border:1px solid rgba(106,178,242,0.15); border-radius:10px; padding:16px 20px; margin-top:12px; }
.tip-box li { color:#c4c6cb; font-size:14px; line-height:1.6; margin-bottom:8px; list-style:none; }
.tip-box li::before { content:'üí° '; }
.detail-card { background:#111119; border-radius:10px; padding:16px 18px; margin-bottom:8px; border-left:3px solid #2b5278; }
.detail-card .dt { color:#e4e6eb; font-size:14px; font-weight:600; margin-bottom:4px; }
.detail-card .dd { color:#9ca3af; font-size:13px; line-height:1.6; }
.detail-card .dl { margin-top:6px; }
.detail-card .dl a { font-size:12px; color:#6ab2f2; background:rgba(106,178,242,0.08); padding:3px 10px; border-radius:6px; display:inline-block; margin-right:6px; margin-top:4px; }
.detail-card .dl a:hover { background:rgba(106,178,242,0.15); text-decoration:none; }
</style>
</head>
<body>
<div class="header">
  <a href="../../index.html" class="back">&larr; Back to Course</a>
  <h1>Basic Theory</h1>
  <a href="../../../uk/basic-theory/level-2/context.html" class="lang-switch">üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</a>
</div>
<div class="container">
  <div class="breadcrumb"><a href="../../index.html">Course</a> / <a href="../index.html">Basic Theory</a> / <a href="index.html">Level 2</a> / Context</div>
  <span class="level-badge level-2">üí° Level 2 ‚Äî User</span>
  <h2>Context</h2>
  <p class="desc">Context windows, how models use context, and managing context effectively.</p>

  <div class="overview">
    <p>The context window is the total amount of text (measured in tokens) that a model can process in a single request ‚Äî including both your input and the model's output. Think of it as the model's &quot;working memory.&quot; Anything outside the context window simply doesn't exist for the model.</p>
    <p>Context windows have grown dramatically: from 4K tokens in early GPT-3.5 to 200K (Claude) and 1M+ (Gemini). But bigger isn't always better ‚Äî models often struggle to effectively use information in the middle of very long contexts. Understanding these dynamics is key to building effective AI applications.</p>
  </div>
  <div class="section-title">Key Topics Covered</div>
  <div class="detail-cards">
    <div class="detail-card"><div class="dt">What Is a Context Window</div><div class="dd">The total tokens (input + output) a model processes in one request ‚Äî its working memory. Everything outside the context window simply doesn't exist for the model.</div><div class="dl"><a href="token.html">Token</a></div></div>
    <div class="detail-card"><div class="dt">Context Window Sizes</div><div class="dd">GPT-4 (128K), Claude (200K), Gemini (1M+), open models (8K-128K). Bigger context means more information available, but cost and latency increase with context size.</div><div class="dl"><a href="../level-1/big-players.html">The Big Players</a></div></div>
    <div class="detail-card"><div class="dt">How Attention Works</div><div class="dd">Each token &quot;attends&quot; to every other token ‚Äî computational cost grows quadratically (O(n^2)). This is why very long contexts are expensive and why efficient attention methods matter.</div><div class="dl"><a href="../level-3/neural-networks.html">Neural Networks</a></div></div>
    <div class="detail-card"><div class="dt">Lost-in-the-Middle Problem</div><div class="dd">Models attend better to the beginning and end of context than the middle. Important information placed in the middle of a long context may be overlooked or given less weight.</div></div>
    <div class="detail-card"><div class="dt">Context Management Strategies</div><div class="dd">Summarization (compress older messages), chunking (process documents in pieces), prioritization (put most relevant info first/last). Essential skills for building production AI apps.</div></div>
    <div class="detail-card"><div class="dt">RAG (Retrieval-Augmented Generation)</div><div class="dd">Pull relevant documents into context on demand rather than stuffing everything in. A search retrieves the most relevant chunks, which are then added to the prompt before generation.</div><div class="dl"><a href="../level-4/rag.html">RAG</a></div></div>
    <div class="detail-card"><div class="dt">Conversation Memory</div><div class="dd">Chatbots simulate long-term memory by managing context: summarizing old messages, maintaining key facts, and selectively including relevant history in each new request.</div></div>
    <div class="detail-card"><div class="dt">Context Engineering</div><div class="dd">Deliberate structuring of what goes into the context window ‚Äî what to include, what to summarize, what to omit. Arguably more important than prompt engineering for complex applications.</div><div class="dl"><a href="prompt.html">Prompt</a></div></div>
    <div class="detail-card"><div class="dt">Sliding Window Processing</div><div class="dd">For documents longer than the context window, process in overlapping chunks that &quot;slide&quot; through the content. Each chunk shares some overlap with the previous for continuity.</div></div>
    <div class="detail-card"><div class="dt">Multi-Turn Conversation Costs</div><div class="dd">Each message in a conversation consumes context. As conversations grow, old messages get truncated or summarized. Understanding this helps you design chatbots that remain coherent over time.</div><div class="dl"><a href="token.html">Token</a></div></div>
  </div>

  <div class="section-title">Key Terms</div>
  <div class="terms-grid">
    <div class="term-card"><strong>Context Window</strong><span>Maximum tokens a model processes at once ‚Äî its working memory for a single request.</span></div>
    <div class="term-card"><strong>Lost-in-the-Middle</strong><span>Models attend better to start and end of context, often missing information in the middle.</span></div>
    <div class="term-card"><strong>RAG</strong><span>Retrieval-Augmented Generation ‚Äî dynamically retrieving relevant documents to add to the context.</span></div>
    <div class="term-card"><strong>Context Engineering</strong><span>The practice of deliberately structuring and managing what information enters the model's context.</span></div>
  </div>

  <div class="section-title">Practical Tips</div>
  <div class="tip-box"><ul>
    <li>Put the most important information at the beginning and end of your prompt ‚Äî models pay less attention to the middle (the &quot;lost in the middle&quot; effect)</li>
    <li>For long-context tasks, chunk your input and summarize irrelevant sections rather than feeding everything in ‚Äî focused context produces better results than exhaustive context</li>
    <li>Use RAG instead of stuffing everything into context ‚Äî retrieve only what's relevant for the specific query, even if the model has a huge context window</li>
  </ul></div>

    <div class="section-title">Related Community Discussions</div>
    <div class="related-topics">
      <a href="#">Agents &amp; Tools</a>
      <a href="#">Feed</a>
    </div>

  <div class="nav-links">
    <a href="token.html">&larr; Token</a>
    <a href="hallucination.html">Hallucinations &rarr;</a>
  </div>
</div>
<div class="footer"><a href="../../index.html">&larr; Back to Course</a></div>
</body>
</html>