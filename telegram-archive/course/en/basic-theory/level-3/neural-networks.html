<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Neural Network Fundamentals - Basic Theory</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background:#0a0a0f; color:#e4e6eb; }
a { color:#6ab2f2; text-decoration:none; }
a:hover { text-decoration:underline; }
.header { background:#111119; padding:16px 24px; border-bottom:1px solid #1e1e2e; display:flex; align-items:center; gap:16px; position:sticky; top:0; z-index:10; }
.header h1 { font-size:18px; color:#6ab2f2; }
.header .back { color:#8696a4; font-size:14px; }
.header .lang-switch { margin-left:auto; font-size:13px; color:#8696a4; }
.container { max-width:900px; margin:0 auto; padding:24px; }
.breadcrumb { color:#8696a4; font-size:13px; margin-bottom:20px; }
.breadcrumb a { color:#6ab2f2; }
.level-badge { display:inline-block; padding:4px 12px; border-radius:20px; font-size:12px; font-weight:600; margin-bottom:16px; }
.level-1 { background:rgba(74,222,128,0.15); color:#4ade80; }
.level-2 { background:rgba(96,165,250,0.15); color:#60a5fa; }
.level-3 { background:rgba(245,158,11,0.15); color:#f59e0b; }
.level-4 { background:rgba(239,68,68,0.15); color:#ef4444; }
.level-5 { background:rgba(168,85,247,0.15); color:#a855f7; }
h2 { font-size:28px; margin-bottom:8px; }
.desc { color:#8696a4; font-size:15px; line-height:1.7; margin-bottom:24px; }
.detail-list { list-style:none; padding:0; }
.detail-list li { padding:10px 16px; margin-bottom:6px; border-radius:8px; background:#111119; border-left:3px solid #2b5278; font-size:14px; line-height:1.5; color:#e4e6eb; }
.detail-list li::before { content:'\2192 '; color:#6ab2f2; }
.section-title { font-size:16px; color:#8696a4; margin:24px 0 12px; text-transform:uppercase; letter-spacing:1px; }
.related-topics { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
.related-topics a { display:inline-block; background:#111119; border:1px solid #1e1e2e; padding:6px 14px; border-radius:8px; font-size:13px; color:#6ab2f2; }
.related-topics a:hover { background:#1e1e2e; text-decoration:none; }
.nav-links { display:flex; justify-content:space-between; margin-top:30px; padding-top:20px; border-top:1px solid #1e1e2e; }
.nav-links a { color:#6ab2f2; font-size:14px; }
.nav-links .disabled { color:#333; }
.topic-grid { display:grid; grid-template-columns:repeat(auto-fill, minmax(260px, 1fr)); gap:12px; margin-top:16px; }
.topic-card { background:#111119; border-radius:12px; padding:18px; border-left:3px solid #2b5278; display:block; transition:background 0.15s; }
.topic-card:hover { background:#1a1a2e; text-decoration:none; }
.topic-card h3 { color:#e4e6eb; font-size:15px; margin-bottom:4px; }
.topic-card .sub { color:#8696a4; font-size:13px; }
.footer { text-align:center; padding:30px; color:#8696a4; font-size:12px; margin-top:30px; border-top:1px solid #1e1e2e; }
.overview { margin-bottom:28px; }
.overview p { color:#c4c6cb; font-size:15px; line-height:1.8; margin-bottom:14px; }
.terms-grid { display:grid; gap:10px; margin-top:12px; }
.term-card { background:#111119; border:1px solid #1e1e2e; border-radius:10px; padding:14px 18px; }
.term-card strong { color:#6ab2f2; font-size:14px; }
.term-card span { color:#c4c6cb; font-size:13px; display:block; margin-top:4px; line-height:1.5; }
.tip-box { background:linear-gradient(135deg, rgba(106,178,242,0.06), rgba(106,178,242,0.02)); border:1px solid rgba(106,178,242,0.15); border-radius:10px; padding:16px 20px; margin-top:12px; }
.tip-box li { color:#c4c6cb; font-size:14px; line-height:1.6; margin-bottom:8px; list-style:none; }
.tip-box li::before { content:'üí° '; }
.detail-card { background:#111119; border-radius:10px; padding:16px 18px; margin-bottom:8px; border-left:3px solid #2b5278; }
.detail-card .dt { color:#e4e6eb; font-size:14px; font-weight:600; margin-bottom:4px; }
.detail-card .dd { color:#9ca3af; font-size:13px; line-height:1.6; }
.detail-card .dl { margin-top:6px; }
.detail-card .dl a { font-size:12px; color:#6ab2f2; background:rgba(106,178,242,0.08); padding:3px 10px; border-radius:6px; display:inline-block; margin-right:6px; margin-top:4px; }
.detail-card .dl a:hover { background:rgba(106,178,242,0.15); text-decoration:none; }
</style>
</head>
<body>
<div class="header">
  <a href="../../index.html" class="back">&larr; Back to Course</a>
  <h1>Basic Theory</h1>
  <a href="../../../uk/basic-theory/level-3/neural-networks.html" class="lang-switch">üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</a>
</div>
<div class="container">
  <div class="breadcrumb"><a href="../../index.html">Course</a> / <a href="../index.html">Basic Theory</a> / <a href="index.html">Level 3</a> / Neural Network Fundamentals</div>
  <span class="level-badge level-3">‚ö° Level 3 ‚Äî Professional</span>
  <h2>Neural Network Fundamentals</h2>
  <p class="desc">Architecture of neural networks - layers, activation functions, and how learning happens.</p>

  <div class="overview">
    <p>Neural networks are the mathematical foundation underlying all modern AI. They are loosely inspired by biological neurons but in practice are systems of matrix multiplications and nonlinear functions organized into layers. Understanding how they work ‚Äî forward propagation, loss computation, and backpropagation ‚Äî is essential for anyone wanting to go beyond surface-level AI usage.</p>
    <p>The field has evolved from simple perceptrons in the 1950s to today's trillion-parameter transformer networks. Each architectural breakthrough ‚Äî from convolutional layers for vision to attention mechanisms for language ‚Äî expanded what neural networks could do. Knowing these fundamentals helps you understand why certain models excel at certain tasks and what the actual limitations of &quot;AI&quot; are.</p>
  </div>
  <div class="section-title">Key Topics Covered</div>
  <div class="detail-cards">
    <div class="detail-card"><div class="dt">Neurons and Weighted Sums</div><div class="dd">A neuron computes a weighted sum of its inputs, adds a bias term, then passes the result through an activation function. This simple operation, repeated billions of times across layers, is how neural networks compute.</div><div class="dl"><a href="../level-1/foundation-models.html">Foundation Models</a></div></div>
    <div class="detail-card"><div class="dt">Network Layers</div><div class="dd">Input layer receives raw data, hidden layers extract increasingly abstract features, output layer produces the final prediction. &quot;Deep&quot; learning means many hidden layers ‚Äî modern LLMs have 80-120+ layers.</div><div class="dl"><a href="../level-2/llm.html">LLM</a></div></div>
    <div class="detail-card"><div class="dt">Activation Functions</div><div class="dd">ReLU (most common, simple max(0,x)), GELU (used in transformers, smoother), Sigmoid (squashes to 0-1), Softmax (outputs probability distribution). These introduce nonlinearity ‚Äî without them, the entire network would collapse to a single linear transformation.</div></div>
    <div class="detail-card"><div class="dt">Forward Propagation</div><div class="dd">Data flows through the network layer by layer ‚Äî each layer transforms its input and passes the result to the next. The final output is a prediction that can be compared to the true answer to compute error.</div></div>
    <div class="detail-card"><div class="dt">Loss Functions</div><div class="dd">Measuring how wrong the prediction is. Cross-entropy loss for classification, MSE for regression, next-token prediction loss for language models. The entire training process is about minimizing this loss function.</div><div class="dl"><a href="data-to-model.html">Data to Model</a></div></div>
    <div class="detail-card"><div class="dt">Backpropagation</div><div class="dd">The algorithm that makes learning possible. It computes how much each weight contributed to the error by applying the chain rule of calculus backwards through the network ‚Äî hence &quot;back&quot; propagation.</div><div class="dl"><a href="training-finetuning.html">Training &amp; Fine-tuning</a></div></div>
    <div class="detail-card"><div class="dt">Gradient Descent Optimization</div><div class="dd">Adjusting weights in the direction that reduces loss. Adam optimizer (used by almost all modern models) adapts learning rates per-parameter. Learning rate scheduling, warmup, and weight decay are critical training hyperparameters.</div><div class="dl"><a href="model-optimization.html">Model Optimization</a></div></div>
    <div class="detail-card"><div class="dt">Convolutional Networks (CNNs)</div><div class="dd">Specialized for spatial data like images. Convolutional filters slide across the input detecting edges, textures, and patterns. Still used in vision AI but increasingly replaced by Vision Transformers (ViT).</div><div class="dl"><a href="../level-1/multimodal-ai.html">Multimodal AI</a></div></div>
    <div class="detail-card"><div class="dt">Recurrent Networks (RNNs, LSTMs)</div><div class="dd">Designed for sequential data ‚Äî text, time series, audio. They maintain a hidden state that carries information across time steps. Largely replaced by Transformers which process sequences in parallel.</div><div class="dl"><a href="model-types.html">Model Types</a></div></div>
    <div class="detail-card"><div class="dt">The Transformer Architecture</div><div class="dd">The 2017 breakthrough that powers all modern LLMs. Self-attention allows each token to attend to every other token in the sequence, capturing long-range dependencies that RNNs struggled with. Multi-head attention runs several attention computations in parallel.</div><div class="dl"><a href="model-types.html">Model Types</a></div></div>
  </div>

  <div class="section-title">Key Terms</div>
  <div class="terms-grid">
    <div class="term-card"><strong>Backpropagation</strong><span>Algorithm for computing how each weight contributes to the error by applying the chain rule backwards through the network.</span></div>
    <div class="term-card"><strong>Gradient Descent</strong><span>Optimization algorithm that iteratively adjusts weights in the direction that reduces error, using computed gradients.</span></div>
    <div class="term-card"><strong>Self-Attention</strong><span>Mechanism where each element in a sequence computes relevance scores with every other element, enabling context-aware processing.</span></div>
    <div class="term-card"><strong>Activation Function</strong><span>Nonlinear function applied after weighted sums ‚Äî without it, neural networks could only model linear relationships.</span></div>
  </div>

  <div class="section-title">Practical Tips</div>
  <div class="tip-box"><ul>
    <li>You don't need to code neural networks from scratch ‚Äî but understanding backpropagation and gradient descent explains why models sometimes fail to learn</li>
    <li>When a model &quot;forgets&quot; things during fine-tuning (catastrophic forgetting), it's because gradient updates for new data override weights learned for old data</li>
    <li>Visualize neural network layers with tools like Netron or TensorBoard to build intuition about how data transforms through the architecture</li>
  </ul></div>

    <div class="section-title">Related Community Discussions</div>
    <div class="related-topics">
      <a href="#">Models</a>
      <a href="#">Video Content</a>
    </div>

  <div class="nav-links">
    <span class="disabled">&larr;</span>
    <a href="data-to-model.html">Data to Model &rarr;</a>
  </div>
</div>
<div class="footer"><a href="../../index.html">&larr; Back to Course</a></div>
</body>
</html>