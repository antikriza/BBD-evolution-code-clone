<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Model Optimization - Basic Theory</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background:#0a0a0f; color:#e4e6eb; }
a { color:#6ab2f2; text-decoration:none; }
a:hover { text-decoration:underline; }
.header { background:#111119; padding:16px 24px; border-bottom:1px solid #1e1e2e; display:flex; align-items:center; gap:16px; position:sticky; top:0; z-index:10; }
.header h1 { font-size:18px; color:#6ab2f2; }
.header .back { color:#8696a4; font-size:14px; }
.header .lang-switch { margin-left:auto; font-size:13px; color:#8696a4; }
.container { max-width:900px; margin:0 auto; padding:24px; }
.breadcrumb { color:#8696a4; font-size:13px; margin-bottom:20px; }
.breadcrumb a { color:#6ab2f2; }
.level-badge { display:inline-block; padding:4px 12px; border-radius:20px; font-size:12px; font-weight:600; margin-bottom:16px; }
.level-1 { background:rgba(74,222,128,0.15); color:#4ade80; }
.level-2 { background:rgba(96,165,250,0.15); color:#60a5fa; }
.level-3 { background:rgba(245,158,11,0.15); color:#f59e0b; }
.level-4 { background:rgba(239,68,68,0.15); color:#ef4444; }
.level-5 { background:rgba(168,85,247,0.15); color:#a855f7; }
h2 { font-size:28px; margin-bottom:8px; }
.desc { color:#8696a4; font-size:15px; line-height:1.7; margin-bottom:24px; }
.detail-list { list-style:none; padding:0; }
.detail-list li { padding:10px 16px; margin-bottom:6px; border-radius:8px; background:#111119; border-left:3px solid #2b5278; font-size:14px; line-height:1.5; color:#e4e6eb; }
.detail-list li::before { content:'\2192 '; color:#6ab2f2; }
.section-title { font-size:16px; color:#8696a4; margin:24px 0 12px; text-transform:uppercase; letter-spacing:1px; }
.related-topics { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
.related-topics a { display:inline-block; background:#111119; border:1px solid #1e1e2e; padding:6px 14px; border-radius:8px; font-size:13px; color:#6ab2f2; }
.related-topics a:hover { background:#1e1e2e; text-decoration:none; }
.nav-links { display:flex; justify-content:space-between; margin-top:30px; padding-top:20px; border-top:1px solid #1e1e2e; }
.nav-links a { color:#6ab2f2; font-size:14px; }
.nav-links .disabled { color:#333; }
.topic-grid { display:grid; grid-template-columns:repeat(auto-fill, minmax(260px, 1fr)); gap:12px; margin-top:16px; }
.topic-card { background:#111119; border-radius:12px; padding:18px; border-left:3px solid #2b5278; display:block; transition:background 0.15s; }
.topic-card:hover { background:#1a1a2e; text-decoration:none; }
.topic-card h3 { color:#e4e6eb; font-size:15px; margin-bottom:4px; }
.topic-card .sub { color:#8696a4; font-size:13px; }
.footer { text-align:center; padding:30px; color:#8696a4; font-size:12px; margin-top:30px; border-top:1px solid #1e1e2e; }
.overview { margin-bottom:28px; }
.overview p { color:#c4c6cb; font-size:15px; line-height:1.8; margin-bottom:14px; }
.terms-grid { display:grid; gap:10px; margin-top:12px; }
.term-card { background:#111119; border:1px solid #1e1e2e; border-radius:10px; padding:14px 18px; }
.term-card strong { color:#6ab2f2; font-size:14px; }
.term-card span { color:#c4c6cb; font-size:13px; display:block; margin-top:4px; line-height:1.5; }
.tip-box { background:linear-gradient(135deg, rgba(106,178,242,0.06), rgba(106,178,242,0.02)); border:1px solid rgba(106,178,242,0.15); border-radius:10px; padding:16px 20px; margin-top:12px; }
.tip-box li { color:#c4c6cb; font-size:14px; line-height:1.6; margin-bottom:8px; list-style:none; }
.tip-box li::before { content:'üí° '; }
.detail-card { background:#111119; border-radius:10px; padding:16px 18px; margin-bottom:8px; border-left:3px solid #2b5278; }
.detail-card .dt { color:#e4e6eb; font-size:14px; font-weight:600; margin-bottom:4px; }
.detail-card .dd { color:#9ca3af; font-size:13px; line-height:1.6; }
.detail-card .dl { margin-top:6px; }
.detail-card .dl a { font-size:12px; color:#6ab2f2; background:rgba(106,178,242,0.08); padding:3px 10px; border-radius:6px; display:inline-block; margin-right:6px; margin-top:4px; }
.detail-card .dl a:hover { background:rgba(106,178,242,0.15); text-decoration:none; }
</style>
</head>
<body>
<div class="header">
  <a href="../../index.html" class="back">&larr; Back to Course</a>
  <h1>Basic Theory</h1>
  <a href="../../../uk/basic-theory/level-3/model-optimization.html" class="lang-switch">üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</a>
</div>
<div class="container">
  <div class="breadcrumb"><a href="../../index.html">Course</a> / <a href="../index.html">Basic Theory</a> / <a href="index.html">Level 3</a> / Model Optimization</div>
  <span class="level-badge level-3">‚ö° Level 3 ‚Äî Professional</span>
  <h2>Model Optimization</h2>
  <p class="desc">Making models faster, smaller, and cheaper to run.</p>

  <div class="overview">
    <p>Running large AI models requires expensive hardware. Model optimization techniques reduce computational requirements while maintaining quality. Quantization (reducing numerical precision), pruning (removing unnecessary connections), and distillation (training smaller models from larger ones) make it possible to run models on consumer hardware that would otherwise require data center GPUs.</p>
    <p>Optimization is what makes AI practical. Without it, only companies with massive GPU clusters could use frontier models. Thanks to quantization and efficient inference engines like llama.cpp and vLLM, a 70B parameter model can run on a gaming PC, and inference costs have dropped 100x in two years. This democratization drives the open-source AI movement.</p>
  </div>
  <div class="section-title">Key Topics Covered</div>
  <div class="detail-cards">
    <div class="detail-card"><div class="dt">Quantization Basics</div><div class="dd">Reducing numerical precision from FP32 (32 bits per weight) ‚Üí FP16 ‚Üí INT8 ‚Üí INT4 shrinks model size 2-8x. Each step trades a small amount of quality for major size and speed improvements.</div><div class="dl"><a href="../level-4/model-formats.html">Model Formats</a></div></div>
    <div class="detail-card"><div class="dt">GPTQ and AWQ</div><div class="dd">GPU-optimized post-training quantization methods. GPTQ uses calibration data for higher quality. AWQ (Activation-aware Weight Quantization) preserves important weights based on activation patterns. Both enable fast GPU inference.</div><div class="dl"><a href="../level-4/hardware.html">Hardware</a></div></div>
    <div class="detail-card"><div class="dt">GGUF and llama.cpp</div><div class="dd">GGUF is the format for llama.cpp, enabling mixed CPU+GPU inference on consumer machines. Supports quantization from Q2 (smallest) to Q8 (highest quality). Q4_K_M is the popular sweet spot.</div><div class="dl"><a href="../level-4/model-formats.html">Model Formats</a></div></div>
    <div class="detail-card"><div class="dt">Pruning</div><div class="dd">Removing weights close to zero that contribute little to output quality. Structured pruning removes entire neurons or attention heads. Can reduce model size 50-90% with careful calibration.</div><div class="dl"><a href="neural-networks.html">Neural Networks</a></div></div>
    <div class="detail-card"><div class="dt">Knowledge Distillation</div><div class="dd">Training a small &quot;student&quot; model from a large &quot;teacher&quot; model. The student learns from the teacher's probability distributions, not just hard labels, capturing richer information. GPT-4 distilled into GPT-4o mini is a practical example.</div><div class="dl"><a href="../level-1/foundation-models.html">Foundation Models</a></div></div>
    <div class="detail-card"><div class="dt">Flash Attention</div><div class="dd">Memory-efficient attention computation by Tri Dao that fuses operations and uses tiling to avoid materializing the full attention matrix. Reduces GPU memory usage 5-20x and speeds up training by 2-4x.</div><div class="dl"><a href="model-types.html">Model Types</a></div></div>
    <div class="detail-card"><div class="dt">Speculative Decoding</div><div class="dd">Using a fast small model to draft multiple tokens at once, then the large model verifies them in a single forward pass. Achieves 2-3x speedup without quality loss since rejected tokens are regenerated.</div><div class="dl"><a href="../level-2/llm.html">LLM</a></div></div>
    <div class="detail-card"><div class="dt">KV Cache Optimization</div><div class="dd">During generation, each token must attend to all previous tokens. KV cache stores these attention states but grows with sequence length. PagedAttention (vLLM) manages this memory like virtual memory in operating systems.</div><div class="dl"><a href="../level-2/context.html">Context</a></div></div>
    <div class="detail-card"><div class="dt">Model Merging</div><div class="dd">Combining weights from multiple fine-tuned models without additional training. Methods like TIES, DARE, and SLERP interpolate between model checkpoints. Community merges on HuggingFace often outperform individual fine-tunes.</div><div class="dl"><a href="training-finetuning.html">Training &amp; Fine-tuning</a></div></div>
    <div class="detail-card"><div class="dt">Practical Impact</div><div class="dd">A 70B model quantized to Q4 fits in 48GB VRAM (2x consumer GPUs). vLLM serves models 24x faster than naive inference. These optimizations mean a $2000 PC can run models that cost $100K+ to train.</div><div class="dl"><a href="../level-4/api-providers.html">API Providers</a></div></div>
  </div>

  <div class="section-title">Key Terms</div>
  <div class="terms-grid">
    <div class="term-card"><strong>Quantization</strong><span>Reducing numerical precision of model weights (FP32‚ÜíINT4) to decrease size 2-8x and speed up inference.</span></div>
    <div class="term-card"><strong>Distillation</strong><span>Training a smaller student model to mimic a larger teacher model's probability distributions and capabilities.</span></div>
    <div class="term-card"><strong>Flash Attention</strong><span>Memory-efficient attention implementation that avoids materializing the full attention matrix, reducing GPU memory 5-20x.</span></div>
    <div class="term-card"><strong>Speculative Decoding</strong><span>Using a fast draft model to generate candidate tokens, verified in batch by the larger model for 2-3x speedup.</span></div>
  </div>

  <div class="section-title">Practical Tips</div>
  <div class="tip-box"><ul>
    <li>For local model inference, start with GGUF Q4_K_M quantization ‚Äî it offers the best quality-to-size ratio for most use cases</li>
    <li>Use vLLM or TGI for serving models in production ‚Äî they implement PagedAttention and continuous batching for 10-24x throughput improvement</li>
    <li>Before optimizing, profile your bottleneck: is it memory (try quantization), compute (try smaller model or distillation), or latency (try speculative decoding)?</li>
  </ul></div>

    <div class="section-title">Related Community Discussions</div>
    <div class="related-topics">
      <a href="#">Models</a>
      <a href="#">Video Content</a>
    </div>

  <div class="nav-links">
    <a href="training-finetuning.html">&larr; Training &amp; Fine-tuning</a>
    <a href="model-types.html">Model Types &amp; Structures &rarr;</a>
  </div>
</div>
<div class="footer"><a href="../../index.html">&larr; Back to Course</a></div>
</body>
</html>