<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Training &amp; Fine-tuning - Basic Theory</title>
<style>
* { margin:0; padding:0; box-sizing:border-box; }
body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background:#0a0a0f; color:#e4e6eb; }
a { color:#6ab2f2; text-decoration:none; }
a:hover { text-decoration:underline; }
.header { background:#111119; padding:16px 24px; border-bottom:1px solid #1e1e2e; display:flex; align-items:center; gap:16px; position:sticky; top:0; z-index:10; }
.header h1 { font-size:18px; color:#6ab2f2; }
.header .back { color:#8696a4; font-size:14px; }
.header .lang-switch { margin-left:auto; font-size:13px; color:#8696a4; }
.container { max-width:900px; margin:0 auto; padding:24px; }
.breadcrumb { color:#8696a4; font-size:13px; margin-bottom:20px; }
.breadcrumb a { color:#6ab2f2; }
.level-badge { display:inline-block; padding:4px 12px; border-radius:20px; font-size:12px; font-weight:600; margin-bottom:16px; }
.level-1 { background:rgba(74,222,128,0.15); color:#4ade80; }
.level-2 { background:rgba(96,165,250,0.15); color:#60a5fa; }
.level-3 { background:rgba(245,158,11,0.15); color:#f59e0b; }
.level-4 { background:rgba(239,68,68,0.15); color:#ef4444; }
.level-5 { background:rgba(168,85,247,0.15); color:#a855f7; }
h2 { font-size:28px; margin-bottom:8px; }
.desc { color:#8696a4; font-size:15px; line-height:1.7; margin-bottom:24px; }
.detail-list { list-style:none; padding:0; }
.detail-list li { padding:10px 16px; margin-bottom:6px; border-radius:8px; background:#111119; border-left:3px solid #2b5278; font-size:14px; line-height:1.5; color:#e4e6eb; }
.detail-list li::before { content:'\2192 '; color:#6ab2f2; }
.section-title { font-size:16px; color:#8696a4; margin:24px 0 12px; text-transform:uppercase; letter-spacing:1px; }
.related-topics { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
.related-topics a { display:inline-block; background:#111119; border:1px solid #1e1e2e; padding:6px 14px; border-radius:8px; font-size:13px; color:#6ab2f2; }
.related-topics a:hover { background:#1e1e2e; text-decoration:none; }
.nav-links { display:flex; justify-content:space-between; margin-top:30px; padding-top:20px; border-top:1px solid #1e1e2e; }
.nav-links a { color:#6ab2f2; font-size:14px; }
.nav-links .disabled { color:#333; }
.topic-grid { display:grid; grid-template-columns:repeat(auto-fill, minmax(260px, 1fr)); gap:12px; margin-top:16px; }
.topic-card { background:#111119; border-radius:12px; padding:18px; border-left:3px solid #2b5278; display:block; transition:background 0.15s; }
.topic-card:hover { background:#1a1a2e; text-decoration:none; }
.topic-card h3 { color:#e4e6eb; font-size:15px; margin-bottom:4px; }
.topic-card .sub { color:#8696a4; font-size:13px; }
.footer { text-align:center; padding:30px; color:#8696a4; font-size:12px; margin-top:30px; border-top:1px solid #1e1e2e; }
.overview { margin-bottom:28px; }
.overview p { color:#c4c6cb; font-size:15px; line-height:1.8; margin-bottom:14px; }
.terms-grid { display:grid; gap:10px; margin-top:12px; }
.term-card { background:#111119; border:1px solid #1e1e2e; border-radius:10px; padding:14px 18px; }
.term-card strong { color:#6ab2f2; font-size:14px; }
.term-card span { color:#c4c6cb; font-size:13px; display:block; margin-top:4px; line-height:1.5; }
.tip-box { background:linear-gradient(135deg, rgba(106,178,242,0.06), rgba(106,178,242,0.02)); border:1px solid rgba(106,178,242,0.15); border-radius:10px; padding:16px 20px; margin-top:12px; }
.tip-box li { color:#c4c6cb; font-size:14px; line-height:1.6; margin-bottom:8px; list-style:none; }
.tip-box li::before { content:'üí° '; }
.detail-card { background:#111119; border-radius:10px; padding:16px 18px; margin-bottom:8px; border-left:3px solid #2b5278; }
.detail-card .dt { color:#e4e6eb; font-size:14px; font-weight:600; margin-bottom:4px; }
.detail-card .dd { color:#9ca3af; font-size:13px; line-height:1.6; }
.detail-card .dl { margin-top:6px; }
.detail-card .dl a { font-size:12px; color:#6ab2f2; background:rgba(106,178,242,0.08); padding:3px 10px; border-radius:6px; display:inline-block; margin-right:6px; margin-top:4px; }
.detail-card .dl a:hover { background:rgba(106,178,242,0.15); text-decoration:none; }
</style>
</head>
<body>
<div class="header">
  <a href="../../index.html" class="back">&larr; Back to Course</a>
  <h1>Basic Theory</h1>
  <a href="../../../uk/basic-theory/level-3/training-finetuning.html" class="lang-switch">üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</a>
</div>
<div class="container">
  <div class="breadcrumb"><a href="../../index.html">Course</a> / <a href="../index.html">Basic Theory</a> / <a href="index.html">Level 3</a> / Training &amp; Fine-tuning</div>
  <span class="level-badge level-3">‚ö° Level 3 ‚Äî Professional</span>
  <h2>Training &amp; Fine-tuning</h2>
  <p class="desc">How models are trained from scratch and adapted for specific tasks.</p>

  <div class="overview">
    <p>Model training happens in stages. Pre-training teaches the model general language understanding by predicting next tokens on internet-scale data ‚Äî this costs millions of dollars and requires thousands of GPUs. Fine-tuning then adapts this general model for specific tasks using much smaller, curated datasets. Finally, alignment training (RLHF or DPO) teaches the model to be helpful, honest, and safe.</p>
    <p>The rise of parameter-efficient methods like LoRA has democratized fine-tuning ‚Äî you can now adapt a 70B parameter model on a single consumer GPU. This has created a vibrant ecosystem of community-created fine-tunes for specific domains, languages, and use cases. Understanding when to fine-tune vs when to just prompt better is a key skill for AI practitioners.</p>
  </div>
  <div class="section-title">Key Topics Covered</div>
  <div class="detail-cards">
    <div class="detail-card"><div class="dt">Pre-training</div><div class="dd">Massive compute (thousands of GPUs for months) on trillions of tokens. The model learns language structure, world knowledge, and reasoning by predicting the next token. Costs $10M-$100M+ for frontier models.</div><div class="dl"><a href="data-to-model.html">Data to Model</a><a href="../level-4/hardware.html">Hardware</a></div></div>
    <div class="detail-card"><div class="dt">Supervised Fine-Tuning (SFT)</div><div class="dd">Training on curated instruction-response pairs to teach the model to follow instructions and produce useful outputs. Thousands to millions of examples, typically taking hours to days on a few GPUs.</div><div class="dl"><a href="../level-2/prompt.html">Prompt</a></div></div>
    <div class="detail-card"><div class="dt">RLHF</div><div class="dd">Reinforcement Learning from Human Feedback ‚Äî humans compare model outputs, a reward model learns their preferences, then the LLM is optimized to maximize that reward. The technique that made ChatGPT work.</div><div class="dl"><a href="../level-5/alignment.html">Alignment</a></div></div>
    <div class="detail-card"><div class="dt">DPO</div><div class="dd">Direct Preference Optimization ‚Äî a simpler alternative to RLHF that eliminates the separate reward model. Directly optimizes the LLM on preference pairs. Increasingly popular for its stability and simplicity.</div><div class="dl"><a href="../level-5/alignment.html">Alignment</a></div></div>
    <div class="detail-card"><div class="dt">LoRA</div><div class="dd">Low-Rank Adaptation ‚Äî fine-tuning only small additional matrices (1-5% of parameters) while keeping the original model frozen. Produces tiny adapter files (10-100MB) that can be swapped in and out.</div><div class="dl"><a href="model-optimization.html">Model Optimization</a></div></div>
    <div class="detail-card"><div class="dt">QLoRA</div><div class="dd">Quantized LoRA ‚Äî combines model quantization (4-bit) with LoRA adapters, enabling fine-tuning of 70B models on a single 24GB GPU. A breakthrough for accessible AI customization.</div><div class="dl"><a href="model-optimization.html">Model Optimization</a></div></div>
    <div class="detail-card"><div class="dt">Full Fine-tuning vs PEFT</div><div class="dd">Full fine-tuning updates all parameters ‚Äî maximum quality but requires multi-GPU setups and risks catastrophic forgetting. Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA are cheaper and preserve base model knowledge.</div><div class="dl"><a href="neural-networks.html">Neural Networks</a></div></div>
    <div class="detail-card"><div class="dt">When to Fine-tune vs Prompt</div><div class="dd">Fine-tune for: consistent style/format, domain-specific knowledge, specific output structures. Prompt for: flexible tasks, rapid iteration, no training data available. Fine-tuning is a commitment; prompting is an experiment.</div><div class="dl"><a href="../level-4/prompting-techniques.html">Prompting Techniques</a></div></div>
    <div class="detail-card"><div class="dt">Training Cost Spectrum</div><div class="dd">Pre-training ($10M+) vs fine-tuning ($100-10K) vs prompting (free/cheap). API-based fine-tuning (OpenAI, Anthropic) costs pennies per example. Self-hosted fine-tuning requires GPU rental ($1-5/hr for A100).</div><div class="dl"><a href="../level-4/api-providers.html">API Providers</a></div></div>
    <div class="detail-card"><div class="dt">Evaluation and Iteration</div><div class="dd">Measuring fine-tuned model quality with held-out test sets, automated metrics (perplexity, BLEU, ROUGE), and human evaluation. Always compare against the base model to quantify improvement.</div><div class="dl"><a href="../level-1/sota.html">SOTA</a></div></div>
  </div>

  <div class="section-title">Key Terms</div>
  <div class="terms-grid">
    <div class="term-card"><strong>RLHF</strong><span>Reinforcement Learning from Human Feedback ‚Äî training models to align with human preferences using a learned reward model.</span></div>
    <div class="term-card"><strong>LoRA</strong><span>Low-Rank Adaptation ‚Äî efficient fine-tuning that trains only small additional matrices while keeping the base model frozen.</span></div>
    <div class="term-card"><strong>SFT</strong><span>Supervised Fine-Tuning ‚Äî training on curated instruction-response pairs to teach the model to follow instructions.</span></div>
    <div class="term-card"><strong>DPO</strong><span>Direct Preference Optimization ‚Äî simpler alternative to RLHF that directly optimizes on human preference pairs without a reward model.</span></div>
  </div>

  <div class="section-title">Practical Tips</div>
  <div class="tip-box"><ul>
    <li>Try prompting and few-shot examples first ‚Äî fine-tuning is worth it only when prompting hits a quality ceiling you cannot overcome</li>
    <li>Start with LoRA/QLoRA before attempting full fine-tuning ‚Äî you'll get 90% of the quality at 5% of the cost and compute</li>
    <li>Always hold out 10-20% of your data for evaluation ‚Äî without a test set, you cannot tell if your fine-tune actually improved the model</li>
  </ul></div>

    <div class="section-title">Related Community Discussions</div>
    <div class="related-topics">
      <a href="#">Models</a>
      <a href="#">Video Content</a>
    </div>

  <div class="nav-links">
    <a href="data-to-model.html">&larr; Data to Model</a>
    <a href="model-optimization.html">Model Optimization &rarr;</a>
  </div>
</div>
<div class="footer"><a href="../../index.html">&larr; Back to Course</a></div>
</body>
</html>