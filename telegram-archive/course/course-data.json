{
  "ui": {
    "en": {
      "lang": "en",
      "langName": "English",
      "siteTitle": "AI & Programming Course",
      "courseSubtitle": "Complete Course Structure ‚Äî AI & Programming",
      "courseSource": "Compiled from community content and knowledge base materials",
      "totalLessons": "Total Lessons",
      "modules": "Modules",
      "kbCourses": "KB Courses",
      "tgTopics": "TG Topics",
      "novice": "NOVICE",
      "noviceSub": "Beginner",
      "advanced": "ADVANCED",
      "advancedSub": "Intermediate",
      "expert": "EXPERT",
      "expertSub": "Expert",
      "modulesWord": "modules",
      "lessonsWord": "lessons",
      "noviceDesc": "Foundational AI knowledge, first steps with AI coding tools, basic terminology and concepts. Covers Basic Theory Levels 1-2, AI Programming basics, and Models for Coding introduction.",
      "advancedDesc": "Deep-dive into neural networks, advanced prompting, MCP, RAG, agents, spec-driven development. Covers Basic Theory Levels 3-4 and advanced AI Programming topics.",
      "expertDesc": "AI horizons and future, production agent systems, multi-agent architectures, voice agents. Covers Basic Theory Level 5, advanced AI Programming, and workshop content.",
      "contentMapped": "content mapped",
      "contentSources": "Content Sources",
      "basicTheory": "Basic Theory",
      "basicTheoryDesc": "5 levels, 42 topics",
      "aiProg": "AI Programming",
      "aiProgDesc": "20 topics",
      "modelsDesc": "6 topics",
      "tgDesc": "Telegram Community",
      "tgDescLong": "discussions and workshops",
      "available": "Available",
      "availableDesc": "Full content from both sources",
      "kbTg": "KB + Community",
      "kbTgDesc": "Combined content from knowledge base and community",
      "fromKb": "From KB",
      "fromKbDesc": "Lesson structure from knowledge base",
      "kbLesson": "KB Lesson",
      "workshop": "Workshop recording",
      "discussion": "Community discussion",
      "full": "Full",
      "lesson": "lesson",
      "lessons": "lessons",
      "backToCourse": "Back to Course",
      "backToTopics": "Back to Topics",
      "course": "Course",
      "basicTheoryTitle": "Basic Theory",
      "level": "Level",
      "levels": "Levels",
      "topics": "Topics",
      "keyTopics": "Key Topics Covered",
      "relatedDiscussions": "Related Community Discussions",
      "tgArchive": "Community Archive",
      "beginner": "Beginner",
      "user": "User",
      "professional": "Professional",
      "master": "Master",
      "horizons": "Horizons",
      "btOverviewDesc": "5 progressive levels covering everything from what generative AI is to AGI, alignment, and the future of intelligence.",
      "selectLang": "Select Language",
      "footer": "AI & Programming Course Structure",
      "switchLang": "–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –≤–µ—Ä—Å—ñ—è",
      "switchLangHref": "../uk/index.html",
      "otherLang": "üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞"
    },
    "uk": {
      "lang": "uk",
      "langName": "–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞",
      "siteTitle": "–ö—É—Ä—Å –∑ –®–Ü —Ç–∞ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è",
      "courseSubtitle": "–ü–æ–≤–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫—É—Ä—Å—É ‚Äî –®–Ü —Ç–∞ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è",
      "courseSource": "–ó—ñ–±—Ä–∞–Ω–æ –∑ –º–∞—Ç–µ—Ä—ñ–∞–ª—ñ–≤ —Å–ø—ñ–ª—å–Ω–æ—Ç–∏ —Ç–∞ –±–∞–∑–∏ –∑–Ω–∞–Ω—å",
      "totalLessons": "–£—Å—å–æ–≥–æ —É—Ä–æ–∫—ñ–≤",
      "modules": "–ú–æ–¥—É–ª—ñ–≤",
      "kbCourses": "–ö—É—Ä—Å–∏ –ë–ó",
      "tgTopics": "–¢–µ–º–∏ TG",
      "novice": "–ù–û–í–ê–ß–û–ö",
      "noviceSub": "–ü–æ—á–∞—Ç–∫–æ–≤–∏–π",
      "advanced": "–ü–†–û–°–£–ù–£–¢–ò–ô",
      "advancedSub": "–°–µ—Ä–µ–¥–Ω—ñ–π",
      "expert": "–ï–ö–°–ü–ï–†–¢",
      "expertSub": "–ï–∫—Å–ø–µ—Ä—Ç–Ω–∏–π",
      "modulesWord": "–º–æ–¥—É–ª—ñ–≤",
      "lessonsWord": "—É—Ä–æ–∫—ñ–≤",
      "noviceDesc": "–ë–∞–∑–æ–≤—ñ –∑–Ω–∞–Ω–Ω—è –ø—Ä–æ –®–Ü, –ø–µ—Ä—à—ñ –∫—Ä–æ–∫–∏ –∑ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –®–Ü –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è, –æ—Å–Ω–æ–≤–Ω–∞ —Ç–µ—Ä–º—ñ–Ω–æ–ª–æ–≥—ñ—è —Ç–∞ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó. –û—Ö–æ–ø–ª—é—î –ë–∞–∑–æ–≤—É —Ç–µ–æ—Ä—ñ—é —Ä—ñ–≤–Ω—ñ–≤ 1-2, –æ—Å–Ω–æ–≤–∏ –®–Ü-–ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è —Ç–∞ –≤—Å—Ç—É–ø –¥–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–∑—Ä–æ–±–∫–∏.",
      "advancedDesc": "–ì–ª–∏–±–æ–∫–µ –∑–∞–Ω—É—Ä–µ–Ω–Ω—è –≤ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂—ñ, –ø—Ä–æ—Å—É–Ω—É—Ç–∏–π –ø—Ä–æ–º–ø—Ç–∏–Ω–≥, MCP, RAG, –∞–≥–µ–Ω—Ç–∏, —Ä–æ–∑—Ä–æ–±–∫–∞ –∑–∞ —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ—è–º–∏. –û—Ö–æ–ø–ª—é—î –ë–∞–∑–æ–≤—É —Ç–µ–æ—Ä—ñ—é —Ä—ñ–≤–Ω—ñ–≤ 3-4 —Ç–∞ –ø—Ä–æ—Å—É–Ω—É—Ç—ñ —Ç–µ–º–∏ –®–Ü-–ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è.",
      "expertDesc": "–ì–æ—Ä–∏–∑–æ–Ω—Ç–∏ —Ç–∞ –º–∞–π–±—É—Ç–Ω—î –®–Ü, –ø—Ä–æ–¥–∞–∫—à–Ω –∞–≥–µ–Ω—Ç–Ω—ñ —Å–∏—Å—Ç–µ–º–∏, –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏, –≥–æ–ª–æ—Å–æ–≤—ñ –∞–≥–µ–Ω—Ç–∏. –û—Ö–æ–ø–ª—é—î –ë–∞–∑–æ–≤—É —Ç–µ–æ—Ä—ñ—é —Ä—ñ–≤–Ω—è 5, –ø—Ä–æ—Å—É–Ω—É—Ç–µ –®–Ü-–ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è —Ç–∞ –≤–æ—Ä–∫—à–æ–ø–∏.",
      "contentMapped": "–∫–æ–Ω—Ç–µ–Ω—Ç—É –∑—ñ–±—Ä–∞–Ω–æ",
      "contentSources": "–î–∂–µ—Ä–µ–ª–∞ –∫–æ–Ω—Ç–µ–Ω—Ç—É",
      "basicTheory": "–ë–∞–∑–æ–≤–∞ —Ç–µ–æ—Ä—ñ—è",
      "basicTheoryDesc": "5 —Ä—ñ–≤–Ω—ñ–≤, 42 —Ç–µ–º–∏",
      "aiProg": "–®–Ü-–ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è",
      "aiProgDesc": "20 —Ç–µ–º",
      "modelsDesc": "6 —Ç–µ–º",
      "tgDesc": "Telegram-—Å–ø—ñ–ª—å–Ω–æ—Ç–∞",
      "tgDescLong": "–æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –≤–æ—Ä–∫—à–æ–ø–∏",
      "available": "–î–æ—Å—Ç—É–ø–Ω–æ",
      "availableDesc": "–ü–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –∑ –æ–±–æ—Ö –¥–∂–µ—Ä–µ–ª",
      "kbTg": "–ë–ó + –°–ø—ñ–ª—å–Ω–æ—Ç–∞",
      "kbTgDesc": "–ö–æ–º–±—ñ–Ω–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç —ñ–∑ –±–∞–∑–∏ –∑–Ω–∞–Ω—å —Ç–∞ —Å–ø—ñ–ª—å–Ω–æ—Ç–∏",
      "fromKb": "–ó –ë–ó",
      "fromKbDesc": "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —É—Ä–æ–∫—É –∑ –±–∞–∑–∏ –∑–Ω–∞–Ω—å",
      "kbLesson": "–£—Ä–æ–∫ –ë–ó",
      "workshop": "–ó–∞–ø–∏—Å –≤–æ—Ä–∫—à–æ–ø—É",
      "discussion": "–û–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è —Å–ø—ñ–ª—å–Ω–æ—Ç–∏",
      "full": "–ü–æ–≤–Ω–∏–π",
      "lesson": "—É—Ä–æ–∫",
      "lessons": "—É—Ä–æ–∫—ñ–≤",
      "backToCourse": "–ù–∞–∑–∞–¥ –¥–æ –∫—É—Ä—Å—É",
      "backToTopics": "–ù–∞–∑–∞–¥ –¥–æ —Ç–µ–º",
      "course": "–ö—É—Ä—Å",
      "basicTheoryTitle": "–ë–∞–∑–æ–≤–∞ —Ç–µ–æ—Ä—ñ—è",
      "level": "–†—ñ–≤–µ–Ω—å",
      "levels": "–†—ñ–≤–Ω—ñ",
      "topics": "–¢–µ–º–∏",
      "keyTopics": "–û—Å–Ω–æ–≤–Ω—ñ —Ç–µ–º–∏",
      "relatedDiscussions": "–ü–æ–≤'—è–∑–∞–Ω—ñ –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è —Å–ø—ñ–ª—å–Ω–æ—Ç–∏",
      "tgArchive": "–ê—Ä—Ö—ñ–≤ —Å–ø—ñ–ª—å–Ω–æ—Ç–∏",
      "beginner": "–ù–æ–≤–∞—á–æ–∫",
      "user": "–ö–æ—Ä–∏—Å—Ç—É–≤–∞—á",
      "professional": "–ü—Ä–æ—Ñ–µ—Å—ñ–æ–Ω–∞–ª",
      "master": "–ú–∞–π—Å—Ç–µ—Ä",
      "horizons": "–ì–æ—Ä–∏–∑–æ–Ω—Ç–∏",
      "btOverviewDesc": "5 –ø—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–∏—Ö —Ä—ñ–≤–Ω—ñ–≤, —â–æ –æ—Ö–æ–ø–ª—é—é—Ç—å –≤—Å–µ –≤—ñ–¥ –æ—Å–Ω–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –®–Ü –¥–æ AGI, –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è —Ç–∞ –º–∞–π–±—É—Ç–Ω—å–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.",
      "selectLang": "–û–±–µ—Ä—ñ—Ç—å –º–æ–≤—É",
      "footer": "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫—É—Ä—Å—É –∑ –®–Ü —Ç–∞ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è",
      "switchLang": "English version",
      "switchLangHref": "../en/index.html",
      "otherLang": "üá¨üáß English"
    }
  },
  "levels": [
    {
      "num": 1,
      "emoji": "üå±",
      "title": {
        "en": "Beginner",
        "uk": "–ù–æ–≤–∞—á–æ–∫"
      },
      "desc": {
        "en": "Foundational concepts of generative AI, major players, model types, and core terminology.",
        "uk": "–ë–∞–∑–æ–≤—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –®–Ü, –æ—Å–Ω–æ–≤–Ω—ñ –≥—Ä–∞–≤—Ü—ñ, —Ç–∏–ø–∏ –º–æ–¥–µ–ª–µ–π —Ç–∞ –∫–ª—é—á–æ–≤–∞ —Ç–µ—Ä–º—ñ–Ω–æ–ª–æ–≥—ñ—è."
      },
      "topics": [
        {
          "slug": "generative-ai",
          "title": {
            "en": "Generative AI",
            "uk": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü"
          },
          "desc": {
            "en": "Introduction to generative artificial intelligence - what it is, how it works, and why it matters.",
            "uk": "–í—Å—Ç—É–ø –¥–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É ‚Äî —â–æ —Ü–µ, —è–∫ –ø—Ä–∞—Ü—é—î —Ç–∞ —á–æ–º—É —Ü–µ –≤–∞–∂–ª–∏–≤–æ."
          },
          "overview": {
            "en": [
              "Generative AI is a class of artificial intelligence systems that can create new content ‚Äî text, images, audio, video, and code ‚Äî rather than simply analyzing or classifying existing data. Unlike traditional machine learning models that predict labels or numbers, generative models learn the underlying patterns and distribution of their training data, then produce entirely new outputs that follow those same patterns.",
              "The field exploded into mainstream awareness with the release of ChatGPT in November 2022, but the foundations were laid years earlier with the Transformer architecture (2017), GPT-1 (2018), and the steady scaling of models that revealed emergent abilities at larger sizes. Today, generative AI powers everything from coding assistants to image generators, and understanding how it works is the essential starting point for anyone working with modern AI tools.",
              "Generative AI operates across multiple modalities. Large Language Models (LLMs) like GPT-4 and Claude generate text. Diffusion models like Stable Diffusion and DALL-E create images. Models like Sora generate video, while Whisper and similar systems handle audio. Increasingly, multimodal models combine several of these capabilities in a single system."
            ],
            "uk": [
              "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü ‚Äî —Ü–µ –∫–ª–∞—Å —Å–∏—Å—Ç–µ–º —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É, —â–æ –º–æ–∂—É—Ç—å —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –Ω–æ–≤–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç ‚Äî —Ç–µ–∫—Å—Ç, –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –∞—É–¥—ñ–æ, –≤—ñ–¥–µ–æ —Ç–∞ –∫–æ–¥ ‚Äî –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —á–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ —ñ—Å–Ω—É—é—á—ñ –¥–∞–Ω—ñ. –ù–∞ –≤—ñ–¥–º—ñ–Ω—É –≤—ñ–¥ —Ç—Ä–∞–¥–∏—Ü—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è, —â–æ –ø–µ—Ä–µ–¥–±–∞—á–∞—é—Ç—å –º—ñ—Ç–∫–∏ —á–∏ —á–∏—Å–ª–∞, –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ñ –º–æ–¥–µ–ª—ñ –≤–∏–≤—á–∞—é—Ç—å –±–∞–∑–æ–≤—ñ –ø–∞—Ç–µ—Ä–Ω–∏ —Ç–∞ —Ä–æ–∑–ø–æ–¥—ñ–ª –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö, –∞ –ø–æ—Ç—ñ–º —Å—Ç–≤–æ—Ä—é—é—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–∞ —Ç–∏–º–∏ –∂ –ø–∞—Ç–µ—Ä–Ω–∞–º–∏.",
              "–¶—è –≥–∞–ª—É–∑—å –≤–∏–±—É—Ö–Ω—É–ª–∞ —É –º–∞—Å–æ–≤—É —Å–≤—ñ–¥–æ–º—ñ—Å—Ç—å –∑ –≤–∏–ø—É—Å–∫–æ–º ChatGPT —É –ª–∏—Å—Ç–æ–ø–∞–¥—ñ 2022 —Ä–æ–∫—É, –∞–ª–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –±—É–≤ –∑–∞–∫–ª–∞–¥–µ–Ω–∏–π —Ä–æ–∫–∞–º–∏ —Ä–∞–Ω—ñ—à–µ –∑ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–æ—é Transformer (2017), GPT-1 (2018) —Ç–∞ –ø–æ—Å—Ç—É–ø–æ–≤–∏–º –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è–º –º–æ–¥–µ–ª–µ–π, —â–æ –≤–∏—è–≤–∏–ª–æ –µ–º–µ—Ä–¥–∂–µ–Ω—Ç–Ω—ñ –∑–¥—ñ–±–Ω–æ—Å—Ç—ñ –ø—Ä–∏ –±—ñ–ª—å—à–∏—Ö —Ä–æ–∑–º—ñ—Ä–∞—Ö. –°—å–æ–≥–æ–¥–Ω—ñ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü –∂–∏–≤–∏—Ç—å –≤—Å–µ ‚Äî –≤—ñ–¥ –∞—Å–∏—Å—Ç–µ–Ω—Ç—ñ–≤ –∫–æ–¥—É–≤–∞–Ω–Ω—è –¥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ñ–≤ –∑–æ–±—Ä–∞–∂–µ–Ω—å, —ñ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –π–æ–≥–æ —Ä–æ–±–æ—Ç–∏ —î –æ—Å–Ω–æ–≤–Ω–æ—é —Å—Ç–∞—Ä—Ç–æ–≤–æ—é —Ç–æ—á–∫–æ—é –¥–ª—è –±—É–¥—å-–∫–æ–≥–æ, —Ö—Ç–æ –ø—Ä–∞—Ü—é—î –∑ —Å—É—á–∞—Å–Ω–∏–º–∏ –®–Ü-—ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏.",
              "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü –ø—Ä–∞—Ü—é—î —á–µ—Ä–µ–∑ –º–Ω–æ–∂–∏–Ω–Ω—ñ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ. –í–µ–ª–∏–∫—ñ –º–æ–≤–Ω—ñ –º–æ–¥–µ–ª—ñ (LLM) —è–∫ GPT-4 —Ç–∞ Claude –≥–µ–Ω–µ—Ä—É—é—Ç—å —Ç–µ–∫—Å—Ç. –î–∏—Ñ—É–∑—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ —è–∫ Stable Diffusion —Ç–∞ DALL-E —Å—Ç–≤–æ—Ä—é—é—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è. –ú–æ–¥–µ–ª—ñ —Ç–∏–ø—É Sora –≥–µ–Ω–µ—Ä—É—é—Ç—å –≤—ñ–¥–µ–æ, –∞ Whisper —Ç–∞ –ø–æ–¥—ñ–±–Ω—ñ —Å–∏—Å—Ç–µ–º–∏ –ø—Ä–∞—Ü—é—é—Ç—å –∑ –∞—É–¥—ñ–æ. –í—Å–µ —á–∞—Å—Ç—ñ—à–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ –ø–æ—î–¥–Ω—É—é—Ç—å –∫—ñ–ª—å–∫–∞ —Ü–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –≤ –æ–¥–Ω—ñ–π —Å–∏—Å—Ç–µ–º—ñ."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Generative vs Traditional AI/ML",
                "desc": "Traditional AI classifies or predicts from data. Generative AI creates new content ‚Äî text, images, code ‚Äî by learning patterns from training data and producing novel outputs.",
                "links": [
                  {
                    "title": "Data Type Classification",
                    "href": "../level-1/data-classification.html"
                  }
                ]
              },
              {
                "text": "Key Generative Modalities",
                "desc": "Text (LLMs like GPT, Claude), images (diffusion models like DALL-E, Midjourney), audio (Whisper, TTS), video (Sora, Runway), and code (Codex, StarCoder). Each modality uses different architectures.",
                "links": [
                  {
                    "title": "Diffusion Models",
                    "href": "diffusion-models.html"
                  },
                  {
                    "title": "Multimodality",
                    "href": "multimodality.html"
                  }
                ]
              },
              {
                "text": "History of Generative AI",
                "desc": "GPT-1 (2018, 117M params) ‚Üí GPT-2 (2019, 1.5B) ‚Üí GPT-3 (2020, 175B) ‚Üí ChatGPT (Nov 2022, public launch) ‚Üí GPT-4 (2023, multimodal). Each step brought qualitative leaps in capability.",
                "links": [
                  {
                    "title": "LLM and GPT",
                    "href": "llm-and-gpt.html"
                  }
                ]
              },
              {
                "text": "Generative vs Discriminative Models",
                "desc": "Discriminative models learn decision boundaries (cat vs dog). Generative models learn the full data distribution and can sample new examples from it. LLMs are generative ‚Äî they model the probability of text.",
                "links": []
              },
              {
                "text": "The Transformer Architecture",
                "desc": "The 2017 \"Attention Is All You Need\" paper introduced self-attention, enabling parallel processing of sequences. Virtually all modern generative AI ‚Äî text, image, video ‚Äî builds on this architecture.",
                "links": [
                  {
                    "title": "Neural Networks",
                    "href": "../level-3/neural-networks.html"
                  },
                  {
                    "title": "Model Types",
                    "href": "../level-3/model-types.html"
                  }
                ]
              },
              {
                "text": "Pre-training at Scale",
                "desc": "Models are trained on trillions of tokens from the internet ‚Äî books, websites, code repos, scientific papers. This phase costs millions of dollars and produces a \"foundation model\" with general capabilities.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "foundation-models.html"
                  },
                  {
                    "title": "Data to Model",
                    "href": "../level-3/data-to-model.html"
                  }
                ]
              },
              {
                "text": "Emergent Abilities",
                "desc": "At certain scales, models suddenly gain capabilities not present in smaller versions: in-context learning, chain-of-thought reasoning, code generation. These emerge from scale, not explicit programming.",
                "links": [
                  {
                    "title": "Reasoning",
                    "href": "reasoning.html"
                  }
                ]
              },
              {
                "text": "Real-World Applications",
                "desc": "Software development (AI pair programming, code review), content creation (writing, design), healthcare (drug discovery), education (tutoring), finance (analysis), legal (document review).",
                "links": []
              },
              {
                "text": "Open vs Closed Ecosystem",
                "desc": "Closed models (GPT-4, Claude) offer superior performance via API. Open-weight models (Llama, Qwen, Mistral) can be run locally, fine-tuned, and inspected. Both ecosystems are thriving.",
                "links": [
                  {
                    "title": "The Big Players",
                    "href": "big-players.html"
                  },
                  {
                    "title": "API Providers",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              },
              {
                "text": "Current Limitations",
                "desc": "Hallucinations (confident but wrong outputs), reasoning gaps (failing on novel logic), context constraints (limited working memory), lack of real-time knowledge, and inability to truly \"understand.\"",
                "links": [
                  {
                    "title": "Hallucinations",
                    "href": "../level-2/hallucination.html"
                  },
                  {
                    "title": "Context",
                    "href": "../level-2/context.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π vs –¢—Ä–∞–¥–∏—Ü—ñ–π–Ω–∏–π –®–Ü/ML",
                "desc": "–¢—Ä–∞–¥–∏—Ü—ñ–π–Ω–∏–π –®–Ü –∫–ª–∞—Å–∏—Ñ—ñ–∫—É—î –∞–±–æ –ø–µ—Ä–µ–¥–±–∞—á–∞—î –∑ –¥–∞–Ω–∏—Ö. –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü —Å—Ç–≤–æ—Ä—é—î –Ω–æ–≤–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç ‚Äî —Ç–µ–∫—Å—Ç, –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –∫–æ–¥ ‚Äî –≤–∏–≤—á–∞—é—á–∏ –ø–∞—Ç–µ—Ä–Ω–∏ –∑ –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö —ñ —Å—Ç–≤–æ—Ä—é—é—á–∏ –Ω–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏.",
                "links": [
                  {
                    "title": "–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∑–∞ —Ç–∏–ø–æ–º –¥–∞–Ω–∏—Ö",
                    "href": "../level-1/data-classification.html"
                  }
                ]
              },
              {
                "text": "–û—Å–Ω–æ–≤–Ω—ñ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ñ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ",
                "desc": "–¢–µ–∫—Å—Ç (LLM —è–∫ GPT, Claude), –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (–¥–∏—Ñ—É–∑—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ —è–∫ DALL-E, Midjourney), –∞—É–¥—ñ–æ (Whisper, TTS), –≤—ñ–¥–µ–æ (Sora, Runway) —Ç–∞ –∫–æ–¥ (Codex, StarCoder). –ö–æ–∂–Ω–∞ –º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ä—ñ–∑–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏.",
                "links": [
                  {
                    "title": "–î–∏—Ñ—É–∑—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "diffusion-models.html"
                  },
                  {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å",
                    "href": "multimodality.html"
                  }
                ]
              },
              {
                "text": "–Ü—Å—Ç–æ—Ä—ñ—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –®–Ü",
                "desc": "GPT-1 (2018, 117M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤) ‚Üí GPT-2 (2019, 1.5B) ‚Üí GPT-3 (2020, 175B) ‚Üí ChatGPT (–ª–∏—Å—Ç–æ–ø–∞–¥ 2022, –ø—É–±–ª—ñ—á–Ω–∏–π –∑–∞–ø—É—Å–∫) ‚Üí GPT-4 (2023, –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∏–π). –ö–æ–∂–µ–Ω –∫—Ä–æ–∫ –ø—Ä–∏–Ω–æ—Å–∏–≤ —è–∫—ñ—Å–Ω—ñ —Å—Ç—Ä–∏–±–∫–∏ —É –º–æ–∂–ª–∏–≤–æ—Å—Ç—è—Ö.",
                "links": [
                  {
                    "title": "LLM —Ç–∞ GPT",
                    "href": "llm-and-gpt.html"
                  }
                ]
              },
              {
                "text": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ñ vs –î–∏—Å–∫—Ä–∏–º—ñ–Ω–∞—Ç–∏–≤–Ω—ñ –º–æ–¥–µ–ª—ñ",
                "desc": "–î–∏—Å–∫—Ä–∏–º—ñ–Ω–∞—Ç–∏–≤–Ω—ñ –º–æ–¥–µ–ª—ñ –≤–∏–≤—á–∞—é—Ç—å –º–µ–∂—ñ —Ä—ñ—à–µ–Ω—å (–∫—ñ—Ç vs —Å–æ–±–∞–∫–∞). –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ñ –º–æ–¥–µ–ª—ñ –≤–∏–≤—á–∞—é—Ç—å –ø–æ–≤–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö —ñ –º–æ–∂—É—Ç—å —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –Ω–æ–≤—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ –∑ –Ω—å–æ–≥–æ. LLM —î –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–º–∏ ‚Äî –≤–æ–Ω–∏ –º–æ–¥–µ–ª—é—é—Ç—å —ñ–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —Ç–µ–∫—Å—Ç—É.",
                "links": []
              },
              {
                "text": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ Transformer",
                "desc": "–°—Ç–∞—Ç—Ç—è 2017 —Ä–æ–∫—É \"Attention Is All You Need\" –≤–≤–µ–ª–∞ —Å–∞–º–æ—É–≤–∞–≥—É, —â–æ –¥–æ–∑–≤–æ–ª—è—î –ø–∞—Ä–∞–ª–µ–ª—å–Ω—É –æ–±—Ä–æ–±–∫—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π. –ü—Ä–∞–∫—Ç–∏—á–Ω–æ –≤–µ—Å—å —Å—É—á–∞—Å–Ω–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü ‚Äî —Ç–µ–∫—Å—Ç, –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –≤—ñ–¥–µ–æ ‚Äî –ø–æ–±—É–¥–æ–≤–∞–Ω–∏–π –Ω–∞ —Ü—ñ–π –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—ñ.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂",
                    "href": "../level-3/neural-networks.html"
                  },
                  {
                    "title": "–¢–∏–ø–∏ –º–æ–¥–µ–ª–µ–π",
                    "href": "../level-3/model-types.html"
                  }
                ]
              },
              {
                "text": "–ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è —É –º–∞—Å—à—Ç–∞–±—ñ",
                "desc": "–ú–æ–¥–µ–ª—ñ –Ω–∞–≤—á–∞—é—Ç—å—Å—è –Ω–∞ —Ç—Ä–∏–ª—å–π–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω—ñ–≤ –∑ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É ‚Äî –∫–Ω–∏–≥–∏, –≤–µ–±-—Å–∞–π—Ç–∏, —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—ó –∫–æ–¥—É, –Ω–∞—É–∫–æ–≤—ñ —Å—Ç–∞—Ç—Ç—ñ. –¶—è —Ñ–∞–∑–∞ –∫–æ—à—Ç—É—î –º—ñ–ª—å–π–æ–Ω–∏ –¥–æ–ª–∞—Ä—ñ–≤ —ñ —Å—Ç–≤–æ—Ä—é—î \"—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å\" —ñ–∑ –∑–∞–≥–∞–ª—å–Ω–∏–º–∏ –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º–∏.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "foundation-models.html"
                  },
                  {
                    "title": "–í—ñ–¥ –¥–∞–Ω–∏—Ö –¥–æ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-3/data-to-model.html"
                  }
                ]
              },
              {
                "text": "–ï–º–µ—Ä–¥–∂–µ–Ω—Ç–Ω—ñ –∑–¥—ñ–±–Ω–æ—Å—Ç—ñ",
                "desc": "–ü—Ä–∏ –ø–µ–≤–Ω–∏—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö –º–æ–¥–µ–ª—ñ —Ä–∞–ø—Ç–æ–≤–æ –æ—Ç—Ä–∏–º—É—é—Ç—å –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ, –≤—ñ–¥—Å—É—Ç–Ω—ñ —É –º–µ–Ω—à–∏—Ö –≤–µ—Ä—Å—ñ—è—Ö: –Ω–∞–≤—á–∞–Ω–Ω—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ, –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è –ª–∞–Ω—Ü—é–≥–æ–º –¥—É–º–æ–∫, –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–¥—É. –í–æ–Ω–∏ –≤–∏–Ω–∏–∫–∞—é—Ç—å –≤—ñ–¥ –º–∞—Å—à—Ç–∞–±—É, –∞ –Ω–µ –≤—ñ–¥ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                    "href": "reasoning.html"
                  }
                ]
              },
              {
                "text": "–†–µ–∞–ª—å–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è",
                "desc": "–†–æ–∑—Ä–æ–±–∫–∞ –ü–ó (–ø–∞—Ä–Ω–µ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è –∑ –®–Ü, –∫–æ–¥-—Ä–µ–≤—é), —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É (–Ω–∞–ø–∏—Å–∞–Ω–Ω—è, –¥–∏–∑–∞–π–Ω), –æ—Ö–æ—Ä–æ–Ω–∞ –∑–¥–æ—Ä–æ–≤'—è (—Ä–æ–∑—Ä–æ–±–∫–∞ –ª—ñ–∫—ñ–≤), –æ—Å–≤—ñ—Ç–∞ (—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä—Å—Ç–≤–æ), —Ñ—ñ–Ω–∞–Ω—Å–∏ (–∞–Ω–∞–ª—ñ–∑), —é—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü—ñ—è (–ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤).",
                "links": []
              },
              {
                "text": "–í—ñ–¥–∫—Ä–∏—Ç–∞ vs –ó–∞–∫—Ä–∏—Ç–∞ –µ–∫–æ—Å–∏—Å—Ç–µ–º–∞",
                "desc": "–ó–∞–∫—Ä–∏—Ç—ñ –º–æ–¥–µ–ª—ñ (GPT-4, Claude) –ø—Ä–æ–ø–æ–Ω—É—é—Ç—å –Ω–∞–π–∫—Ä–∞—â—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —á–µ—Ä–µ–∑ API. –ú–æ–¥–µ–ª—ñ –∑ –≤—ñ–¥–∫—Ä–∏—Ç–∏–º–∏ –≤–∞–≥–∞–º–∏ (Llama, Qwen, Mistral) –º–æ–∂–Ω–∞ –∑–∞–ø—É—Å–∫–∞—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ, —Ñ–∞–π–Ω-—Ç—é–Ω–∏—Ç–∏ —Ç–∞ –¥–æ—Å–ª—ñ–¥–∂—É–≤–∞—Ç–∏.",
                "links": [
                  {
                    "title": "–í–µ–ª–∏–∫—ñ –≥—Ä–∞–≤—Ü—ñ",
                    "href": "big-players.html"
                  },
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              },
              {
                "text": "–ü–æ—Ç–æ—á–Ω—ñ –æ–±–º–µ–∂–µ–Ω–Ω—è",
                "desc": "–ì–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó (–≤–ø–µ–≤–Ω–µ–Ω—ñ –∞–ª–µ —Ö–∏–±–Ω—ñ –≤–∏—Ö–æ–¥–∏), –ø—Ä–æ–≥–∞–ª–∏–Ω–∏ –º—ñ—Ä–∫—É–≤–∞–Ω—å (–∑–±–æ—ó –Ω–∞ –Ω–æ–≤—ñ–π –ª–æ–≥—ñ—Ü—ñ), –æ–±–º–µ–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (–æ–±–º–µ–∂–µ–Ω–∞ —Ä–æ–±–æ—á–∞ –ø–∞–º'—è—Ç—å), –≤—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –∑–Ω–∞–Ω—å —É —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ —Ç–∞ –Ω–µ–∑–¥–∞—Ç–Ω—ñ—Å—Ç—å —Å–ø—Ä–∞–≤–¥—ñ \"—Ä–æ–∑—É–º—ñ—Ç–∏.\"",
                "links": [
                  {
                    "title": "–ì–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó",
                    "href": "../level-2/hallucination.html"
                  },
                  {
                    "title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç",
                    "href": "../level-2/context.html"
                  }
                ]
              }
            ]
          },
          "sections": [
            {
              "title": {
                "en": "How Generative AI Works",
                "uk": "–Ø–∫ –ø—Ä–∞—Ü—é—î –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü"
              },
              "items": {
                "en": [
                  "Models learn statistical patterns from billions of text/image examples during pre-training",
                  "Text generation works by predicting the next token (word piece) in a sequence, one at a time",
                  "Image generation (diffusion) works by learning to remove noise from random static, guided by text descriptions",
                  "Temperature and sampling parameters control creativity vs determinism in outputs",
                  "Models have no true understanding ‚Äî they are sophisticated pattern matchers operating on statistical regularities",
                  "Fine-tuning and RLHF align raw model capabilities with human preferences and safety"
                ],
                "uk": [
                  "–ú–æ–¥–µ–ª—ñ –≤–∏–≤—á–∞—é—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω—ñ –ø–∞—Ç–µ—Ä–Ω–∏ –∑ –º—ñ–ª—å—è—Ä–¥—ñ–≤ —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö/–≥—Ä–∞—Ñ—ñ—á–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –ø—ñ–¥ —á–∞—Å –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è",
                  "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É –ø—Ä–∞—Ü—é—î —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (—á–∞—Å—Ç–∏–Ω–∏ —Å–ª–æ–≤–∞) –≤ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ, –ø–æ –æ–¥–Ω–æ–º—É –∑–∞ —Ä–∞–∑",
                  "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–æ–±—Ä–∞–∂–µ–Ω—å (–¥–∏—Ñ—É–∑—ñ—è) –ø—Ä–∞—Ü—é—î —á–µ—Ä–µ–∑ –Ω–∞–≤—á–∞–Ω–Ω—è –≤–∏–¥–∞–ª–µ–Ω–Ω—è —à—É–º—É –∑ –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö –ø–µ—Ä–µ—à–∫–æ–¥, –∫–µ—Ä–æ–≤–∞–Ω–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤–∏–º–∏ –æ–ø–∏—Å–∞–º–∏",
                  "–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —Å–µ–º–ø–ª—ñ–Ω–≥—É –∫–æ–Ω—Ç—Ä–æ–ª—é—é—Ç—å –∫—Ä–µ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å vs –¥–µ—Ç–µ—Ä–º—ñ–Ω—ñ–∑–º —É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö",
                  "–ú–æ–¥–µ–ª—ñ –Ω–µ –º–∞—é—Ç—å —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ —Ä–æ–∑—É–º—ñ–Ω–Ω—è ‚Äî —Ü–µ —Å–∫–ª–∞–¥–Ω—ñ –∑—ñ—Å—Ç–∞–≤–Ω–∏–∫–∏ –ø–∞—Ç–µ—Ä–Ω—ñ–≤, —â–æ –ø—Ä–∞—Ü—é—é—Ç—å –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∏—Ö –∑–∞–∫–æ–Ω–æ–º—ñ—Ä–Ω–æ—Å—Ç—è—Ö",
                  "–§–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ —Ç–∞ RLHF –≤–∏—Ä—ñ–≤–Ω—é—é—Ç—å —Å–∏—Ä—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –º–æ–¥–µ–ª–µ–π –∑ –ª—é–¥—Å—å–∫–∏–º–∏ –ø–µ—Ä–µ–≤–∞–≥–∞–º–∏ —Ç–∞ –±–µ–∑–ø–µ–∫–æ—é"
                ]
              }
            },
            {
              "title": {
                "en": "Impact by Industry",
                "uk": "–í–ø–ª–∏–≤ –∑–∞ –≥–∞–ª—É–∑—è–º–∏"
              },
              "items": {
                "en": [
                  "Software development: AI pair programming, code generation, automated testing, debugging",
                  "Content & media: automated writing, image/video creation, personalized content",
                  "Healthcare: drug discovery, medical imaging analysis, clinical documentation",
                  "Education: personalized tutoring, automated grading, content creation",
                  "Finance: fraud detection, report generation, risk analysis",
                  "Legal: document review, contract analysis, research assistance"
                ],
                "uk": [
                  "–†–æ–∑—Ä–æ–±–∫–∞ –ü–ó: –ø–∞—Ä–Ω–µ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è –∑ –®–Ü, –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–¥—É, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–æ–≤–∞–Ω–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è, –¥–µ–±–∞–≥—ñ–Ω–≥",
                  "–ö–æ–Ω—Ç–µ–Ω—Ç —Ç–∞ –º–µ–¥—ñ–∞: –∞–≤—Ç–æ–º–∞—Ç–∏–∑–æ–≤–∞–Ω–µ –Ω–∞–ø–∏—Å–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤, —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å/–≤—ñ–¥–µ–æ, –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç",
                  "–û—Ö–æ—Ä–æ–Ω–∞ –∑–¥–æ—Ä–æ–≤'—è: —Ä–æ–∑—Ä–æ–±–∫–∞ –ª—ñ–∫—ñ–≤, –∞–Ω–∞–ª—ñ–∑ –º–µ–¥–∏—á–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å, –∫–ª—ñ–Ω—ñ—á–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è",
                  "–û—Å–≤—ñ—Ç–∞: –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–µ —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä—Å—Ç–≤–æ, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–æ–≤–∞–Ω–µ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è, —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É",
                  "–§—ñ–Ω–∞–Ω—Å–∏: –≤–∏—è–≤–ª–µ–Ω–Ω—è —à–∞—Ö—Ä–∞–π—Å—Ç–≤–∞, –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–≤—ñ—Ç—ñ–≤, –∞–Ω–∞–ª—ñ–∑ —Ä–∏–∑–∏–∫—ñ–≤",
                  "–Æ—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü—ñ—è: –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤, –∞–Ω–∞–ª—ñ–∑ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ñ–≤, –¥–æ–ø–æ–º–æ–≥–∞ –≤ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è—Ö"
                ]
              }
            }
          ],
          "keyTerms": {
            "en": [
              {
                "term": "Generative AI",
                "def": "AI systems that create new content (text, images, code) rather than just analyzing existing data."
              },
              {
                "term": "Foundation Model",
                "def": "A large model pre-trained on broad data that serves as a base for many downstream tasks."
              },
              {
                "term": "Transformer",
                "def": "The neural network architecture (2017) that powers virtually all modern generative AI."
              },
              {
                "term": "Pre-training",
                "def": "The initial phase of training a model on massive datasets before task-specific adaptation."
              },
              {
                "term": "Emergent Abilities",
                "def": "Capabilities that appear suddenly in models only when they reach a certain scale."
              }
            ],
            "uk": [
              {
                "term": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü",
                "def": "–°–∏—Å—Ç–µ–º–∏ –®–Ü, —â–æ —Å—Ç–≤–æ—Ä—é—é—Ç—å –Ω–æ–≤–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç (—Ç–µ–∫—Å—Ç, –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –∫–æ–¥), –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∞–Ω–∞–ª—ñ–∑—É—é—Ç—å —ñ—Å–Ω—É—é—á—ñ –¥–∞–Ω—ñ."
              },
              {
                "term": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å",
                "def": "–í–µ–ª–∏–∫–∞ –º–æ–¥–µ–ª—å, –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω–∞ –Ω–∞ —à–∏—Ä–æ–∫–∏—Ö –¥–∞–Ω–∏—Ö, —â–æ —Å–ª—É–≥—É—î –æ—Å–Ω–æ–≤–æ—é –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö –∑–∞–¥–∞—á."
              },
              {
                "term": "Transformer",
                "def": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂—ñ (2017), —â–æ –∂–∏–≤–∏—Ç—å –ø—Ä–∞–∫—Ç–∏—á–Ω–æ –≤–µ—Å—å —Å—É—á–∞—Å–Ω–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü."
              },
              {
                "term": "–ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è",
                "def": "–ü–æ—á–∞—Ç–∫–æ–≤–∞ —Ñ–∞–∑–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –º–∞—Å–∏–≤–Ω–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–∏—Ö –ø–µ—Ä–µ–¥ –∑–∞–¥–∞—á–Ω–æ-—Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–æ—é –∞–¥–∞–ø—Ç–∞—Ü—ñ—î—é."
              },
              {
                "term": "–ï–º–µ—Ä–¥–∂–µ–Ω—Ç–Ω—ñ –∑–¥—ñ–±–Ω–æ—Å—Ç—ñ",
                "def": "–ú–æ–∂–ª–∏–≤–æ—Å—Ç—ñ, —â–æ —Ä–∞–ø—Ç–æ–≤–æ –∑'—è–≤–ª—è—é—Ç—å—Å—è —É –º–æ–¥–µ–ª–µ–π –ª–∏—à–µ –ø—Ä–∏ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—ñ –ø–µ–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±—É."
              }
            ]
          },
          "tips": {
            "en": [
              "Start by experimenting with ChatGPT or Claude to build intuition before diving into technical details",
              "Remember that generative AI is probabilistic ‚Äî the same prompt can produce different outputs each time",
              "Understanding the difference between generative and discriminative AI helps you choose the right tool for each task"
            ],
            "uk": [
              "–ü–æ—á–Ω—ñ—Ç—å –∑ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤ –∑ ChatGPT –∞–±–æ Claude –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ —ñ–Ω—Ç—É—ó—Ü—ñ—ó –ø–µ—Ä–µ–¥ –∑–∞–Ω—É—Ä–µ–Ω–Ω—è–º —É —Ç–µ—Ö–Ω—ñ—á–Ω—ñ –¥–µ—Ç–∞–ª—ñ",
              "–ü–∞–º'—è—Ç–∞–π—Ç–µ, —â–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü —î —ñ–º–æ–≤—ñ—Ä–Ω—ñ—Å–Ω–∏–º ‚Äî —Ç–æ–π —Å–∞–º–∏–π –ø—Ä–æ–º–ø—Ç –º–æ–∂–µ –¥–∞–≤–∞—Ç–∏ —Ä—ñ–∑–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–æ–∂–Ω–æ–≥–æ —Ä–∞–∑—É",
              "–†–æ–∑—É–º—ñ–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ü—ñ –º—ñ–∂ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–º —Ç–∞ –¥–∏—Å–∫—Ä–∏–º—ñ–Ω–∞—Ç–∏–≤–Ω–∏–º –®–Ü –¥–æ–ø–æ–º–∞–≥–∞—î –æ–±—Ä–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∫–æ–∂–Ω–æ—ó –∑–∞–¥–∞—á—ñ"
            ]
          },
          "related": [
            "Feed",
            "AI Digest"
          ]
        },
        {
          "slug": "big-players",
          "title": {
            "en": "The Big Players",
            "uk": "–í–µ–ª–∏–∫—ñ –≥—Ä–∞–≤—Ü—ñ"
          },
          "desc": {
            "en": "Overview of major companies and organizations driving the AI revolution.",
            "uk": "–û–≥–ª—è–¥ –æ—Å–Ω–æ–≤–Ω–∏—Ö –∫–æ–º–ø–∞–Ω—ñ–π —Ç–∞ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π, —â–æ —Ä—É—Ö–∞—é—Ç—å —Ä–µ–≤–æ–ª—é—Ü—ñ—é –®–Ü."
          },
          "overview": {
            "en": [
              "The generative AI landscape is shaped by a handful of major organizations competing to build the most capable models. Understanding who these players are, their philosophies, and their key products is essential for navigating the rapidly evolving AI ecosystem.",
              "The competitive dynamics are complex: some companies like OpenAI and Anthropic focus on closed-source frontier models, while Meta and Mistral champion open-weight approaches. Google DeepMind leverages massive infrastructure, while Chinese labs like DeepSeek have shown that impressive results can be achieved with novel architectures and training approaches."
            ],
            "uk": [
              "–õ–∞–Ω–¥—à–∞—Ñ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –®–Ü —Ñ–æ—Ä–º—É—î—Ç—å—Å—è –∫—ñ–ª—å–∫–æ–º–∞ –≤–µ–ª–∏–∫–∏–º–∏ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è–º–∏, —â–æ –∑–º–∞–≥–∞—é—Ç—å—Å—è –∑–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–∞–π–∑–¥–∞—Ç–Ω—ñ—à–∏—Ö –º–æ–¥–µ–ª–µ–π. –†–æ–∑—É–º—ñ–Ω–Ω—è —Ö—Ç–æ —Ü—ñ –≥—Ä–∞–≤—Ü—ñ, —ó—Ö —Ñ—ñ–ª–æ—Å–æ—Ñ—ñ–π —Ç–∞ –∫–ª—é—á–æ–≤–∏—Ö –ø—Ä–æ–¥—É–∫—Ç—ñ–≤ —î –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–º –¥–ª—è –æ—Ä—ñ—î–Ω—Ç–∞—Ü—ñ—ó —É —à–≤–∏–¥–∫–æ –∑–º—ñ–Ω—é–≤–∞–Ω—ñ–π –µ–∫–æ—Å–∏—Å—Ç–µ–º—ñ –®–Ü.",
              "–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞ –¥–∏–Ω–∞–º—ñ–∫–∞ —Å–∫–ª–∞–¥–Ω–∞: –¥–µ—è–∫—ñ –∫–æ–º–ø–∞–Ω—ñ—ó —è–∫ OpenAI —Ç–∞ Anthropic —Ñ–æ–∫—É—Å—É—é—Ç—å—Å—è –Ω–∞ –∑–∞–∫—Ä–∏—Ç–∏—Ö —Ñ—Ä–æ–Ω—Ç–∏—Ä–Ω–∏—Ö –º–æ–¥–µ–ª—è—Ö, —Ç–æ–¥—ñ —è–∫ Meta —Ç–∞ Mistral –≤—ñ–¥—Å—Ç–æ—é—é—Ç—å –ø—ñ–¥—Ö—ñ–¥ –≤—ñ–¥–∫—Ä–∏—Ç–∏—Ö –≤–∞–≥. Google DeepMind –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –º–∞—Å–∏–≤–Ω—É —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É, –∞ –∫–∏—Ç–∞–π—Å—å–∫—ñ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ—ó —è–∫ DeepSeek –ø–æ–∫–∞–∑–∞–ª–∏, —â–æ –≤—Ä–∞–∂–∞—é—á–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –º–æ–∂–Ω–∞ –¥–æ—Å—è–≥—Ç–∏ –∑ –Ω–æ–≤–∞—Ç–æ—Ä—Å—å–∫–∏–º–∏ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞–º–∏ —Ç–∞ –ø—ñ–¥—Ö–æ–¥–∞–º–∏ –¥–æ –Ω–∞–≤—á–∞–Ω–Ω—è."
            ]
          },
          "details": {
            "en": [
              {
                "text": "OpenAI",
                "desc": "GPT-4, ChatGPT, DALL-E 3, Sora, o1/o3 reasoning models. Pioneer of the current AI wave. First to demonstrate scaling laws and bring LLMs to the mainstream.",
                "links": [
                  {
                    "title": "LLM and GPT",
                    "href": "llm-and-gpt.html"
                  },
                  {
                    "title": "Reasoning",
                    "href": "reasoning.html"
                  }
                ]
              },
              {
                "text": "Anthropic",
                "desc": "Claude family (Opus, Sonnet, Haiku). Constitutional AI safety approach. Founded by ex-OpenAI researchers focused on AI safety. Known for long context windows (200K tokens).",
                "links": [
                  {
                    "title": "Context",
                    "href": "../level-2/context.html"
                  },
                  {
                    "title": "AI Safety",
                    "href": "../level-5/ai-safety.html"
                  }
                ]
              },
              {
                "text": "Google DeepMind",
                "desc": "Gemini (Ultra/Pro/Flash/Nano), AlphaFold protein folding breakthrough, massive compute infrastructure. Natively multimodal models with largest context windows (1M+ tokens).",
                "links": [
                  {
                    "title": "Multimodality",
                    "href": "multimodality.html"
                  },
                  {
                    "title": "Foundation Models",
                    "href": "foundation-models.html"
                  }
                ]
              },
              {
                "text": "Meta AI",
                "desc": "Llama open-source model family (Llama 3, 3.1, 3.2). FAIR fundamental research. Champion of open-weight AI ‚Äî anyone can download, run, and fine-tune their models.",
                "links": [
                  {
                    "title": "Training & Fine-tuning",
                    "href": "../level-3/training-finetuning.html"
                  }
                ]
              },
              {
                "text": "Mistral AI",
                "desc": "European AI lab (Paris). Mistral, Mixtral MoE models. Open-weight approach. Proved that smaller European labs can compete with US giants using efficient architectures.",
                "links": [
                  {
                    "title": "Model Types",
                    "href": "../level-3/model-types.html"
                  }
                ]
              },
              {
                "text": "xAI (Grok)",
                "desc": "Elon Musk's AI lab. Grok models with real-time X (Twitter) data access. Focus on reducing censorship and maximizing helpfulness.",
                "links": []
              },
              {
                "text": "Chinese Labs",
                "desc": "DeepSeek (R1 reasoning model, rivaling o1), Alibaba (Qwen open-weight family), Baidu (ERNIE), ByteDance (Doubao). Rapidly closing the gap with Western labs.",
                "links": [
                  {
                    "title": "Reasoning",
                    "href": "reasoning.html"
                  },
                  {
                    "title": "SOTA",
                    "href": "sota.html"
                  }
                ]
              },
              {
                "text": "Specialized Labs",
                "desc": "Stability AI (Stable Diffusion image gen), Cohere (enterprise NLP), AI21 (Jamba hybrid model), Runway (video generation). Domain-specific leaders.",
                "links": [
                  {
                    "title": "Diffusion Models",
                    "href": "diffusion-models.html"
                  }
                ]
              },
              {
                "text": "Research Institutions",
                "desc": "Allen AI (OLMo fully open), EleutherAI (open research community), LAION (open datasets). Pushing for transparent, reproducible AI research.",
                "links": [
                  {
                    "title": "Data to Model",
                    "href": "../level-3/data-to-model.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "OpenAI",
                "desc": "GPT-4, ChatGPT, DALL-E 3, Sora, –º–æ–¥–µ–ª—ñ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è o1/o3. –ü—ñ–æ–Ω–µ—Ä –ø–æ—Ç–æ—á–Ω–æ—ó —Ö–≤–∏–ª—ñ –®–Ü. –ü–µ—Ä—à–∏–º–∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä—É–≤–∞–ª–∏ –∑–∞–∫–æ–Ω–∏ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è —Ç–∞ –≤–∏–≤–µ–ª–∏ LLM —É –º–µ–π–Ω—Å—Ç—Ä—ñ–º.",
                "links": [
                  {
                    "title": "LLM —Ç–∞ GPT",
                    "href": "llm-and-gpt.html"
                  },
                  {
                    "title": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                    "href": "reasoning.html"
                  }
                ]
              },
              {
                "text": "Anthropic",
                "desc": "–°—ñ–º–µ–π—Å—Ç–≤–æ Claude (Opus, Sonnet, Haiku). –ü—ñ–¥—Ö—ñ–¥ –±–µ–∑–ø–µ–∫–∏ Constitutional AI. –ó–∞—Å–Ω–æ–≤–∞–Ω–∏–π –∫–æ–ª–∏—à–Ω—ñ–º–∏ –¥–æ—Å–ª—ñ–¥–Ω–∏–∫–∞–º–∏ OpenAI. –í—ñ–¥–æ–º–∏–π –¥–æ–≤–≥–∏–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏–º–∏ –≤—ñ–∫–Ω–∞–º–∏ (200K —Ç–æ–∫–µ–Ω—ñ–≤).",
                "links": [
                  {
                    "title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç",
                    "href": "../level-2/context.html"
                  },
                  {
                    "title": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü",
                    "href": "../level-5/ai-safety.html"
                  }
                ]
              },
              {
                "text": "Google DeepMind",
                "desc": "Gemini (Ultra/Pro/Flash/Nano), –ø—Ä–æ—Ä–∏–≤ AlphaFold —É —Ñ–æ–ª–¥—ñ–Ω–≥—É –±—ñ–ª–∫—ñ–≤, –º–∞—Å–∏–≤–Ω–∞ –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∞ —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞. –ù–∞—Ç–∏–≤–Ω–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ –∑ –Ω–∞–π–±—ñ–ª—å—à–∏–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏–º–∏ –≤—ñ–∫–Ω–∞–º–∏ (1M+ —Ç–æ–∫–µ–Ω—ñ–≤).",
                "links": [
                  {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å",
                    "href": "multimodality.html"
                  },
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "foundation-models.html"
                  }
                ]
              },
              {
                "text": "Meta AI",
                "desc": "–í—ñ–¥–∫—Ä–∏—Ç–µ —Å—ñ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Llama (Llama 3, 3.1, 3.2). –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è FAIR. –ß–µ–º–ø—ñ–æ–Ω –≤—ñ–¥–∫—Ä–∏—Ç–æ–≥–æ –®–Ü ‚Äî –±—É–¥—å-—Ö—Ç–æ –º–æ–∂–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏, –∑–∞–ø—É—Å—Ç–∏—Ç–∏ —Ç–∞ —Ñ–∞–π–Ω-—Ç—é–Ω–∏—Ç–∏ —ó—Ö –º–æ–¥–µ–ª—ñ.",
                "links": [
                  {
                    "title": "–ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥",
                    "href": "../level-3/training-finetuning.html"
                  }
                ]
              },
              {
                "text": "Mistral AI",
                "desc": "–Ñ–≤—Ä–æ–ø–µ–π—Å—å–∫–∞ –®–Ü-–ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ—è (–ü–∞—Ä–∏–∂). –ú–æ–¥–µ–ª—ñ Mistral, Mixtral MoE. –ü—ñ–¥—Ö—ñ–¥ –≤—ñ–¥–∫—Ä–∏—Ç–∏—Ö –≤–∞–≥. –î–æ–≤–µ–ª–∏, —â–æ –º–µ–Ω—à—ñ —î–≤—Ä–æ–ø–µ–π—Å—å–∫—ñ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ—ó –º–æ–∂—É—Ç—å –∫–æ–Ω–∫—É—Ä—É–≤–∞—Ç–∏ –∑ –∞–º–µ—Ä–∏–∫–∞–Ω—Å—å–∫–∏–º–∏ –≥—ñ–≥–∞–Ω—Ç–∞–º–∏.",
                "links": [
                  {
                    "title": "–¢–∏–ø–∏ –º–æ–¥–µ–ª–µ–π",
                    "href": "../level-3/model-types.html"
                  }
                ]
              },
              {
                "text": "xAI (Grok)",
                "desc": "–®–Ü-–ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ—è –Ü–ª–æ–Ω–∞ –ú–∞—Å–∫–∞. –ú–æ–¥–µ–ª—ñ Grok –∑ –¥–æ—Å—Ç—É–ø–æ–º –¥–æ —Ä–µ–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö X (Twitter). –§–æ–∫—É—Å –Ω–∞ –∑–º–µ–Ω—à–µ–Ω–Ω—ñ —Ü–µ–Ω–∑—É—Ä–∏ —Ç–∞ –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó –∫–æ—Ä–∏—Å–Ω–æ—Å—Ç—ñ.",
                "links": []
              },
              {
                "text": "–ö–∏—Ç–∞–π—Å—å–∫—ñ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ—ó",
                "desc": "DeepSeek (R1 –º–æ–¥–µ–ª—å –º—ñ—Ä–∫—É–≤–∞–Ω—å, –∫–æ–Ω–∫—É—Ä—É—î –∑ o1), Alibaba (–≤—ñ–¥–∫—Ä–∏—Ç–µ —Å—ñ–º–µ–π—Å—Ç–≤–æ Qwen), Baidu (ERNIE), ByteDance (Doubao). –°—Ç—Ä—ñ–º–∫–æ —Å–∫–æ—Ä–æ—á—É—é—Ç—å —Ä–æ–∑—Ä–∏–≤ —ñ–∑ –∑–∞—Ö—ñ–¥–Ω–∏–º–∏ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ—è–º–∏.",
                "links": [
                  {
                    "title": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                    "href": "reasoning.html"
                  },
                  {
                    "title": "SOTA",
                    "href": "sota.html"
                  }
                ]
              },
              {
                "text": "–°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ—ó",
                "desc": "Stability AI (–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–æ–±—Ä–∞–∂–µ–Ω—å Stable Diffusion), Cohere (–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∏–π NLP), AI21 (–≥—ñ–±—Ä–∏–¥–Ω–∞ –º–æ–¥–µ–ª—å Jamba), Runway (–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–µ–æ). –õ—ñ–¥–µ—Ä–∏ —É —Å–≤–æ—ó—Ö –¥–æ–º–µ–Ω–∞—Ö.",
                "links": [
                  {
                    "title": "–î–∏—Ñ—É–∑—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "diffusion-models.html"
                  }
                ]
              },
              {
                "text": "–î–æ—Å–ª—ñ–¥–Ω–∏—Ü—å–∫—ñ —ñ–Ω—Å—Ç–∏—Ç—É—Ü—ñ—ó",
                "desc": "Allen AI (–ø–æ–≤–Ω—ñ—Å—Ç—é –≤—ñ–¥–∫—Ä–∏—Ç–∏–π OLMo), EleutherAI (–≤—ñ–¥–∫—Ä–∏—Ç–∞ –¥–æ—Å–ª—ñ–¥–Ω–∏—Ü—å–∫–∞ —Å–ø—ñ–ª—å–Ω–æ—Ç–∞), LAION (–≤—ñ–¥–∫—Ä–∏—Ç—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏). –ü—Ä–æ—Å—É–≤–∞—é—Ç—å –ø—Ä–æ–∑–æ—Ä—ñ, –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –®–Ü.",
                "links": [
                  {
                    "title": "–í—ñ–¥ –¥–∞–Ω–∏—Ö –¥–æ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-3/data-to-model.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Frontier Model",
                "def": "The most capable AI models at the cutting edge of performance, typically from major labs."
              },
              {
                "term": "Open-Weight",
                "def": "Models where the trained weights are publicly released, allowing anyone to run them locally."
              },
              {
                "term": "Closed-Source",
                "def": "Models accessible only via API, with weights and training details kept proprietary."
              },
              {
                "term": "MoE (Mixture of Experts)",
                "def": "Architecture where only a subset of model parameters activate per input, enabling larger models at lower compute cost."
              }
            ],
            "uk": [
              {
                "term": "–§—Ä–æ–Ω—Ç–∏—Ä–Ω–∞ –º–æ–¥–µ–ª—å",
                "def": "–ù–∞–π–∑–¥–∞—Ç–Ω—ñ—à—ñ –º–æ–¥–µ–ª—ñ –®–Ü –Ω–∞ –ø–µ—Ä–µ–¥–Ω—å–æ–º—É –∫—Ä–∞—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ, –∑–∞–∑–≤–∏—á–∞–π –≤—ñ–¥ –≤–µ–ª–∏–∫–∏—Ö –ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ–π."
              },
              {
                "term": "–í—ñ–¥–∫—Ä–∏—Ç—ñ –≤–∞–≥–∏",
                "def": "–ú–æ–¥–µ–ª—ñ, –¥–µ –Ω–∞–≤—á–µ–Ω—ñ –≤–∞–≥–∏ –ø—É–±–ª—ñ—á–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ñ, —â–æ –¥–æ–∑–≤–æ–ª—è—î –±—É–¥—å-–∫–æ–º—É –∑–∞–ø—É—Å–∫–∞—Ç–∏ —ó—Ö –ª–æ–∫–∞–ª—å–Ω–æ."
              },
              {
                "term": "–ó–∞–∫—Ä–∏—Ç–∏–π –∫–æ–¥",
                "def": "–ú–æ–¥–µ–ª—ñ, –¥–æ—Å—Ç—É–ø–Ω—ñ –ª–∏—à–µ —á–µ—Ä–µ–∑ API, –∑ –≤–∞–≥–∞–º–∏ —Ç–∞ –¥–µ—Ç–∞–ª—è–º–∏ –Ω–∞–≤—á–∞–Ω–Ω—è —è–∫ –ø—Ä–æ–ø—Ä—ñ—î—Ç–∞—Ä–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è."
              },
              {
                "term": "MoE (Mixture of Experts)",
                "def": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞, –¥–µ –ª–∏—à–µ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–æ–¥–µ–ª—ñ –∞–∫—Ç–∏–≤—É—î—Ç—å—Å—è –Ω–∞ –∫–æ–∂–Ω–∏–π –≤—Ö—ñ–¥, –¥–æ–∑–≤–æ–ª—è—é—á–∏ –±—ñ–ª—å—à—ñ –º–æ–¥–µ–ª—ñ –ø—Ä–∏ –º–µ–Ω—à—ñ–π –≤–∞—Ä—Ç–æ—Å—Ç—ñ –æ–±—á–∏—Å–ª–µ–Ω—å."
              }
            ]
          },
          "tips": {
            "en": [
              "Follow AI news aggregators to stay current ‚Äî the competitive landscape changes monthly",
              "Don't lock into one provider. Try models from different labs to understand their strengths",
              "Open-weight models (Llama, Qwen, Mistral) are great for learning since you can inspect and run them locally"
            ],
            "uk": [
              "–°–ª—ñ–¥–∫—É–π—Ç–µ –∑–∞ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞–º–∏ –Ω–æ–≤–∏–Ω –®–Ü ‚Äî –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏–π –ª–∞–Ω–¥—à–∞—Ñ—Ç –∑–º—ñ–Ω—é—î—Ç—å—Å—è —â–æ–º—ñ—Å—è—Ü—è",
              "–ù–µ –ø—Ä–∏–≤'—è–∑—É–π—Ç–µ—Å—å –¥–æ –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞. –°–ø—Ä–æ–±—É–π—Ç–µ –º–æ–¥–µ–ª—ñ –≤—ñ–¥ —Ä—ñ–∑–Ω–∏—Ö –ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ–π, —â–æ–± –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —ó—Ö —Å–∏–ª—å–Ω—ñ —Å—Ç–æ—Ä–æ–Ω–∏",
              "–ú–æ–¥–µ–ª—ñ –∑ –≤—ñ–¥–∫—Ä–∏—Ç–∏–º–∏ –≤–∞–≥–∞–º–∏ (Llama, Qwen, Mistral) —á—É–¥–æ–≤–æ –ø—ñ–¥—Ö–æ–¥—è—Ç—å –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è, –±–æ —ó—Ö –º–æ–∂–Ω–∞ –¥–æ—Å–ª—ñ–¥–∂—É–≤–∞—Ç–∏ —Ç–∞ –∑–∞–ø—É—Å–∫–∞—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ"
            ]
          },
          "related": [
            "Models",
            "Feed"
          ]
        },
        {
          "slug": "llm-and-gpt",
          "title": {
            "en": "LLM and GPT",
            "uk": "LLM —Ç–∞ GPT"
          },
          "desc": {
            "en": "Large Language Models and the GPT architecture that started the revolution.",
            "uk": "–í–µ–ª–∏–∫—ñ –º–æ–≤–Ω—ñ –º–æ–¥–µ–ª—ñ —Ç–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ GPT, —â–æ —Ä–æ–∑–ø–æ—á–∞–ª–∞ —Ä–µ–≤–æ–ª—é—Ü—ñ—é."
          },
          "overview": {
            "en": [
              "Large Language Models (LLMs) are neural networks trained on massive text datasets that can understand and generate human language. The \"large\" refers to both the number of parameters (billions) and the scale of training data (trillions of tokens from the internet). LLMs are the backbone of modern AI assistants like ChatGPT, Claude, and Gemini.",
              "GPT ‚Äî Generative Pre-trained Transformer ‚Äî is the specific architecture family from OpenAI that popularized LLMs. The key insight was that scaling up a simple next-token prediction objective on internet-scale data produces remarkably capable models. This pattern of \"pre-train at scale, then fine-tune for tasks\" has become the dominant paradigm across all AI labs."
            ],
            "uk": [
              "–í–µ–ª–∏–∫—ñ –º–æ–≤–Ω—ñ –º–æ–¥–µ–ª—ñ (LLM) ‚Äî —Ü–µ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂—ñ, –Ω–∞–≤—á–µ–Ω—ñ –Ω–∞ –º–∞—Å–∏–≤–Ω–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–∏—Ö, —â–æ –º–æ–∂—É—Ç—å —Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–∞ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –ª—é–¥—Å—å–∫—É –º–æ–≤—É. \"–í–µ–ª–∏–∫—ñ\" —Å—Ç–æ—Å—É—î—Ç—å—Å—è —è–∫ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ (–º—ñ–ª—å—è—Ä–¥–∏), —Ç–∞–∫ —ñ –º–∞—Å—à—Ç–∞–±—É –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö (—Ç—Ä–∏–ª—å–π–æ–Ω–∏ —Ç–æ–∫–µ–Ω—ñ–≤ –∑ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É). LLM —î –æ—Å–Ω–æ–≤–æ—é —Å—É—á–∞—Å–Ω–∏—Ö –®–Ü-–∞—Å–∏—Å—Ç–µ–Ω—Ç—ñ–≤ —è–∫ ChatGPT, Claude —Ç–∞ Gemini.",
              "GPT ‚Äî Generative Pre-trained Transformer ‚Äî —Ü–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–µ —Å—ñ–º–µ–π—Å—Ç–≤–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä –≤—ñ–¥ OpenAI, —â–æ –ø–æ–ø—É–ª—è—Ä–∏–∑—É–≤–∞–ª–æ LLM. –ö–ª—é—á–æ–≤–∞ —ñ–¥–µ—è –ø–æ–ª—è–≥–∞–ª–∞ –≤ —Ç–æ–º—É, —â–æ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –ø—Ä–æ—Å—Ç–æ—ó –∑–∞–¥–∞—á—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –Ω–∞ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞—Å—à—Ç–∞–±–Ω–∏—Ö –¥–∞–Ω–∏—Ö —Å—Ç–≤–æ—Ä—é—î –Ω–∞–¥–∑–≤–∏—á–∞–π–Ω–æ –∑–¥–∞—Ç–Ω—ñ –º–æ–¥–µ–ª—ñ. –¶–µ–π –ø–∞—Ç–µ—Ä–Ω \"–ø–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è –≤ –º–∞—Å—à—Ç–∞–±—ñ, –ø–æ—Ç—ñ–º —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –¥–ª—è –∑–∞–¥–∞—á\" —Å—Ç–∞–≤ –¥–æ–º—ñ–Ω—É—é—á–æ—é –ø–∞—Ä–∞–¥–∏–≥–º–æ—é –≤ —É—Å—ñ—Ö –®–Ü-–ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ—è—Ö."
            ]
          },
          "details": {
            "en": [
              {
                "text": "What is an LLM",
                "desc": "A neural network with billions of parameters trained on massive text data. It learns the statistical structure of language and can generate, analyze, translate, and reason about text.",
                "links": [
                  {
                    "title": "Neural Networks",
                    "href": "../level-3/neural-networks.html"
                  }
                ]
              },
              {
                "text": "The Transformer Architecture",
                "desc": "The 2017 \"Attention Is All You Need\" paper introduced self-attention ‚Äî allowing each token to attend to every other token in the sequence. This replaced slower recurrent architectures and enabled parallelization.",
                "links": [
                  {
                    "title": "Model Types",
                    "href": "../level-3/model-types.html"
                  }
                ]
              },
              {
                "text": "GPT: Generative Pre-trained Transformer",
                "desc": "OpenAI's decoder-only architecture. \"Generative\" = generates text, \"Pre-trained\" = trained on broad data first, \"Transformer\" = the underlying architecture. This design became the template for all modern LLMs.",
                "links": [
                  {
                    "title": "The Big Players",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "Next-Token Prediction",
                "desc": "LLMs generate text one token at a time, always predicting the most probable next token given all previous ones. This simple objective, at scale, produces remarkably capable models.",
                "links": [
                  {
                    "title": "Token",
                    "href": "../level-2/token.html"
                  }
                ]
              },
              {
                "text": "Scaling Laws",
                "desc": "Chinchilla and earlier research showed that model performance improves predictably with more compute, data, and parameters. This mathematical relationship drives the industry push toward larger models.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "foundation-models.html"
                  }
                ]
              },
              {
                "text": "Emergent Abilities",
                "desc": "At certain scales, models suddenly gain capabilities not present in smaller versions ‚Äî in-context learning, chain-of-thought reasoning, code generation. These emerge from scale, not explicit programming.",
                "links": [
                  {
                    "title": "Reasoning",
                    "href": "reasoning.html"
                  }
                ]
              },
              {
                "text": "Key Model Families",
                "desc": "GPT-4/o1 (OpenAI), Claude 3.5/4 (Anthropic), Gemini (Google), Llama 3 (Meta), Qwen 2.5 (Alibaba), Mistral/Mixtral (Mistral AI). Each has different strengths and trade-offs.",
                "links": [
                  {
                    "title": "The Big Players",
                    "href": "big-players.html"
                  },
                  {
                    "title": "SOTA",
                    "href": "sota.html"
                  }
                ]
              },
              {
                "text": "Model Sizes",
                "desc": "From 1B parameter \"small\" models (run on phones) to 1T+ parameter frontier models (require data centers). Common sizes: 7B, 13B, 34B, 70B, 405B. Larger usually = more capable but slower and more expensive.",
                "links": [
                  {
                    "title": "Model Optimization",
                    "href": "../level-3/model-optimization.html"
                  }
                ]
              },
              {
                "text": "Training Pipeline",
                "desc": "Pre-training (trillions of tokens, months of GPU time) ‚Üí Supervised Fine-Tuning with human-curated examples ‚Üí RLHF/DPO alignment to make models helpful and safe.",
                "links": [
                  {
                    "title": "Training & Fine-tuning",
                    "href": "../level-3/training-finetuning.html"
                  }
                ]
              },
              {
                "text": "Inference",
                "desc": "How models actually run: forward pass through the network, KV cache for efficient generation, batching multiple requests, streaming tokens to the user as they are generated.",
                "links": [
                  {
                    "title": "Hardware Basics",
                    "href": "../level-4/hardware.html"
                  },
                  {
                    "title": "API Providers",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–©–æ —Ç–∞–∫–µ LLM",
                "desc": "–ù–µ–π—Ä–æ–º–µ—Ä–µ–∂–∞ –∑ –º—ñ–ª—å—è—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤, –Ω–∞–≤—á–µ–Ω–∞ –Ω–∞ –º–∞—Å–∏–≤–Ω–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö. –í–æ–Ω–∞ –≤–∏–≤—á–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–æ–≤–∏ —ñ –º–æ–∂–µ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏, –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏, –ø–µ—Ä–µ–∫–ª–∞–¥–∞—Ç–∏ —Ç–∞ –º—ñ—Ä–∫—É–≤–∞—Ç–∏ –ø—Ä–æ —Ç–µ–∫—Å—Ç.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂",
                    "href": "../level-3/neural-networks.html"
                  }
                ]
              },
              {
                "text": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ Transformer",
                "desc": "–°—Ç–∞—Ç—Ç—è 2017 \"Attention Is All You Need\" –≤–≤–µ–ª–∞ —Å–∞–º–æ—É–≤–∞–≥—É ‚Äî –¥–æ–∑–≤–æ–ª—è—é—á–∏ –∫–æ–∂–Ω–æ–º—É —Ç–æ–∫–µ–Ω—É –∑–≤–µ—Ä—Ç–∞—Ç–∏ —É–≤–∞–≥—É –Ω–∞ –∫–æ–∂–µ–Ω —ñ–Ω—à–∏–π –≤ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ. –¶–µ –∑–∞–º—ñ–Ω–∏–ª–æ –ø–æ–≤—ñ–ª—å–Ω—ñ—à—ñ —Ä–µ–∫—É—Ä–µ–Ω—Ç–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ —Ç–∞ –¥–∞–ª–æ –ø–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—é.",
                "links": [
                  {
                    "title": "–¢–∏–ø–∏ –º–æ–¥–µ–ª–µ–π",
                    "href": "../level-3/model-types.html"
                  }
                ]
              },
              {
                "text": "GPT: Generative Pre-trained Transformer",
                "desc": "–î–µ–∫–æ–¥–µ—Ä-only –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ OpenAI. \"Generative\" = –≥–µ–Ω–µ—Ä—É—î —Ç–µ–∫—Å—Ç, \"Pre-trained\" = —Å–ø–æ—á–∞—Ç–∫—É –Ω–∞–≤—á–µ–Ω–∞ –Ω–∞ —à–∏—Ä–æ–∫–∏—Ö –¥–∞–Ω–∏—Ö, \"Transformer\" = –±–∞–∑–æ–≤–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞. –¶–µ–π –¥–∏–∑–∞–π–Ω —Å—Ç–∞–≤ —à–∞–±–ª–æ–Ω–æ–º –¥–ª—è –≤—Å—ñ—Ö —Å—É—á–∞—Å–Ω–∏—Ö LLM.",
                "links": [
                  {
                    "title": "–í–µ–ª–∏–∫—ñ –≥—Ä–∞–≤—Ü—ñ",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞",
                "desc": "LLM –≥–µ–Ω–µ—Ä—É—é—Ç—å —Ç–µ–∫—Å—Ç –ø–æ –æ–¥–Ω–æ–º—É —Ç–æ–∫–µ–Ω—É, –∑–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–¥–±–∞—á–∞—é—á–∏ –Ω–∞–π—ñ–º–æ–≤—ñ—Ä–Ω—ñ—à–∏–π –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ç–æ–∫–µ–Ω –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —É—Å—ñ—Ö –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö. –¶—è –ø—Ä–æ—Å—Ç–∞ –∑–∞–¥–∞—á–∞, —É –º–∞—Å—à—Ç–∞–±—ñ, —Å—Ç–≤–æ—Ä—é—î –Ω–∞–¥–∑–≤–∏—á–∞–π–Ω–æ –∑–¥–∞—Ç–Ω—ñ –º–æ–¥–µ–ª—ñ.",
                "links": [
                  {
                    "title": "–¢–æ–∫–µ–Ω",
                    "href": "../level-2/token.html"
                  }
                ]
              },
              {
                "text": "–ó–∞–∫–æ–Ω–∏ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è",
                "desc": "–î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è Chinchilla –ø–æ–∫–∞–∑–∞–ª–∏, —â–æ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –ø–æ–∫—Ä–∞—â—É—î—Ç—å—Å—è –ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω–æ –∑ –±—ñ–ª—å—à–∏–º–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è–º–∏, –¥–∞–Ω–∏–º–∏ —Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏. –¶–µ–π –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏–π –∑–≤'—è–∑–æ–∫ —Ä—É—Ö–∞—î —ñ–Ω–¥—É—Å—Ç—Ä—ñ—é –¥–æ –±—ñ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "foundation-models.html"
                  }
                ]
              },
              {
                "text": "–ï–º–µ—Ä–¥–∂–µ–Ω—Ç–Ω—ñ –∑–¥—ñ–±–Ω–æ—Å—Ç—ñ",
                "desc": "–ü—Ä–∏ –ø–µ–≤–Ω–∏—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö –º–æ–¥–µ–ª—ñ —Ä–∞–ø—Ç–æ–≤–æ –æ—Ç—Ä–∏–º—É—é—Ç—å –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ, –≤—ñ–¥—Å—É—Ç–Ω—ñ —É –º–µ–Ω—à–∏—Ö –≤–µ—Ä—Å—ñ—è—Ö ‚Äî –Ω–∞–≤—á–∞–Ω–Ω—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ, –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è –ª–∞–Ω—Ü—é–≥–æ–º –¥—É–º–æ–∫, –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–¥—É.",
                "links": [
                  {
                    "title": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                    "href": "reasoning.html"
                  }
                ]
              },
              {
                "text": "–ö–ª—é—á–æ–≤—ñ —Å—ñ–º–µ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π",
                "desc": "GPT-4/o1 (OpenAI), Claude 3.5/4 (Anthropic), Gemini (Google), Llama 3 (Meta), Qwen 2.5 (Alibaba), Mistral/Mixtral (Mistral AI). –ö–æ–∂–Ω–µ –º–∞—î —Å–≤–æ—ó –ø–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –∫–æ–º–ø—Ä–æ–º—ñ—Å–∏.",
                "links": [
                  {
                    "title": "–í–µ–ª–∏–∫—ñ –≥—Ä–∞–≤—Ü—ñ",
                    "href": "big-players.html"
                  },
                  {
                    "title": "SOTA",
                    "href": "sota.html"
                  }
                ]
              },
              {
                "text": "–†–æ–∑–º—ñ—Ä–∏ –º–æ–¥–µ–ª–µ–π",
                "desc": "–í—ñ–¥ 1B –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ \"–º–∞–ª–∏—Ö\" –º–æ–¥–µ–ª–µ–π (–∑–∞–ø—É—Å–∫–∞—é—Ç—å—Å—è –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–∞—Ö) –¥–æ 1T+ —Ñ—Ä–æ–Ω—Ç–∏—Ä–Ω–∏—Ö (–≤–∏–º–∞–≥–∞—é—Ç—å –¥–∞—Ç–∞-—Ü–µ–Ω—Ç—Ä—ñ–≤). –ü–æ—à–∏—Ä–µ–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏: 7B, 13B, 34B, 70B, 405B. –ë—ñ–ª—å—à–µ –∑–∞–∑–≤–∏—á–∞–π = –∑–¥–∞—Ç–Ω—ñ—à–µ, –∞–ª–µ –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ.",
                "links": [
                  {
                    "title": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π",
                    "href": "../level-3/model-optimization.html"
                  }
                ]
              },
              {
                "text": "–ü–∞–π–ø–ª–∞–π–Ω –Ω–∞–≤—á–∞–Ω–Ω—è",
                "desc": "–ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è (—Ç—Ä–∏–ª—å–π–æ–Ω–∏ —Ç–æ–∫–µ–Ω—ñ–≤, –º—ñ—Å—è—Ü—ñ GPU —á–∞—Å—É) ‚Üí –ö–µ—Ä–æ–≤–∞–Ω–µ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –∑ –∫—É—Ä–∞—Ç–æ—Ä—Å—å–∫–∏–º–∏ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏ ‚Üí RLHF/DPO –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –¥–ª—è –∫–æ—Ä–∏—Å–Ω–æ—Å—Ç—ñ —Ç–∞ –±–µ–∑–ø–µ–∫–∏.",
                "links": [
                  {
                    "title": "–ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥",
                    "href": "../level-3/training-finetuning.html"
                  }
                ]
              },
              {
                "text": "–Ü–Ω—Ñ–µ—Ä–µ–Ω—Å",
                "desc": "–Ø–∫ –º–æ–¥–µ–ª—ñ –ø—Ä–∞—Ü—é—é—Ç—å: –ø—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥ —á–µ—Ä–µ–∑ –º–µ—Ä–µ–∂—É, KV –∫–µ—à –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—ó –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó, –±–∞—Ç—á–∏–Ω–≥ –∑–∞–ø–∏—Ç—ñ–≤, —Å—Ç—Ä–∏–º—ñ–Ω–≥ —Ç–æ–∫–µ–Ω—ñ–≤ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É –ø–æ –º—ñ—Ä—ñ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è",
                    "href": "../level-4/hardware.html"
                  },
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "LLM",
                "def": "Large Language Model ‚Äî a neural network with billions of parameters trained to predict and generate text."
              },
              {
                "term": "Transformer",
                "def": "Neural network architecture using self-attention, enabling parallel processing of sequences."
              },
              {
                "term": "Next-Token Prediction",
                "def": "The core training objective: given previous tokens, predict the most likely next one."
              },
              {
                "term": "Scaling Laws",
                "def": "Mathematical relationships showing model performance improves predictably with more compute, data, and parameters."
              },
              {
                "term": "Autoregressive",
                "def": "Generating output one token at a time, where each new token depends on all previous ones."
              }
            ],
            "uk": [
              {
                "term": "LLM",
                "def": "–í–µ–ª–∏–∫–∞ –º–æ–≤–Ω–∞ –º–æ–¥–µ–ª—å ‚Äî –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂–∞ –∑ –º—ñ–ª—å—è—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤, –Ω–∞–≤—á–µ–Ω–∞ –ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞—Ç–∏ —Ç–∞ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —Ç–µ–∫—Å—Ç."
              },
              {
                "term": "Transformer",
                "def": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂—ñ –∑ —Å–∞–º–æ—É–≤–∞–≥–æ—é, —â–æ –¥–æ–∑–≤–æ–ª—è—î –ø–∞—Ä–∞–ª–µ–ª—å–Ω—É –æ–±—Ä–æ–±–∫—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π."
              },
              {
                "term": "–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞",
                "def": "–û—Å–Ω–æ–≤–Ω–∞ –∑–∞–¥–∞—á–∞ –Ω–∞–≤—á–∞–Ω–Ω—è: –º–∞—é—á–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ —Ç–æ–∫–µ–Ω–∏, –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ –Ω–∞–π—ñ–º–æ–≤—ñ—Ä–Ω—ñ—à–∏–π –Ω–∞—Å—Ç—É–ø–Ω–∏–π."
              },
              {
                "term": "–ó–∞–∫–æ–Ω–∏ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è",
                "def": "–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω—ñ –∑–≤'—è–∑–∫–∏, —â–æ –ø–æ–∫–∞–∑—É—é—Ç—å –ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ –∑ –±—ñ–ª—å—à–∏–º–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è–º–∏, –¥–∞–Ω–∏–º–∏ —Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."
              },
              {
                "term": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å–∏–≤–Ω–∏–π",
                "def": "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø–æ –æ–¥–Ω–æ–º—É —Ç–æ–∫–µ–Ω—É –∑–∞ —Ä–∞–∑, –¥–µ –∫–æ–∂–µ–Ω –Ω–æ–≤–∏–π —Ç–æ–∫–µ–Ω –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ —É—Å—ñ—Ö –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö."
              }
            ]
          },
          "tips": {
            "en": [
              "When comparing LLMs, check both the base model and its fine-tuned variants ‚Äî a well-tuned 70B can outperform a poorly-tuned 405B model",
              "Model size alone doesn't determine quality ‚Äî architecture, training data quality, and alignment methods matter just as much",
              "For cost-effective development, prototype with frontier models (GPT-4, Claude), then evaluate if a smaller open model can handle your specific use case"
            ],
            "uk": [
              "–ü–æ—Ä—ñ–≤–Ω—é—é—á–∏ LLM, –ø–µ—Ä–µ–≤—ñ—Ä—è–π—Ç–µ —è–∫ –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å —Ç–∞–∫ —ñ —Ñ–∞–π–Ω-—Ç—é–Ω–µ–Ω—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ ‚Äî –¥–æ–±—Ä–µ —Ç—é–Ω–µ–Ω–∞ 70B –º–æ–∂–µ –ø–µ—Ä–µ–≤–µ—Ä—à–∏—Ç–∏ –ø–æ–≥–∞–Ω–æ —Ç—é–Ω–µ–Ω—É 405B",
              "–†–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ —Å–∞–º –ø–æ —Å–æ–±—ñ –Ω–µ –≤–∏–∑–Ω–∞—á–∞—î —è–∫—ñ—Å—Ç—å ‚Äî –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞, —è–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö —Ç–∞ –º–µ—Ç–æ–¥–∏ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –≤–∞–∂–∞—Ç—å –Ω–µ –º–µ–Ω—à–µ",
              "–î–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—ó —Ä–æ–∑—Ä–æ–±–∫–∏ –ø—Ä–æ—Ç–æ—Ç–∏–ø—É–π—Ç–µ –∑ —Ñ—Ä–æ–Ω—Ç–∏—Ä–Ω–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ (GPT-4, Claude), –ø–æ—Ç—ñ–º –æ—Ü—ñ–Ω—ñ—Ç—å —á–∏ –º–µ–Ω—à–∞ –≤—ñ–¥–∫—Ä–∏—Ç–∞ –º–æ–¥–µ–ª—å –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –≤–∞—à–æ–≥–æ –≤–∏–ø–∞–¥–∫—É"
            ]
          },
          "related": [
            "Models",
            "Feed"
          ]
        },
        {
          "slug": "diffusion-models",
          "title": {
            "en": "Diffusion Models",
            "uk": "–î–∏—Ñ—É–∑—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ"
          },
          "desc": {
            "en": "Understanding image and video generation with diffusion-based approaches.",
            "uk": "–†–æ–∑—É–º—ñ–Ω–Ω—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑–æ–±—Ä–∞–∂–µ–Ω—å —Ç–∞ –≤—ñ–¥–µ–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –¥–∏—Ñ—É–∑—ñ–π–Ω–∏—Ö –ø—ñ–¥—Ö–æ–¥—ñ–≤."
          },
          "overview": {
            "en": [
              "Diffusion models are the dominant approach for AI image and video generation. They work by learning to reverse a noise-adding process: given an image gradually corrupted into random noise, the model learns to denoise it step by step. At generation time, the model starts with pure noise and iteratively refines it into a coherent image guided by a text prompt.",
              "This approach has proven remarkably powerful. Models like Stable Diffusion, DALL-E 3, Midjourney, and Flux can generate photorealistic images, artistic illustrations, and even video from text descriptions. The ecosystem includes customization tools like LoRA adapters and ControlNet that allow fine-tuning generation for specific styles or structural constraints."
            ],
            "uk": [
              "–î–∏—Ñ—É–∑—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ ‚Äî —Ü–µ –¥–æ–º—ñ–Ω—É—é—á–∏–π –ø—ñ–¥—Ö—ñ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑–æ–±—Ä–∞–∂–µ–Ω—å —Ç–∞ –≤—ñ–¥–µ–æ –∑ –®–Ü. –í–æ–Ω–∏ –ø—Ä–∞—Ü—é—é—Ç—å —á–µ—Ä–µ–∑ –Ω–∞–≤—á–∞–Ω–Ω—è –æ–±–µ—Ä–Ω–µ–Ω–Ω—é –ø—Ä–æ—Ü–µ—Å—É –¥–æ–¥–∞–≤–∞–Ω–Ω—è —à—É–º—É: –º–∞—é—á–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –ø–æ—Å—Ç—É–ø–æ–≤–æ –∑—ñ–ø—Å–æ–≤–∞–Ω–µ –¥–æ –≤–∏–ø–∞–¥–∫–æ–≤–æ–≥–æ —à—É–º—É, –º–æ–¥–µ–ª—å –≤—á–∏—Ç—å—Å—è —Ä–æ–∑—à—É–º–ª—é–≤–∞—Ç–∏ –π–æ–≥–æ –∫—Ä–æ–∫ –∑–∞ –∫—Ä–æ–∫–æ–º. –ü—ñ–¥ —á–∞—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –º–æ–¥–µ–ª—å –ø–æ—á–∏–Ω–∞—î –∑ —á–∏—Å—Ç–æ–≥–æ —à—É–º—É —Ç–∞ —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É—Ç–æ—á–Ω—é—î –π–æ–≥–æ —É –∑–≤'—è–∑–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –∫–µ—Ä–æ–≤–∞–Ω–µ —Ç–µ–∫—Å—Ç–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º.",
              "–¶–µ–π –ø—ñ–¥—Ö—ñ–¥ –≤–∏—è–≤–∏–≤—Å—è –Ω–∞–¥–∑–≤–∏—á–∞–π–Ω–æ –ø–æ—Ç—É–∂–Ω–∏–º. –ú–æ–¥–µ–ª—ñ —è–∫ Stable Diffusion, DALL-E 3, Midjourney —Ç–∞ Flux –º–æ–∂—É—Ç—å –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —Ñ–æ—Ç–æ—Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —Ö—É–¥–æ–∂–Ω—ñ —ñ–ª—é—Å—Ç—Ä–∞—Ü—ñ—ó —ñ –Ω–∞–≤—ñ—Ç—å –≤—ñ–¥–µ–æ –∑ —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –æ–ø–∏—Å—ñ–≤. –ï–∫–æ—Å–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞—î —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –∫–∞—Å—Ç–æ–º—ñ–∑–∞—Ü—ñ—ó —è–∫ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∏ —Ç–∞ ControlNet, —â–æ –¥–æ–∑–≤–æ–ª—è—é—Ç—å —Ç–æ–Ω–∫–æ –Ω–∞–ª–∞—à—Ç–æ–≤—É–≤–∞—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Å—Ç–∏–ª—ñ–≤ –∞–±–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∏—Ö –æ–±–º–µ–∂–µ–Ω—å."
            ]
          },
          "details": {
            "en": [
              {
                "text": "How Diffusion Works",
                "desc": "Forward process gradually adds noise to an image until it becomes random static. The model learns to reverse this ‚Äî starting from noise and iteratively denoising into a coherent image.",
                "links": []
              },
              {
                "text": "Text Conditioning",
                "desc": "CLIP or T5 text encoders translate your text prompt into a guidance signal. This signal steers the denoising process to produce images matching your description.",
                "links": [
                  {
                    "title": "Prompt",
                    "href": "../level-2/prompt.html"
                  }
                ]
              },
              {
                "text": "Key Models",
                "desc": "Stable Diffusion 3 (open), DALL-E 3 (OpenAI), Midjourney v6 (subscription), Flux (Black Forest Labs), Ideogram (text rendering). Each has different strengths in style, quality, and control.",
                "links": [
                  {
                    "title": "The Big Players",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "Latent Diffusion",
                "desc": "Working in compressed latent space (64x smaller than raw pixels) makes generation fast and memory-efficient. A VAE encoder/decoder bridges between pixel and latent spaces.",
                "links": []
              },
              {
                "text": "ControlNet",
                "desc": "Add structural guidance via edge maps, depth maps, pose estimation, or segmentation masks. Lets you control the composition while the diffusion model handles details and style.",
                "links": []
              },
              {
                "text": "LoRA Adapters",
                "desc": "Lightweight fine-tuning (typically 10-100MB) that teaches the model new styles, characters, or concepts without full retraining. Community shares thousands of LoRAs on CivitAI and HuggingFace.",
                "links": [
                  {
                    "title": "Training & Fine-tuning",
                    "href": "../level-3/training-finetuning.html"
                  }
                ]
              },
              {
                "text": "Video Generation",
                "desc": "Sora (OpenAI), Runway Gen-3, Kling (Kuaishou), Pika extend diffusion to temporal sequences. Video gen adds motion consistency and temporal coherence challenges.",
                "links": [
                  {
                    "title": "Multimodality",
                    "href": "multimodality.html"
                  }
                ]
              },
              {
                "text": "Inpainting & Outpainting",
                "desc": "Edit specific regions of generated or real images. Inpainting replaces a masked area, outpainting extends the image beyond its borders. Both use the same diffusion process.",
                "links": []
              },
              {
                "text": "Diffusion vs GANs",
                "desc": "GANs (Generative Adversarial Networks) are faster but harder to train and less diverse. Diffusion models produce higher quality and more varied outputs at the cost of slower generation.",
                "links": []
              },
              {
                "text": "Local Tools",
                "desc": "ComfyUI (node-based, flexible) and Automatic1111 (web UI, user-friendly) are popular open-source interfaces for running Stable Diffusion locally on your own GPU.",
                "links": [
                  {
                    "title": "Hardware Basics",
                    "href": "../level-4/hardware.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–Ø–∫ –ø—Ä–∞—Ü—é—î –¥–∏—Ñ—É–∑—ñ—è",
                "desc": "–ü—Ä—è–º–∏–π –ø—Ä–æ—Ü–µ—Å –ø–æ—Å—Ç—É–ø–æ–≤–æ –¥–æ–¥–∞—î —à—É–º –¥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –ø–æ–∫–∏ –≤–æ–Ω–æ –Ω–µ —Å—Ç–∞—î –≤–∏–ø–∞–¥–∫–æ–≤–æ—é —Å—Ç–∞—Ç–∏–∫–æ—é. –ú–æ–¥–µ–ª—å –≤—á–∏—Ç—å—Å—è –æ–±–µ—Ä—Ç–∞—Ç–∏ —Ü–µ ‚Äî –ø–æ—á–∏–Ω–∞—é—á–∏ –∑ —à—É–º—É —Ç–∞ —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —Ä–æ–∑—à—É–º–ª—é—é—á–∏ –≤ –∑–≤'—è–∑–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è.",
                "links": []
              },
              {
                "text": "–¢–µ–∫—Å—Ç–æ–≤–µ –∫–æ–Ω–¥–∏—Ü—ñ–æ–Ω—É–≤–∞–Ω–Ω—è",
                "desc": "CLIP –∞–±–æ T5 —Ç–µ–∫—Å—Ç–æ–≤—ñ –µ–Ω–∫–æ–¥–µ—Ä–∏ –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—é—Ç—å –≤–∞—à –ø—Ä–æ–º–ø—Ç —É —Å–∏–≥–Ω–∞–ª –∫–µ—Ä—É–≤–∞–Ω–Ω—è. –¶–µ–π —Å–∏–≥–Ω–∞–ª —Å–ø—Ä—è–º–æ–≤—É—î –ø—Ä–æ—Ü–µ—Å —Ä–æ–∑—à—É–º–ª–µ–Ω–Ω—è –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ –≤–∞—à–æ–≥–æ –æ–ø–∏—Å—É.",
                "links": [
                  {
                    "title": "–ü—Ä–æ–º–ø—Ç",
                    "href": "../level-2/prompt.html"
                  }
                ]
              },
              {
                "text": "–ö–ª—é—á–æ–≤—ñ –º–æ–¥–µ–ª—ñ",
                "desc": "Stable Diffusion 3 (–≤—ñ–¥–∫—Ä–∏—Ç–∏–π), DALL-E 3 (OpenAI), Midjourney v6 (–ø—ñ–¥–ø–∏—Å–∫–∞), Flux (Black Forest Labs), Ideogram (—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Ç–µ–∫—Å—Ç—É). –ö–æ–∂–µ–Ω –º–∞—î —Å–≤–æ—ó –ø–µ—Ä–µ–≤–∞–≥–∏ –≤ —Å—Ç–∏–ª—ñ, —è–∫–æ—Å—Ç—ñ —Ç–∞ –∫–æ–Ω—Ç—Ä–æ–ª—ñ.",
                "links": [
                  {
                    "title": "–í–µ–ª–∏–∫—ñ –≥—Ä–∞–≤—Ü—ñ",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "–õ–∞—Ç–µ–Ω—Ç–Ω–∞ –¥–∏—Ñ—É–∑—ñ—è",
                "desc": "–†–æ–±–æ—Ç–∞ —É —Å—Ç–∏—Å–Ω—É—Ç–æ–º—É –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ (—É 64 —Ä–∞–∑–∏ –º–µ–Ω—à–µ –∑–∞ —Å–∏—Ä—ñ –ø—ñ–∫—Å–µ–ª—ñ) —Ä–æ–±–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é —à–≤–∏–¥–∫–æ—é —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—é –∑–∞ –ø–∞–º'—è—Ç—Ç—é. VAE encoder/decoder –∑–≤'—è–∑—É—î –ø—ñ–∫—Å–µ–ª—å–Ω–∏–π —Ç–∞ –ª–∞—Ç–µ–Ω—Ç–Ω–∏–π –ø—Ä–æ—Å—Ç–æ—Ä–∏.",
                "links": []
              },
              {
                "text": "ControlNet",
                "desc": "–î–æ–¥–∞–≤–∞–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –∫–µ—Ä—É–≤–∞–Ω–Ω—è —á–µ—Ä–µ–∑ –∫–∞—Ä—Ç–∏ –∫—Ä–∞—ó–≤, –≥–ª–∏–±–∏–Ω–∏, –æ—Ü—ñ–Ω–∫—É –ø–æ–∑–∏ –∞–±–æ –º–∞—Å–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó. –î–æ–∑–≤–æ–ª—è—î –∫–æ–Ω—Ç—Ä–æ–ª—é–≤–∞—Ç–∏ –∫–æ–º–ø–æ–∑–∏—Ü—ñ—é, –ø–æ–∫–∏ –¥–∏—Ñ—É–∑—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å –ø—Ä–∞—Ü—é—î –∑ –¥–µ—Ç–∞–ª—è–º–∏ —Ç–∞ —Å—Ç–∏–ª–µ–º.",
                "links": []
              },
              {
                "text": "LoRA –∞–¥–∞–ø—Ç–µ—Ä–∏",
                "desc": "–õ–µ–≥–∫–µ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ (–∑–∞–∑–≤–∏—á–∞–π 10-100–ú–ë), —â–æ –≤—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–æ–≤–∏–º —Å—Ç–∏–ª—è–º, –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º –∞–±–æ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—è–º –±–µ–∑ –ø–æ–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è. –°–ø—ñ–ª—å–Ω–æ—Ç–∞ –¥—ñ–ª–∏—Ç—å—Å—è —Ç–∏—Å—è—á–∞–º–∏ LoRA –Ω–∞ CivitAI —Ç–∞ HuggingFace.",
                "links": [
                  {
                    "title": "–ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥",
                    "href": "../level-3/training-finetuning.html"
                  }
                ]
              },
              {
                "text": "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–µ–æ",
                "desc": "Sora (OpenAI), Runway Gen-3, Kling (Kuaishou), Pika —Ä–æ–∑—à–∏—Ä—é—é—Ç—å –¥–∏—Ñ—É–∑—ñ—é –Ω–∞ —á–∞—Å–æ–≤—ñ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ. –í—ñ–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–æ–¥–∞—î –≤–∏–∫–ª–∏–∫–∏ —á–∞—Å–æ–≤–æ—ó –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—ñ —Ç–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ —Ä—É—Ö—É.",
                "links": [
                  {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å",
                    "href": "multimodality.html"
                  }
                ]
              },
              {
                "text": "–Ü–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥ —Ç–∞ –∞—É—Ç–ø–µ–π–Ω—Ç–∏–Ω–≥",
                "desc": "–†–µ–¥–∞–≥—É–≤–∞–Ω–Ω—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –æ–±–ª–∞—Å—Ç–µ–π –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏—Ö –∞–±–æ —Ä–µ–∞–ª—å–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å. –Ü–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥ –∑–∞–º—ñ–Ω—é—î –∑–∞–º–∞—Å–∫–æ–≤–∞–Ω—É –æ–±–ª–∞—Å—Ç—å, –∞—É—Ç–ø–µ–π–Ω—Ç–∏–Ω–≥ —Ä–æ–∑—à–∏—Ä—é—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞ –π–æ–≥–æ –º–µ–∂—ñ.",
                "links": []
              },
              {
                "text": "–î–∏—Ñ—É–∑—ñ—è vs GAN",
                "desc": "GAN (–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ñ –∑–º–∞–≥–∞–ª—å–Ω—ñ –º–µ—Ä–µ–∂—ñ) —à–≤–∏–¥—à—ñ, –∞–ª–µ —Å–∫–ª–∞–¥–Ω—ñ—à—ñ –≤ –Ω–∞–≤—á–∞–Ω–Ω—ñ —Ç–∞ –º–µ–Ω—à —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ. –î–∏—Ñ—É–∑—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ –¥–∞—é—Ç—å –≤–∏—â—É —è–∫—ñ—Å—Ç—å —Ç–∞ –±—ñ–ª—å—à —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–∞ —Ä–∞—Ö—É–Ω–æ–∫ –ø–æ–≤—ñ–ª—å–Ω—ñ—à–æ—ó –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó.",
                "links": []
              },
              {
                "text": "–õ–æ–∫–∞–ª—å–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏",
                "desc": "ComfyUI (–≤—É–∑–ª–æ–≤–∏–π, –≥–Ω—É—á–∫–∏–π) —Ç–∞ Automatic1111 (–≤–µ–± UI, –∑—Ä—É—á–Ω–∏–π) ‚Äî –ø–æ–ø—É–ª—è—Ä–Ω—ñ –≤—ñ–¥–∫—Ä–∏—Ç—ñ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫—É Stable Diffusion –Ω–∞ –≤–ª–∞—Å–Ω–æ–º—É GPU.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è",
                    "href": "../level-4/hardware.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Diffusion",
                "def": "Image generation process that starts from random noise and iteratively refines it into a coherent output."
              },
              {
                "term": "Latent Space",
                "def": "Compressed mathematical representation of images that models work in for efficiency."
              },
              {
                "term": "LoRA",
                "def": "Low-Rank Adaptation ‚Äî a lightweight method to customize models without full retraining."
              },
              {
                "term": "ControlNet",
                "def": "Extension that adds structural guidance (edges, poses, depth) to diffusion generation."
              }
            ],
            "uk": [
              {
                "term": "–î–∏—Ñ—É–∑—ñ—è",
                "def": "–ü—Ä–æ—Ü–µ—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑–æ–±—Ä–∞–∂–µ–Ω—å, —â–æ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ –≤–∏–ø–∞–¥–∫–æ–≤–æ–≥–æ —à—É–º—É —Ç–∞ —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É—Ç–æ—á–Ω—é—î –π–æ–≥–æ —É –∑–≤'—è–∑–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç."
              },
              {
                "term": "–õ–∞—Ç–µ–Ω—Ç–Ω–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä",
                "def": "–°—Ç–∏—Å–Ω—É—Ç–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å, –≤ —è–∫–æ–º—É –º–æ–¥–µ–ª—ñ –ø—Ä–∞—Ü—é—é—Ç—å –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ."
              },
              {
                "term": "LoRA",
                "def": "Low-Rank Adaptation ‚Äî –ª–µ–≥–∫–∏–π –º–µ—Ç–æ–¥ –∫–∞—Å—Ç–æ–º—ñ–∑–∞—Ü—ñ—ó –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–æ–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è."
              },
              {
                "term": "ControlNet",
                "def": "–†–æ–∑—à–∏—Ä–µ–Ω–Ω—è, —â–æ –¥–æ–¥–∞—î —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–µ –∫–µ—Ä—É–≤–∞–Ω–Ω—è (–∫—Ä–∞—ó, –ø–æ–∑–∏, –≥–ª–∏–±–∏–Ω–∞) –¥–æ –¥–∏—Ñ—É–∑—ñ–π–Ω–æ—ó –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó."
              }
            ]
          },
          "tips": {
            "en": [
              "For best image generation results, study existing images on platforms like CivitAI to learn which keywords and styles produce consistent quality",
              "Use ControlNet or reference images to maintain consistency across multiple generations ‚Äî pure text prompts alone give unpredictable variation",
              "Video generation AI is progressing fast but still struggles with physics and temporal consistency ‚Äî use it for creative exploration, not production"
            ],
            "uk": [
              "–î–ª—è –Ω–∞–π–∫—Ä–∞—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑–æ–±—Ä–∞–∂–µ–Ω—å –≤–∏–≤—á—ñ—Ç—å –Ω–∞—è–≤–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö —è–∫ CivitAI, —â–æ–± –¥—ñ–∑–Ω–∞—Ç–∏—Å—å —è–∫—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–∞—é—Ç—å —Å—Ç–∞–±—ñ–ª—å–Ω—É —è–∫—ñ—Å—Ç—å",
              "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ ControlNet –∞–±–æ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ –º—ñ–∂ –∫—ñ–ª—å–∫–æ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è–º–∏ ‚Äî —á–∏—Å—Ç—ñ —Ç–µ–∫—Å—Ç–æ–≤—ñ –ø—Ä–æ–º–ø—Ç–∏ –¥–∞—é—Ç—å –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω—ñ –≤–∞—Ä—ñ–∞—Ü—ñ—ó",
              "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–µ–æ –®–Ü –ø—Ä–æ–≥—Ä–µ—Å—É—î —à–≤–∏–¥–∫–æ, –∞–ª–µ –≤—Å–µ —â–µ –º–∞—î –ø—Ä–æ–±–ª–µ–º–∏ –∑ —Ñ—ñ–∑–∏–∫–æ—é —Ç–∞ —á–∞—Å–æ–≤–æ—é –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—é ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è, –Ω–µ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω—É"
            ]
          },
          "related": [
            "Feed"
          ]
        },
        {
          "slug": "multimodality",
          "title": {
            "en": "Multimodality",
            "uk": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å"
          },
          "desc": {
            "en": "AI models that work with multiple data types simultaneously.",
            "uk": "–ú–æ–¥–µ–ª—ñ –®–Ü, —â–æ –ø—Ä–∞—Ü—é—é—Ç—å –∑ –∫—ñ–ª—å–∫–æ–º–∞ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–∏—Ö –æ–¥–Ω–æ—á–∞—Å–Ω–æ."
          },
          "overview": {
            "en": [
              "Multimodal AI refers to models that can process and generate multiple types of data ‚Äî text, images, audio, video ‚Äî within a single system. Rather than separate models for each data type, modern multimodal models understand the relationships between modalities, enabling powerful capabilities like describing images, answering questions about documents, or generating images from text.",
              "The trend toward multimodality is accelerating. GPT-4V, Claude Vision, and Gemini can all analyze images alongside text. Gemini processes audio and video natively. This convergence means that a single model can increasingly handle tasks that previously required specialized pipelines."
            ],
            "uk": [
              "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∏–π –®–Ü —Å—Ç–æ—Å—É—î—Ç—å—Å—è –º–æ–¥–µ–ª–µ–π, —â–æ –º–æ–∂—É—Ç—å –æ–±—Ä–æ–±–ª—è—Ç–∏ —Ç–∞ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∫—ñ–ª—å–∫–∞ —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö ‚Äî —Ç–µ–∫—Å—Ç, –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –∞—É–¥—ñ–æ, –≤—ñ–¥–µ–æ ‚Äî –≤ –æ–¥–Ω—ñ–π —Å–∏—Å—Ç–µ–º—ñ. –ó–∞–º—ñ—Å—Ç—å –æ–∫—Ä–µ–º–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–∏–ø—É –¥–∞–Ω–∏—Ö, —Å—É—á–∞—Å–Ω—ñ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ —Ä–æ–∑—É–º—ñ—é—Ç—å –∑–≤'—è–∑–∫–∏ –º—ñ–∂ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ –ø–æ—Ç—É–∂–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —è–∫ –æ–ø–∏—Å –∑–æ–±—Ä–∞–∂–µ–Ω—å, –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è –ø—Ä–æ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –∞–±–æ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–æ–±—Ä–∞–∂–µ–Ω—å –∑ —Ç–µ–∫—Å—Ç—É.",
              "–¢—Ä–µ–Ω–¥ –¥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ –ø—Ä–∏—Å–∫–æ—Ä—é—î—Ç—å—Å—è. GPT-4V, Claude Vision —Ç–∞ Gemini –º–æ–∂—É—Ç—å –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä–∞–∑–æ–º –∑ —Ç–µ–∫—Å—Ç–æ–º. Gemini –æ–±—Ä–æ–±–ª—è—î –∞—É–¥—ñ–æ —Ç–∞ –≤—ñ–¥–µ–æ –Ω–∞—Ç–∏–≤–Ω–æ. –¶—è –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü—ñ—è –æ–∑–Ω–∞—á–∞—î, —â–æ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –º–æ–∂–µ –≤—Å–µ –±—ñ–ª—å—à–µ –æ–±—Ä–æ–±–ª—è—Ç–∏ –∑–∞–¥–∞—á—ñ, —â–æ —Ä–∞–Ω—ñ—à–µ –≤–∏–º–∞–≥–∞–ª–∏ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –ø–∞–π–ø–ª–∞–π–Ω—ñ–≤."
            ]
          },
          "details": {
            "en": [
              {
                "text": "What is Multimodality",
                "desc": "A single model processing and generating multiple data types ‚Äî text, images, audio, video ‚Äî within one unified system. Moving beyond text-only AI.",
                "links": [
                  {
                    "title": "Data Type Classification",
                    "href": "data-classification.html"
                  }
                ]
              },
              {
                "text": "Vision-Language Models",
                "desc": "GPT-4V, Claude Vision (Sonnet/Opus), Gemini Pro Vision. These models \"see\" images and reason about them using natural language ‚Äî describing scenes, reading text, analyzing charts.",
                "links": [
                  {
                    "title": "The Big Players",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "Image Understanding",
                "desc": "Scene description, OCR (reading text from images), chart/diagram analysis, visual question answering. Models can analyze screenshots, photos, documents, and diagrams.",
                "links": []
              },
              {
                "text": "Audio Understanding & Generation",
                "desc": "Whisper (OpenAI) transcribes speech to text. TTS (text-to-speech) models synthesize natural voice. Voice cloning reproduces a specific person's voice from samples.",
                "links": []
              },
              {
                "text": "Document Understanding",
                "desc": "Parsing complex layouts ‚Äî PDFs, invoices, handwritten text, multi-column documents. Combines OCR with language understanding for intelligent data extraction.",
                "links": []
              },
              {
                "text": "Cross-Modal Generation",
                "desc": "Text-to-image (DALL-E, Midjourney), image-to-text (captioning), text-to-audio (TTS), audio-to-text (transcription). Converting between data types seamlessly.",
                "links": [
                  {
                    "title": "Diffusion Models",
                    "href": "diffusion-models.html"
                  }
                ]
              },
              {
                "text": "Video Understanding",
                "desc": "Temporal analysis, action recognition, video QA ‚Äî understanding what happens across frames over time. More complex than single-image analysis.",
                "links": []
              },
              {
                "text": "Native vs Adapter Multimodality",
                "desc": "Some models (Gemini) are natively multimodal from pre-training. Others bolt vision adapters onto existing text models. Native tends to be more capable and efficient.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "foundation-models.html"
                  }
                ]
              },
              {
                "text": "Gemini Approach",
                "desc": "Google's Gemini was pre-trained natively on text + images + audio + video simultaneously. This gives deeper cross-modal understanding compared to adapter-based approaches.",
                "links": [
                  {
                    "title": "The Big Players",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "Real-World Applications",
                "desc": "Accessibility tools (image descriptions for blind users), content moderation (detecting harmful images), medical imaging, autonomous driving, document processing pipelines.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "–©–æ —Ç–∞–∫–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å",
                "desc": "–û–¥–Ω–∞ –º–æ–¥–µ–ª—å –æ–±—Ä–æ–±–ª—è—î —Ç–∞ –≥–µ–Ω–µ—Ä—É—î –∫—ñ–ª—å–∫–∞ —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö ‚Äî —Ç–µ–∫—Å—Ç, –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –∞—É–¥—ñ–æ, –≤—ñ–¥–µ–æ ‚Äî –≤ –æ–¥–Ω—ñ–π —É–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω—ñ–π —Å–∏—Å—Ç–µ–º—ñ. –í–∏—Ö—ñ–¥ –∑–∞ –º–µ–∂—ñ –ª–∏—à–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –®–Ü.",
                "links": [
                  {
                    "title": "–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∑–∞ —Ç–∏–ø–æ–º –¥–∞–Ω–∏—Ö",
                    "href": "data-classification.html"
                  }
                ]
              },
              {
                "text": "–í—ñ–∑—É–∞–ª—å–Ω–æ-–º–æ–≤–Ω—ñ –º–æ–¥–µ–ª—ñ",
                "desc": "GPT-4V, Claude Vision (Sonnet/Opus), Gemini Pro Vision. –¶—ñ –º–æ–¥–µ–ª—ñ \"–±–∞—á–∞—Ç—å\" –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–∞ –º—ñ—Ä–∫—É—é—Ç—å –ø—Ä–æ –Ω–∏—Ö –ø—Ä–∏—Ä–æ–¥–Ω–æ—é –º–æ–≤–æ—é ‚Äî –æ–ø–∏—Å—É—é—á–∏ —Å—Ü–µ–Ω–∏, —á–∏—Ç–∞—é—á–∏ —Ç–µ–∫—Å—Ç, –∞–Ω–∞–ª—ñ–∑—É—é—á–∏ –≥—Ä–∞—Ñ—ñ–∫–∏.",
                "links": [
                  {
                    "title": "–í–µ–ª–∏–∫—ñ –≥—Ä–∞–≤—Ü—ñ",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "–†–æ–∑—É–º—ñ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å",
                "desc": "–û–ø–∏—Å —Å—Ü–µ–Ω, OCR (—á–∏—Ç–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ –∑–æ–±—Ä–∞–∂–µ–Ω—å), –∞–Ω–∞–ª—ñ–∑ –≥—Ä–∞—Ñ—ñ–∫—ñ–≤/–¥—ñ–∞–≥—Ä–∞–º, –≤—ñ–∑—É–∞–ª—å–Ω–µ QA. –ú–æ–¥–µ–ª—ñ –º–æ–∂—É—Ç—å –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Å–∫—Ä—ñ–Ω—à–æ—Ç–∏, —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ—ñ—ó, –¥–æ–∫—É–º–µ–Ω—Ç–∏ —Ç–∞ –¥—ñ–∞–≥—Ä–∞–º–∏.",
                "links": []
              },
              {
                "text": "–†–æ–∑—É–º—ñ–Ω–Ω—è —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∞—É–¥—ñ–æ",
                "desc": "Whisper (OpenAI) —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î –º–æ–≤—É –≤ —Ç–µ–∫—Å—Ç. TTS –º–æ–¥–µ–ª—ñ —Å–∏–Ω—Ç–µ–∑—É—é—Ç—å –ø—Ä–∏—Ä–æ–¥–Ω–∏–π –≥–æ–ª–æ—Å. –ö–ª–æ–Ω—É–≤–∞–Ω–Ω—è –≥–æ–ª–æ—Å—É –≤—ñ–¥—Ç–≤–æ—Ä—é—î –≥–æ–ª–æ—Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –ª—é–¥–∏–Ω–∏ —ñ–∑ –∑—Ä–∞–∑–∫—ñ–≤.",
                "links": []
              },
              {
                "text": "–†–æ–∑—É–º—ñ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤",
                "desc": "–ü–∞—Ä—Å–∏–Ω–≥ —Å–∫–ª–∞–¥–Ω–∏—Ö –º–∞–∫–µ—Ç—ñ–≤ ‚Äî PDF, —Ä–∞—Ö—É–Ω–∫—ñ–≤, —Ä—É–∫–æ–ø–∏—Å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É, –±–∞–≥–∞—Ç–æ–∫–æ–ª–æ–Ω–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤. –ü–æ—î–¥–Ω—É—î OCR –∑ –º–æ–≤–Ω–∏–º —Ä–æ–∑—É–º—ñ–Ω–Ω—è–º –¥–ª—è —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ—ó –µ–∫—Å—Ç—Ä–∞–∫—Ü—ñ—ó –¥–∞–Ω–∏—Ö.",
                "links": []
              },
              {
                "text": "–ö—Ä–æ—Å–º–æ–¥–∞–ª—å–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è",
                "desc": "–¢–µ–∫—Å—Ç-—É-–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (DALL-E, Midjourney), –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è-–≤-—Ç–µ–∫—Å—Ç (–ø—ñ–¥–ø–∏—Å–∏), —Ç–µ–∫—Å—Ç-–≤-–∞—É–¥—ñ–æ (TTS), –∞—É–¥—ñ–æ-–≤-—Ç–µ–∫—Å—Ç (—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è). –ë–µ–∑—à–æ–≤–Ω–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –º—ñ–∂ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–∏—Ö.",
                "links": [
                  {
                    "title": "–î–∏—Ñ—É–∑—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "diffusion-models.html"
                  }
                ]
              },
              {
                "text": "–†–æ–∑—É–º—ñ–Ω–Ω—è –≤—ñ–¥–µ–æ",
                "desc": "–ß–∞—Å–æ–≤–∏–π –∞–Ω–∞–ª—ñ–∑, —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –¥—ñ–π, –≤—ñ–¥–µ–æ QA ‚Äî —Ä–æ–∑—É–º—ñ–Ω–Ω—è —â–æ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è —á–µ—Ä–µ–∑ –∫–∞–¥—Ä–∏ –ø—Ä–æ—Ç—è–≥–æ–º —á–∞—Å—É. –°–∫–ª–∞–¥–Ω—ñ—à–µ –∑–∞ –∞–Ω–∞–ª—ñ–∑ –æ–¥–Ω–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è.",
                "links": []
              },
              {
                "text": "–ù–∞—Ç–∏–≤–Ω–∞ vs –ê–¥–∞–ø—Ç–µ—Ä–Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å",
                "desc": "–î–µ—è–∫—ñ –º–æ–¥–µ–ª—ñ (Gemini) –Ω–∞—Ç–∏–≤–Ω–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è. –Ü–Ω—à—ñ –ø—Ä–∏–∫—Ä—É—á—É—é—Ç—å –∞–¥–∞–ø—Ç–µ—Ä–∏ –∑–æ—Ä—É –¥–æ —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –º–æ–¥–µ–ª–µ–π. –ù–∞—Ç–∏–≤–Ω—ñ –∑–∞–∑–≤–∏—á–∞–π –±—ñ–ª—å—à –∑–¥–∞—Ç–Ω—ñ —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "foundation-models.html"
                  }
                ]
              },
              {
                "text": "–ü—ñ–¥—Ö—ñ–¥ Gemini",
                "desc": "Gemini –≤—ñ–¥ Google –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω–∏–π –Ω–∞—Ç–∏–≤–Ω–æ –Ω–∞ —Ç–µ–∫—Å—Ç—ñ + –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è—Ö + –∞—É–¥—ñ–æ + –≤—ñ–¥–µ–æ –æ–¥–Ω–æ—á–∞—Å–Ω–æ. –¶–µ –¥–∞—î –≥–ª–∏–±—à–µ –∫—Ä–æ—Å–º–æ–¥–∞–ª—å–Ω–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ –∞–¥–∞–ø—Ç–µ—Ä–Ω–∏–º–∏ –ø—ñ–¥—Ö–æ–¥–∞–º–∏.",
                "links": [
                  {
                    "title": "–í–µ–ª–∏–∫—ñ –≥—Ä–∞–≤—Ü—ñ",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "–†–µ–∞–ª—å–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è",
                "desc": "–Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ (–æ–ø–∏—Å –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–ª—è –Ω–µ–∑—Ä—è—á–∏—Ö), –º–æ–¥–µ—Ä–∞—Ü—ñ—è –∫–æ–Ω—Ç–µ–Ω—Ç—É (–≤–∏—è–≤–ª–µ–Ω–Ω—è —à–∫—ñ–¥–ª–∏–≤–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å), –º–µ–¥–∏—á–Ω–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è, –∞–≤—Ç–æ–Ω–æ–º–Ω–µ –≤–æ–¥—ñ–Ω–Ω—è, –ø–∞–π–ø–ª–∞–π–Ω–∏ –æ–±—Ä–æ–±–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Modality",
                "def": "A type of data input/output: text, image, audio, video, or code."
              },
              {
                "term": "Vision-Language Model",
                "def": "Model that can understand and reason about images alongside text."
              },
              {
                "term": "OCR",
                "def": "Optical Character Recognition ‚Äî extracting text from images of documents or screens."
              },
              {
                "term": "Cross-Modal",
                "def": "Converting between data types, e.g., generating an image from text description."
              }
            ],
            "uk": [
              {
                "term": "–ú–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å",
                "def": "–¢–∏–ø –≤—Ö—ñ–¥–Ω–∏—Ö/–≤–∏—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö: —Ç–µ–∫—Å—Ç, –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –∞—É–¥—ñ–æ, –≤—ñ–¥–µ–æ –∞–±–æ –∫–æ–¥."
              },
              {
                "term": "–í—ñ–∑—É–∞–ª—å–Ω–æ-–º–æ–≤–Ω–∞ –º–æ–¥–µ–ª—å",
                "def": "–ú–æ–¥–µ–ª—å, —â–æ –º–æ–∂–µ —Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–∞ –º—ñ—Ä–∫—É–≤–∞—Ç–∏ –ø—Ä–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä–∞–∑–æ–º –∑ —Ç–µ–∫—Å—Ç–æ–º."
              },
              {
                "term": "OCR",
                "def": "–û–ø—Ç–∏—á–Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è —Å–∏–º–≤–æ–ª—ñ–≤ ‚Äî –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É —ñ–∑ –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∞–±–æ –µ–∫—Ä–∞–Ω—ñ–≤."
              },
              {
                "term": "–ö—Ä–æ—Å–º–æ–¥–∞–ª—å–Ω–∏–π",
                "def": "–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –º—ñ–∂ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–∏—Ö, –Ω–∞–ø—Ä., –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å—É."
              }
            ]
          },
          "tips": {
            "en": [
              "When working with vision models, providing high-resolution images with clear context significantly improves accuracy ‚Äî blurry or cropped images cause hallucinations",
              "Multimodal models are great for automating document processing ‚Äî invoices, receipts, forms ‚Äî by combining OCR with language understanding in a single step",
              "Test multimodal inputs carefully: models often perform differently on photos vs screenshots vs diagrams, even when the content is similar"
            ],
            "uk": [
              "–ü—Ä–∏ —Ä–æ–±–æ—Ç—ñ –∑ –≤—ñ–∑—É–∞–ª—å–Ω–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–∞–¥–∞–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å –≤–∏—Å–æ–∫–æ—ó —Ä–æ–∑–¥—ñ–ª—å–Ω–æ—Å—Ç—ñ –∑ —á—ñ—Ç–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∑–Ω–∞—á–Ω–æ –ø–æ–∫—Ä–∞—â—É—î —Ç–æ—á–Ω—ñ—Å—Ç—å ‚Äî —Ä–æ–∑–º–∏—Ç—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Å–ø—Ä–∏—á–∏–Ω—è—é—Ç—å –≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó",
              "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ —á—É–¥–æ–≤—ñ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –æ–±—Ä–æ–±–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ ‚Äî —Ä–∞—Ö—É–Ω–∫–∏, —á–µ–∫–∏, —Ñ–æ—Ä–º–∏ ‚Äî –ø–æ—î–¥–Ω—É—é—á–∏ OCR –∑ –º–æ–≤–Ω–∏–º —Ä–æ–∑—É–º—ñ–Ω–Ω—è–º –≤ –æ–¥–Ω–æ–º—É –∫—Ä–æ—Ü—ñ",
              "–†–µ—Ç–µ–ª—å–Ω–æ —Ç–µ—Å—Ç—É–π—Ç–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ –≤—Ö–æ–¥–∏: –º–æ–¥–µ–ª—ñ —á–∞—Å—Ç–æ –ø—Ä–∞—Ü—é—é—Ç—å –ø–æ-—Ä—ñ–∑–Ω–æ–º—É –Ω–∞ —Ñ–æ—Ç–æ vs —Å–∫—Ä—ñ–Ω—à–æ—Ç–∞—Ö vs –¥—ñ–∞–≥—Ä–∞–º–∞—Ö, –Ω–∞–≤—ñ—Ç—å –∫–æ–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ö–æ–∂–∏–π"
            ]
          },
          "related": [
            "Models",
            "Feed"
          ]
        },
        {
          "slug": "reasoning",
          "title": {
            "en": "Reasoning",
            "uk": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è"
          },
          "desc": {
            "en": "AI reasoning capabilities - chain of thought, thinking models, and logical inference.",
            "uk": "–ó–¥–∞—Ç–Ω–æ—Å—Ç—ñ –®–Ü –¥–æ –º—ñ—Ä–∫—É–≤–∞–Ω—å ‚Äî –ª–∞–Ω—Ü—é–≥ –¥—É–º–æ–∫, –º–æ–¥–µ–ª—ñ –∑ –º–∏—Å–ª–µ–Ω–Ω—è–º, –ª–æ–≥—ñ—á–Ω–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫."
          },
          "overview": {
            "en": [
              "Reasoning is one of the most important and rapidly evolving capabilities of modern AI. While early LLMs could generate fluent text, they often struggled with multi-step logic, math, and complex problem-solving. The introduction of chain-of-thought prompting and dedicated reasoning models has dramatically improved these capabilities.",
              "Models like OpenAI o1/o3, DeepSeek-R1, and QwQ use \"thinking tokens\" ‚Äî they reason step-by-step internally before producing a final answer. This mirrors the human distinction between fast intuitive thinking (System 1) and slow deliberate reasoning (System 2). Understanding these capabilities and their limits is crucial for knowing when to trust AI outputs."
            ],
            "uk": [
              "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è ‚Äî –æ–¥–Ω–∞ –∑ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö —Ç–∞ –Ω–∞–π—à–≤–∏–¥—à–µ –µ–≤–æ–ª—é—Ü—ñ–æ–Ω—É—é—á–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π —Å—É—á–∞—Å–Ω–æ–≥–æ –®–Ü. –•–æ—á–∞ —Ä–∞–Ω–Ω—ñ LLM –º–æ–≥–ª–∏ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –ø–ª–∞–≤–Ω–∏–π —Ç–µ–∫—Å—Ç, –≤–æ–Ω–∏ —á–∞—Å—Ç–æ –º–∞–ª–∏ –ø—Ä–æ–±–ª–µ–º–∏ –∑ –±–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤–æ—é –ª–æ–≥—ñ–∫–æ—é, –º–∞—Ç–µ–º–∞—Ç–∏–∫–æ—é —Ç–∞ —Å–∫–ª–∞–¥–Ω–∏–º –≤–∏—Ä—ñ—à–µ–Ω–Ω—è–º –∑–∞–¥–∞—á. –í–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É –ª–∞–Ω—Ü—é–≥–æ–º –¥—É–º–æ–∫ —Ç–∞ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –º—ñ—Ä–∫—É–≤–∞–Ω—å –¥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø–æ–∫—Ä–∞—â–∏–ª–æ —Ü—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ.",
              "–ú–æ–¥–µ–ª—ñ —è–∫ OpenAI o1/o3, DeepSeek-R1 —Ç–∞ QwQ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å \"—Ç–æ–∫–µ–Ω–∏ –º–∏—Å–ª–µ–Ω–Ω—è\" ‚Äî –≤–æ–Ω–∏ –º—ñ—Ä–∫—É—é—Ç—å –∫—Ä–æ–∫ –∑–∞ –∫—Ä–æ–∫–æ–º –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –ø–µ—Ä–µ–¥ –≤–∏–¥–∞—á–µ—é —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ. –¶–µ –≤—ñ–¥–∑–µ—Ä–∫–∞–ª—é—î –ª—é–¥—Å—å–∫–µ —Ä–æ–∑—Ä—ñ–∑–Ω–µ–Ω–Ω—è –º—ñ–∂ —à–≤–∏–¥–∫–∏–º —ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω–∏–º –º–∏—Å–ª–µ–Ω–Ω—è–º (–°–∏—Å—Ç–µ–º–∞ 1) —Ç–∞ –ø–æ–≤—ñ–ª—å–Ω–∏–º –æ–±–¥—É–º–∞–Ω–∏–º –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è–º (–°–∏—Å—Ç–µ–º–∞ 2). –†–æ–∑—É–º—ñ–Ω–Ω—è —Ü–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π —Ç–∞ —ó—Ö –æ–±–º–µ–∂–µ–Ω—å –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–µ –¥–ª—è –∑–Ω–∞–Ω–Ω—è, –∫–æ–ª–∏ –¥–æ–≤—ñ—Ä—è—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –®–Ü."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Chain-of-Thought (CoT) Prompting",
                "desc": "Asking models to \"think step by step\" dramatically improves accuracy on complex problems. Instead of jumping to answers, the model shows its work ‚Äî breaking problems into manageable steps.",
                "links": [
                  {
                    "title": "Prompting Techniques",
                    "href": "../level-4/prompting-techniques.html"
                  },
                  {
                    "title": "Prompt",
                    "href": "../level-2/prompt.html"
                  }
                ]
              },
              {
                "text": "Reasoning Models",
                "desc": "OpenAI o1/o3, DeepSeek-R1, QwQ (Alibaba) are specifically trained for multi-step reasoning. They use extra inference-time compute to \"think longer\" before answering.",
                "links": [
                  {
                    "title": "The Big Players",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "Thinking Tokens",
                "desc": "Internal reasoning traces generated before the final answer. These tokens are the model's \"scratch pad\" ‚Äî working through logic, checking steps, considering alternatives.",
                "links": [
                  {
                    "title": "Token",
                    "href": "../level-2/token.html"
                  }
                ]
              },
              {
                "text": "Extended Thinking",
                "desc": "Allocating more compute at inference time for harder problems. The model can \"think longer\" on complex questions, trading speed for accuracy. Test-time compute scaling.",
                "links": []
              },
              {
                "text": "Math Reasoning",
                "desc": "Solving competition-level math (AIME, AMC), formal proofs, symbolic manipulation. Reasoning models have made dramatic progress here ‚Äî approaching human expert level.",
                "links": [
                  {
                    "title": "SOTA",
                    "href": "sota.html"
                  }
                ]
              },
              {
                "text": "Code Reasoning",
                "desc": "Debugging complex codebases, analyzing architecture, implementing sophisticated algorithms. Code reasoning is one of the most practically valuable AI capabilities.",
                "links": [
                  {
                    "title": "Vibecoding",
                    "href": "../level-2/vibecoding.html"
                  }
                ]
              },
              {
                "text": "Logical Inference",
                "desc": "Syllogisms, deduction, constraint satisfaction, planning. Models can follow logical rules but still struggle with certain types of novel reasoning and common-sense physics.",
                "links": []
              },
              {
                "text": "System 1 vs System 2 Thinking",
                "desc": "Kahneman's framework applied to AI: System 1 = fast intuitive responses (standard LLM), System 2 = slow deliberate reasoning (reasoning models with thinking tokens).",
                "links": []
              },
              {
                "text": "Current Limitations",
                "desc": "Reasoning models still fail on truly novel problems, can produce convincing but wrong chains of reasoning, and may overthink simple questions. Verification remains essential.",
                "links": [
                  {
                    "title": "Hallucinations",
                    "href": "../level-2/hallucination.html"
                  }
                ]
              },
              {
                "text": "Reasoning Benchmarks",
                "desc": "MATH (competition math), GSM8K (grade school), ARC-AGI (general reasoning), SWE-bench (real software engineering), Codeforces (competitive programming).",
                "links": [
                  {
                    "title": "SOTA",
                    "href": "sota.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–ü—Ä–æ–º–ø—Ç–∏–Ω–≥ –ª–∞–Ω—Ü—é–≥–æ–º –¥—É–º–æ–∫ (CoT)",
                "desc": "–ü—Ä–æ—Ö–∞–Ω–Ω—è –º–æ–¥–µ–ª—è–º \"–¥—É–º–∞—Ç–∏ –∫—Ä–æ–∫ –∑–∞ –∫—Ä–æ–∫–æ–º\" –¥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø–æ–∫—Ä–∞—â—É—î —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ó–∞–º—ñ—Å—Ç—å —Å—Ç—Ä–∏–±–∫–∞ –¥–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ, –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—É—î —Å–≤–æ—é —Ä–æ–±–æ—Ç—É ‚Äî —Ä–æ–∑–±–∏–≤–∞—é—á–∏ –∑–∞–¥–∞—á—ñ –Ω–∞ –∫–µ—Ä–æ–≤–∞–Ω—ñ –∫—Ä–æ–∫–∏.",
                "links": [
                  {
                    "title": "–¢–µ—Ö–Ω—ñ–∫–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É",
                    "href": "../level-4/prompting-techniques.html"
                  },
                  {
                    "title": "–ü—Ä–æ–º–ø—Ç",
                    "href": "../level-2/prompt.html"
                  }
                ]
              },
              {
                "text": "–ú–æ–¥–µ–ª—ñ –º—ñ—Ä–∫—É–≤–∞–Ω—å",
                "desc": "OpenAI o1/o3, DeepSeek-R1, QwQ (Alibaba) —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ –Ω–∞–≤—á–µ–Ω—ñ –¥–ª—è –±–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤–∏—Ö –º—ñ—Ä–∫—É–≤–∞–Ω—å. –í–æ–Ω–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –ø—ñ–¥ —á–∞—Å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É —â–æ–± \"–¥—É–º–∞—Ç–∏ –¥–æ–≤—à–µ\" –ø–µ—Ä–µ–¥ –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é.",
                "links": [
                  {
                    "title": "–í–µ–ª–∏–∫—ñ –≥—Ä–∞–≤—Ü—ñ",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "–¢–æ–∫–µ–Ω–∏ –º–∏—Å–ª–µ–Ω–Ω—è",
                "desc": "–í–Ω—É—Ç—Ä—ñ—à–Ω—ñ —Å–ª—ñ–¥–∏ –º—ñ—Ä–∫—É–≤–∞–Ω—å, –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ –ø–µ—Ä–µ–¥ —Ñ—ñ–Ω–∞–ª—å–Ω–æ—é –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é. –¶—ñ —Ç–æ–∫–µ–Ω–∏ ‚Äî \"—á–µ—Ä–Ω–µ—Ç–∫–∞\" –º–æ–¥–µ–ª—ñ ‚Äî –ø—Ä–æ–ø—Ä–∞—Ü—å–æ–≤—É–≤–∞–Ω–Ω—è –ª–æ–≥—ñ–∫–∏, –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫—Ä–æ–∫—ñ–≤, —Ä–æ–∑–≥–ª—è–¥ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤.",
                "links": [
                  {
                    "title": "–¢–æ–∫–µ–Ω",
                    "href": "../level-2/token.html"
                  }
                ]
              },
              {
                "text": "–†–æ–∑—à–∏—Ä–µ–Ω–µ –º–∏—Å–ª–µ–Ω–Ω—è",
                "desc": "–í–∏–¥—ñ–ª–µ–Ω–Ω—è –±—ñ–ª—å—à–µ –æ–±—á–∏—Å–ª–µ–Ω—å –ø—ñ–¥ —á–∞—Å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –¥–ª—è —Å–∫–ª–∞–¥–Ω—ñ—à–∏—Ö –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å –º–æ–∂–µ \"–¥—É–º–∞—Ç–∏ –¥–æ–≤—à–µ\" –Ω–∞–¥ —Å–∫–ª–∞–¥–Ω–∏–º–∏ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è–º–∏, –æ–±–º—ñ–Ω—é—é—á–∏ —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å.",
                "links": []
              },
              {
                "text": "–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–µ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                "desc": "–†–æ–∑–≤'—è–∑–∞–Ω–Ω—è –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏—Ö –∑–∞–¥–∞—á –æ–ª—ñ–º–ø—ñ–∞–¥–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è (AIME, AMC), —Ñ–æ—Ä–º–∞–ª—å–Ω—ñ –¥–æ–≤–µ–¥–µ–Ω–Ω—è, —Å–∏–º–≤–æ–ª—ñ—á–Ω—ñ –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—ó. –ú–æ–¥–µ–ª—ñ –º—ñ—Ä–∫—É–≤–∞–Ω—å –∑—Ä–æ–±–∏–ª–∏ –¥—Ä–∞–º–∞—Ç–∏—á–Ω–∏–π –ø—Ä–æ–≥—Ä–µ—Å ‚Äî –Ω–∞–±–ª–∏–∂–∞—é—á–∏—Å—å –¥–æ —Ä—ñ–≤–Ω—è –ª—é–¥—Å—å–∫–∏—Ö –µ–∫—Å–ø–µ—Ä—Ç—ñ–≤.",
                "links": [
                  {
                    "title": "SOTA",
                    "href": "sota.html"
                  }
                ]
              },
              {
                "text": "–ö–æ–¥–æ–≤–µ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                "desc": "–î–µ–±–∞–≥—ñ–Ω–≥ —Å–∫–ª–∞–¥–Ω–∏—Ö –∫–æ–¥–æ–≤–∏—Ö –±–∞–∑, –∞–Ω–∞–ª—ñ–∑ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏, —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∫–ª–∞–¥–Ω–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤. –ö–æ–¥–æ–≤–µ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è ‚Äî –æ–¥–Ω–∞ –∑ –Ω–∞–π–ø—Ä–∞–∫—Ç–∏—á–Ω—ñ—à–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –®–Ü.",
                "links": [
                  {
                    "title": "–í–∞–π–±–∫–æ–¥–∏–Ω–≥",
                    "href": "../level-2/vibecoding.html"
                  }
                ]
              },
              {
                "text": "–õ–æ–≥—ñ—á–Ω–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫",
                "desc": "–°–∏–ª–æ–≥—ñ–∑–º–∏, –¥–µ–¥—É–∫—Ü—ñ—è, –∑–∞–¥–æ–≤–æ–ª–µ–Ω–Ω—è –æ–±–º–µ–∂–µ–Ω—å, –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è. –ú–æ–¥–µ–ª—ñ –º–æ–∂—É—Ç—å —Å–ª—ñ–¥—É–≤–∞—Ç–∏ –ª–æ–≥—ñ—á–Ω–∏–º –ø—Ä–∞–≤–∏–ª–∞–º, –∞–ª–µ –≤—Å–µ —â–µ –º–∞—é—Ç—å –ø—Ä–æ–±–ª–µ–º–∏ –∑ –Ω–æ–≤–∏–º–∏ —Ç–∏–ø–∞–º–∏ –º—ñ—Ä–∫—É–≤–∞–Ω—å.",
                "links": []
              },
              {
                "text": "–ú–∏—Å–ª–µ–Ω–Ω—è –°–∏—Å—Ç–µ–º–∏ 1 vs –°–∏—Å—Ç–µ–º–∏ 2",
                "desc": "–§—Ä–µ–π–º–≤–æ—Ä–∫ –ö–∞–Ω–µ–º–∞–Ω–∞ –¥–ª—è –®–Ü: –°–∏—Å—Ç–µ–º–∞ 1 = —à–≤–∏–¥–∫—ñ —ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π LLM), –°–∏—Å—Ç–µ–º–∞ 2 = –ø–æ–≤—ñ–ª—å–Ω–µ –æ–±–¥—É–º–∞–Ω–µ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è (–º–æ–¥–µ–ª—ñ –∑ —Ç–æ–∫–µ–Ω–∞–º–∏ –º–∏—Å–ª–µ–Ω–Ω—è).",
                "links": []
              },
              {
                "text": "–ü–æ—Ç–æ—á–Ω—ñ –æ–±–º–µ–∂–µ–Ω–Ω—è",
                "desc": "–ú–æ–¥–µ–ª—ñ –º—ñ—Ä–∫—É–≤–∞–Ω—å –≤—Å–µ —â–µ –∑–±–æ—è—Ç—å –Ω–∞ —Å–ø—Ä–∞–≤–¥—ñ –Ω–æ–≤–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –º–æ–∂—É—Ç—å –≤–∏–¥–∞–≤–∞—Ç–∏ –ø–µ—Ä–µ–∫–æ–Ω–ª–∏–≤—ñ –∞–ª–µ —Ö–∏–±–Ω—ñ –ª–∞–Ω—Ü—é–≥–∏ –º—ñ—Ä–∫—É–≤–∞–Ω—å, —Ç–∞ –º–æ–∂—É—Ç—å –ø–µ—Ä–µ–¥—É–º—É–≤–∞—Ç–∏ –ø—Ä–æ—Å—Ç—ñ –ø–∏—Ç–∞–Ω–Ω—è. –í–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—é.",
                "links": [
                  {
                    "title": "–ì–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó",
                    "href": "../level-2/hallucination.html"
                  }
                ]
              },
              {
                "text": "–ë–µ–Ω—á–º–∞—Ä–∫–∏ –º—ñ—Ä–∫—É–≤–∞–Ω—å",
                "desc": "MATH (–æ–ª—ñ–º–ø—ñ–∞–¥–Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞), GSM8K (—à–∫—ñ–ª—å–Ω–∞), ARC-AGI (–∑–∞–≥–∞–ª—å–Ω–µ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è), SWE-bench (—Ä–µ–∞–ª—å–Ω–∞ —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—è –ü–ó), Codeforces (–∑–º–∞–≥–∞–ª—å–Ω–µ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è).",
                "links": [
                  {
                    "title": "SOTA",
                    "href": "sota.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Chain-of-Thought",
                "def": "Technique where models explain their reasoning step-by-step before giving an answer."
              },
              {
                "term": "Thinking Tokens",
                "def": "Internal reasoning traces generated by reasoning models before the final output."
              },
              {
                "term": "System 1/System 2",
                "def": "Kahneman's framework: fast intuitive vs slow deliberate thinking, applied to AI."
              },
              {
                "term": "Test-Time Compute",
                "def": "Allocating more processing at inference time to improve reasoning on harder problems."
              }
            ],
            "uk": [
              {
                "term": "–õ–∞–Ω—Ü—é–≥ –¥—É–º–æ–∫",
                "def": "–¢–µ—Ö–Ω—ñ–∫–∞, –¥–µ –º–æ–¥–µ–ª—ñ –ø–æ—è—Å–Ω—é—é—Ç—å —Å–≤–æ—î –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è –∫—Ä–æ–∫ –∑–∞ –∫—Ä–æ–∫–æ–º –ø–µ—Ä–µ–¥ –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é."
              },
              {
                "term": "–¢–æ–∫–µ–Ω–∏ –º–∏—Å–ª–µ–Ω–Ω—è",
                "def": "–í–Ω—É—Ç—Ä—ñ—à–Ω—ñ —Å–ª—ñ–¥–∏ –º—ñ—Ä–∫—É–≤–∞–Ω—å, –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—è–º–∏ –ø–µ—Ä–µ–¥ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º."
              },
              {
                "term": "–°–∏—Å—Ç–µ–º–∞ 1/–°–∏—Å—Ç–µ–º–∞ 2",
                "def": "–§—Ä–µ–π–º–≤–æ—Ä–∫ –ö–∞–Ω–µ–º–∞–Ω–∞: —à–≤–∏–¥–∫–µ —ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω–µ vs –ø–æ–≤—ñ–ª—å–Ω–µ –æ–±–¥—É–º–∞–Ω–µ –º–∏—Å–ª–µ–Ω–Ω—è, –∑–∞—Å—Ç–æ—Å–æ–≤–∞–Ω–µ –¥–æ –®–Ü."
              },
              {
                "term": "–û–±—á–∏—Å–ª–µ–Ω–Ω—è –ø—ñ–¥ —á–∞—Å —Ç–µ—Å—Ç—É",
                "def": "–í–∏–¥—ñ–ª–µ–Ω–Ω—è –±—ñ–ª—å—à–µ –æ–±—Ä–æ–±–∫–∏ –ø—ñ–¥ —á–∞—Å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –º—ñ—Ä–∫—É–≤–∞–Ω—å –Ω–∞ —Å–∫–ª–∞–¥–Ω—ñ—à–∏—Ö –∑–∞–¥–∞—á–∞—Ö."
              }
            ]
          },
          "tips": {
            "en": [
              "For complex problems, explicitly ask the model to \"think step by step\" ‚Äî this simple instruction activates chain-of-thought reasoning and can double accuracy on math and logic tasks",
              "Reasoning models (o1, Claude with extended thinking) cost more and are slower ‚Äî use them for hard problems, and faster models for simple tasks",
              "If a model gives a wrong answer, try breaking the problem into smaller sub-questions rather than just asking again ‚Äî decomposition often fixes reasoning failures"
            ],
            "uk": [
              "–î–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–¥–∞—á —è–≤–Ω–æ –ø–æ–ø—Ä–æ—Å—ñ—Ç—å –º–æ–¥–µ–ª—å \"–¥—É–º–∞—Ç–∏ –∫—Ä–æ–∫ –∑–∞ –∫—Ä–æ–∫–æ–º\" ‚Äî —Ü—è –ø—Ä–æ—Å—Ç–∞ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è –∞–∫—Ç–∏–≤—É—î –ª–∞–Ω—Ü—é–≥ –¥—É–º–æ–∫ —ñ –º–æ–∂–µ –ø–æ–¥–≤–æ—ó—Ç–∏ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏—Ö —Ç–∞ –ª–æ–≥—ñ—á–Ω–∏—Ö –∑–∞–¥–∞—á–∞—Ö",
              "–ú–æ–¥–µ–ª—ñ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è (o1, Claude –∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–º –º–∏—Å–ª–µ–Ω–Ω—è–º) –¥–æ—Ä–æ–∂—á—ñ —Ç–∞ –ø–æ–≤—ñ–ª—å–Ω—ñ—à—ñ ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ —ó—Ö –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–¥–∞—á, –∞ —à–≤–∏–¥—à—ñ –º–æ–¥–µ–ª—ñ –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö",
              "–Ø–∫—â–æ –º–æ–¥–µ–ª—å –¥–∞–ª–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å, —Å–ø—Ä–æ–±—É–π—Ç–µ —Ä–æ–∑–±–∏—Ç–∏ –∑–∞–¥–∞—á—É –Ω–∞ –º–µ–Ω—à—ñ –ø—ñ–¥–∑–∞–¥–∞—á—ñ –∑–∞–º—ñ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è ‚Äî –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü—ñ—è —á–∞—Å—Ç–æ –≤–∏–ø—Ä–∞–≤–ª—è—î –∑–±–æ—ó –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è"
            ]
          },
          "related": [
            "Models",
            "Feed"
          ]
        },
        {
          "slug": "foundation-models",
          "title": {
            "en": "Foundation Models",
            "uk": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ"
          },
          "desc": {
            "en": "The concept of foundation models - large pre-trained models adapted for many tasks.",
            "uk": "–ö–æ–Ω—Ü–µ–ø—Ü—ñ—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π ‚Äî –≤–µ–ª–∏–∫–∏—Ö –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∑–∞–¥–∞—á."
          },
          "overview": {
            "en": [
              "Foundation models are large AI models pre-trained on broad, diverse datasets that can be adapted to a wide range of downstream tasks. The term was coined by Stanford's HAI center to describe a paradigm shift: instead of training a new model for each task, you train one massive model and then adapt it through fine-tuning, prompting, or few-shot learning.",
              "GPT-4, Claude, Gemini, and Llama are all foundation models. Their power comes from scale ‚Äî both in parameters and training data ‚Äî which gives them general capabilities that can be directed toward specific applications without starting from scratch each time."
            ],
            "uk": [
              "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ ‚Äî —Ü–µ –≤–µ–ª–∏–∫—ñ –º–æ–¥–µ–ª—ñ –®–Ü, –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω—ñ –Ω–∞ —à–∏—Ä–æ–∫–∏—Ö, —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–∏—Ö, —â–æ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –∞–¥–∞–ø—Ç–æ–≤–∞–Ω—ñ –¥–ª—è —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä—É –∑–∞–¥–∞—á. –¢–µ—Ä–º—ñ–Ω –±—É–≤ –∑–∞–ø—Ä–æ–ø–æ–Ω–æ–≤–∞–Ω–∏–π —Ü–µ–Ω—Ç—Ä–æ–º HAI –°—Ç–µ–Ω—Ñ–æ—Ä–¥—É –¥–ª—è –æ–ø–∏—Å—É –ø–∞—Ä–∞–¥–∏–≥–º–∞–ª—å–Ω–æ–≥–æ –∑—Å—É–≤—É: –∑–∞–º—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ –¥–ª—è –∫–æ–∂–Ω–æ—ó –∑–∞–¥–∞—á—ñ, –≤–∏ –Ω–∞–≤—á–∞—î—Ç–µ –æ–¥–Ω—É –º–∞—Å–∏–≤–Ω—É –º–æ–¥–µ–ª—å —ñ –ø–æ—Ç—ñ–º –∞–¥–∞–ø—Ç—É—î—Ç–µ —ó—ó —á–µ—Ä–µ–∑ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥, –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ –∞–±–æ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –∫—ñ–ª—å–∫–æ—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.",
              "GPT-4, Claude, Gemini —Ç–∞ Llama ‚Äî —É—Å—ñ —î —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –á—Ö —Å–∏–ª–∞ –ø–æ—Ö–æ–¥–∏—Ç—å –≤—ñ–¥ –º–∞—Å—à—Ç–∞–±—É ‚Äî —è–∫ —É –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö, —Ç–∞–∫ —ñ –≤ –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö ‚Äî —â–æ –¥–∞—î —ó–º –∑–∞–≥–∞–ª—å–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ, —è–∫—ñ –º–æ–∂—É—Ç—å –±—É—Ç–∏ —Å–ø—Ä—è–º–æ–≤–∞–Ω—ñ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –±–µ–∑ –ø–æ—á–∞—Ç–∫—É –∑ –Ω—É–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ä–∞–∑—É."
            ]
          },
          "details": {
            "en": [
              {
                "text": "What Makes a Model \"Foundational\"",
                "desc": "Broad pre-training on diverse data enables adaptation to virtually any downstream task ‚Äî translation, coding, analysis, creative writing ‚Äî without training a new model each time.",
                "links": [
                  {
                    "title": "Generative AI",
                    "href": "generative-ai.html"
                  }
                ]
              },
              {
                "text": "Pre-training at Scale",
                "desc": "Foundation models are trained on trillions of tokens from books, web pages, code repositories, and scientific papers. This massive exposure creates general-purpose knowledge and language understanding.",
                "links": [
                  {
                    "title": "Data to Model",
                    "href": "../level-3/data-to-model.html"
                  }
                ]
              },
              {
                "text": "Transfer Learning",
                "desc": "Knowledge gained during pre-training transfers to specific tasks. A model trained on general text can be adapted for medical diagnosis, legal analysis, or code generation without starting from scratch.",
                "links": []
              },
              {
                "text": "Generalist vs Specialist Trade-offs",
                "desc": "Foundation models are generalists ‚Äî good at many things but not the best at any one. Task-specific fine-tuning creates specialists that excel in narrow domains at the cost of versatility.",
                "links": [
                  {
                    "title": "Training & Fine-tuning",
                    "href": "../level-3/training-finetuning.html"
                  }
                ]
              },
              {
                "text": "The Model Ecosystem",
                "desc": "Base models ‚Üí fine-tunes ‚Üí distilled versions ‚Üí API services. Each step in the chain trades generality for specificity, or size for speed, creating a rich ecosystem of model variants.",
                "links": [
                  {
                    "title": "Model Optimization",
                    "href": "../level-3/model-optimization.html"
                  }
                ]
              },
              {
                "text": "Open Foundation Models",
                "desc": "Llama 3, Qwen 2.5, Mistral, OLMo ‚Äî anyone can download, run locally, fine-tune, and inspect these models. Open weights enable research, customization, and privacy-sensitive deployments.",
                "links": [
                  {
                    "title": "The Big Players",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "Closed Foundation Models",
                "desc": "GPT-4, Claude, Gemini ‚Äî accessed only via API. These typically offer the highest performance but with less transparency and control. You pay per token and trust the provider.",
                "links": [
                  {
                    "title": "API Providers",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              },
              {
                "text": "Fine-tuning for Domains",
                "desc": "Adapting a foundation model for specific domains (medical, legal, coding) using curated datasets. This preserves general capabilities while dramatically improving domain-specific performance.",
                "links": [
                  {
                    "title": "Training & Fine-tuning",
                    "href": "../level-3/training-finetuning.html"
                  }
                ]
              },
              {
                "text": "Few-Shot Learning",
                "desc": "Providing examples in the prompt to guide the model without any retraining. Foundation models can learn new tasks on-the-fly from just 2-5 examples ‚Äî a capability that emerges at scale.",
                "links": [
                  {
                    "title": "Prompt",
                    "href": "../level-2/prompt.html"
                  }
                ]
              },
              {
                "text": "Cost of Training",
                "desc": "Frontier foundation models cost $100M+ to pre-train from scratch. But fine-tuning costs $100-$10K, and prompting is nearly free ‚Äî the ecosystem makes foundation model capabilities accessible at every budget level.",
                "links": [
                  {
                    "title": "SOTA",
                    "href": "sota.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–©–æ —Ä–æ–±–∏—Ç—å –º–æ–¥–µ–ª—å \"—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ—é\"",
                "desc": "–®–∏—Ä–æ–∫–µ –ø–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–æ–∑–≤–æ–ª—è—î –∞–¥–∞–ø—Ç–∞—Ü—ñ—é –¥–æ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ –±—É–¥—å-—è–∫–æ—ó –∑–∞–¥–∞—á—ñ ‚Äî –ø–µ—Ä–µ–∫–ª–∞–¥—É, –∫–æ–¥—É–≤–∞–Ω–Ω—è, –∞–Ω–∞–ª—ñ–∑—É, —Ç–≤–æ—Ä—á–æ–≥–æ –Ω–∞–ø–∏—Å–∞–Ω–Ω—è ‚Äî –±–µ–∑ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ –∫–æ–∂–Ω–æ–≥–æ —Ä–∞–∑—É.",
                "links": [
                  {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü",
                    "href": "generative-ai.html"
                  }
                ]
              },
              {
                "text": "–ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è —É –º–∞—Å—à—Ç–∞–±—ñ",
                "desc": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ –Ω–∞–≤—á–∞—é—Ç—å—Å—è –Ω–∞ —Ç—Ä–∏–ª—å–π–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω—ñ–≤ –∑ –∫–Ω–∏–≥, –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–æ–∫, —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—ó–≤ –∫–æ–¥—É —Ç–∞ –Ω–∞—É–∫–æ–≤–∏—Ö —Å—Ç–∞—Ç–µ–π. –¶–µ –º–∞—Å–∏–≤–Ω–µ –∑–∞–Ω—É—Ä–µ–Ω–Ω—è —Å—Ç–≤–æ—Ä—é—î –∑–∞–≥–∞–ª—å–Ω—ñ –∑–Ω–∞–Ω–Ω—è —Ç–∞ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –º–æ–≤–∏.",
                "links": [
                  {
                    "title": "–í—ñ–¥ –¥–∞–Ω–∏—Ö –¥–æ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-3/data-to-model.html"
                  }
                ]
              },
              {
                "text": "–¢—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è",
                "desc": "–ó–Ω–∞–Ω–Ω—è, –æ—Ç—Ä–∏–º–∞–Ω—ñ –ø—ñ–¥ —á–∞—Å –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è, –ø–µ—Ä–µ–Ω–æ—Å—è—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –∑–∞–¥–∞—á—ñ. –ú–æ–¥–µ–ª—å, –Ω–∞–≤—á–µ–Ω–∞ –Ω–∞ –∑–∞–≥–∞–ª—å–Ω–æ–º—É —Ç–µ–∫—Å—Ç—ñ, –º–æ–∂–µ –±—É—Ç–∏ –∞–¥–∞–ø—Ç–æ–≤–∞–Ω–∞ –¥–ª—è –º–µ–¥–∏—á–Ω–æ—ó –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏, —é—Ä–∏–¥–∏—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É —á–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–¥—É.",
                "links": []
              },
              {
                "text": "–ì–µ–Ω–µ—Ä–∞–ª—ñ—Å—Ç vs –°–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç",
                "desc": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ ‚Äî –≥–µ–Ω–µ—Ä–∞–ª—ñ—Å—Ç–∏: –¥–æ–±—Ä—ñ —É –±–∞–≥–∞—Ç—å–æ—Ö —Ä–µ—á–∞—Ö, –∞–ª–µ –Ω–µ –Ω–∞–π–∫—Ä–∞—â—ñ –≤ –∂–æ–¥–Ω—ñ–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ–π. –ó–∞–¥–∞—á–Ω–æ-—Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏–π —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ —Å—Ç–≤–æ—Ä—é—î —Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç—ñ–≤ –∑–∞ —Ä–∞—Ö—É–Ω–æ–∫ —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—ñ.",
                "links": [
                  {
                    "title": "–ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥",
                    "href": "../level-3/training-finetuning.html"
                  }
                ]
              },
              {
                "text": "–ï–∫–æ—Å–∏—Å—Ç–µ–º–∞ –º–æ–¥–µ–ª–µ–π",
                "desc": "–ë–∞–∑–æ–≤—ñ –º–æ–¥–µ–ª—ñ ‚Üí —Ñ–∞–π–Ω-—Ç—é–Ω–∏ ‚Üí –¥–∏—Å—Ç–∏–ª—å–æ–≤–∞–Ω—ñ –≤–µ—Ä—Å—ñ—ó ‚Üí API-—Å–µ—Ä–≤—ñ—Å–∏. –ö–æ–∂–µ–Ω –∫—Ä–æ–∫ —É –ª–∞–Ω—Ü—é–≥—É –æ–±–º—ñ–Ω—é—î –∑–∞–≥–∞–ª—å–Ω—ñ—Å—Ç—å –Ω–∞ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ—Å—Ç—å –∞–±–æ —Ä–æ–∑–º—ñ—Ä –Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å.",
                "links": [
                  {
                    "title": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π",
                    "href": "../level-3/model-optimization.html"
                  }
                ]
              },
              {
                "text": "–í—ñ–¥–∫—Ä–∏—Ç—ñ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                "desc": "Llama 3, Qwen 2.5, Mistral, OLMo ‚Äî –±—É–¥—å-—Ö—Ç–æ –º–æ–∂–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏, –∑–∞–ø—É—Å—Ç–∏—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ, —Ñ–∞–π–Ω-—Ç—é–Ω–∏—Ç–∏ —Ç–∞ –¥–æ—Å–ª—ñ–¥–∂—É–≤–∞—Ç–∏ —Ü—ñ –º–æ–¥–µ–ª—ñ. –í—ñ–¥–∫—Ä–∏—Ç—ñ –≤–∞–≥–∏ –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —Ç–∞ –∫–∞—Å—Ç–æ–º—ñ–∑–∞—Ü—ñ—é.",
                "links": [
                  {
                    "title": "–í–µ–ª–∏–∫—ñ –≥—Ä–∞–≤—Ü—ñ",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "–ó–∞–∫—Ä–∏—Ç—ñ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                "desc": "GPT-4, Claude, Gemini ‚Äî –¥–æ—Å—Ç—É–ø –ª–∏—à–µ —á–µ—Ä–µ–∑ API. –ó–∞–∑–≤–∏—á–∞–π –ø—Ä–æ–ø–æ–Ω—É—é—Ç—å –Ω–∞–π–≤–∏—â—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å, –∞–ª–µ –∑ –º–µ–Ω—à–æ—é –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—é —Ç–∞ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º. –í–∏ –ø–ª–∞—Ç–∏—Ç–µ –∑–∞ —Ç–æ–∫–µ–Ω.",
                "links": [
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              },
              {
                "text": "–§–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –¥–ª—è –¥–æ–º–µ–Ω—ñ–≤",
                "desc": "–ê–¥–∞–ø—Ç–∞—Ü—ñ—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ—ó –º–æ–¥–µ–ª—ñ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –¥–æ–º–µ–Ω—ñ–≤ (–º–µ–¥–∏—Ü–∏–Ω–∞, —é—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü—ñ—è, –∫–æ–¥—É–≤–∞–Ω–Ω—è) –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –∫—É—Ä–∞—Ç–æ—Ä—Å—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤. –ó–±–µ—Ä—ñ–≥–∞—î –∑–∞–≥–∞–ª—å–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ, –ø–æ–∫—Ä–∞—â—É—é—á–∏ –¥–æ–º–µ–Ω–Ω—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å.",
                "links": [
                  {
                    "title": "–ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥",
                    "href": "../level-3/training-finetuning.html"
                  }
                ]
              },
              {
                "text": "–ù–∞–≤—á–∞–Ω–Ω—è –∑ –∫—ñ–ª—å–∫–æ—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤",
                "desc": "–ù–∞–¥–∞–Ω–Ω—è –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —É –ø—Ä–æ–º–ø—Ç—ñ –¥–ª—è –∫–µ—Ä—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–ª—é –±–µ–∑ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è. –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ –º–æ–∂—É—Ç—å –≤–∏–≤—á–∏—Ç–∏ –Ω–æ–≤—ñ –∑–∞–¥–∞—á—ñ –Ω–∞ –ª—å–æ—Ç—É –∑ –ª–∏—à–µ 2-5 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.",
                "links": [
                  {
                    "title": "–ü—Ä–æ–º–ø—Ç",
                    "href": "../level-2/prompt.html"
                  }
                ]
              },
              {
                "text": "–í–∞—Ä—Ç—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è",
                "desc": "–§—Ä–æ–Ω—Ç–∏—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ –∫–æ—à—Ç—É—é—Ç—å $100M+ –¥–ª—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è. –ê–ª–µ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –∫–æ—à—Ç—É—î $100-$10K, –∞ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ –º–∞–π–∂–µ –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∏–π ‚Äî –µ–∫–æ—Å–∏—Å—Ç–µ–º–∞ —Ä–æ–±–∏—Ç—å –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –¥–æ—Å—Ç—É–ø–Ω–∏–º–∏ –Ω–∞ –∫–æ–∂–Ω–æ–º—É –±—é–¥–∂–µ—Ç—ñ.",
                "links": [
                  {
                    "title": "SOTA",
                    "href": "sota.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Foundation Model",
                "def": "Large model pre-trained on broad data, adapted to many tasks via fine-tuning or prompting."
              },
              {
                "term": "Transfer Learning",
                "def": "Applying knowledge gained from one task/dataset to a different but related task."
              },
              {
                "term": "Few-Shot Learning",
                "def": "Providing a few examples in the prompt to guide model behavior without retraining."
              },
              {
                "term": "Fine-Tuning",
                "def": "Further training a pre-trained model on task-specific data to improve its performance on that task."
              }
            ],
            "uk": [
              {
                "term": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å",
                "def": "–í–µ–ª–∏–∫–∞ –º–æ–¥–µ–ª—å, –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω–∞ –Ω–∞ —à–∏—Ä–æ–∫–∏—Ö –¥–∞–Ω–∏—Ö, –∞–¥–∞–ø—Ç–æ–≤–∞–Ω–∞ –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö –∑–∞–¥–∞—á —á–µ—Ä–µ–∑ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –∞–±–æ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥."
              },
              {
                "term": "–¢—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è",
                "def": "–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –∑–Ω–∞–Ω—å, –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –∑ –æ–¥–Ω—ñ—î—ó –∑–∞–¥–∞—á—ñ/–Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö –¥–æ —ñ–Ω—à–æ—ó, –∞–ª–µ –ø–æ–≤'—è–∑–∞–Ω–æ—ó –∑–∞–¥–∞—á—ñ."
              },
              {
                "term": "–ù–∞–≤—á–∞–Ω–Ω—è –∑ –∫—ñ–ª—å–∫–æ—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤",
                "def": "–ù–∞–¥–∞–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —É –ø—Ä–æ–º–ø—Ç—ñ –¥–ª—è –∫–µ—Ä—É–≤–∞–Ω–Ω—è –ø–æ–≤–µ–¥—ñ–Ω–∫–æ—é –º–æ–¥–µ–ª—ñ –±–µ–∑ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è."
              },
              {
                "term": "–§–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥",
                "def": "–ü–æ–¥–∞–ª—å—à–µ –Ω–∞–≤—á–∞–Ω–Ω—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ –Ω–∞ –∑–∞–¥–∞—á–Ω–æ-—Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö."
              }
            ]
          },
          "tips": {
            "en": [
              "Don't assume the newest model is always best for your task ‚Äî smaller, specialized models often outperform general-purpose frontier models in narrow domains",
              "Open-weight models (Llama, Qwen, Mistral) let you run AI locally with no API costs and full data privacy ‚Äî evaluate them before committing to paid APIs",
              "Foundation models improve rapidly ‚Äî reevaluate your model choice every 3-6 months, as a free open model today may match last year's frontier"
            ],
            "uk": [
              "–ù–µ –ø—Ä–∏–ø—É—Å–∫–∞–π—Ç–µ —â–æ –Ω–∞–π–Ω–æ–≤—ñ—à–∞ –º–æ–¥–µ–ª—å –∑–∞–≤–∂–¥–∏ –Ω–∞–π–∫—Ä–∞—â–∞ –¥–ª—è –≤–∞—à–æ—ó –∑–∞–¥–∞—á—ñ ‚Äî –º–µ–Ω—à—ñ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ —á–∞—Å—Ç–æ –ø–µ—Ä–µ–≤–µ—Ä—à—É—é—Ç—å –∑–∞–≥–∞–ª—å–Ω—ñ —Ñ—Ä–æ–Ω—Ç–∏—Ä–Ω—ñ —É –≤—É–∑—å–∫–∏—Ö –¥–æ–º–µ–Ω–∞—Ö",
              "–í—ñ–¥–∫—Ä–∏—Ç—ñ –º–æ–¥–µ–ª—ñ (Llama, Qwen, Mistral) –¥–æ–∑–≤–æ–ª—è—é—Ç—å –∑–∞–ø—É—Å–∫–∞—Ç–∏ –®–Ü –ª–æ–∫–∞–ª—å–Ω–æ –±–µ–∑ –≤–∏—Ç—Ä–∞—Ç –Ω–∞ API —Ç–∞ –∑ –ø–æ–≤–Ω–æ—é –ø—Ä–∏–≤–∞—Ç–Ω—ñ—Å—Ç—é ‚Äî –æ—Ü—ñ–Ω—ñ—Ç—å —ó—Ö –ø–µ—Ä–µ–¥ –∑–æ–±–æ–≤'—è–∑–∞–Ω–Ω—è–º –ø–ª–∞—Ç–Ω–∏—Ö API",
              "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ –ø–æ–∫—Ä–∞—â—É—é—Ç—å—Å—è —Å—Ç—Ä—ñ–º–∫–æ ‚Äî –ø–µ—Ä–µ–æ—Ü—ñ–Ω—é–π—Ç–µ –≤–∏–±—ñ—Ä –∫–æ–∂–Ω—ñ 3-6 –º—ñ—Å—è—Ü—ñ–≤, –±–æ –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∞ –≤—ñ–¥–∫—Ä–∏—Ç–∞ –º–æ–¥–µ–ª—å —Å—å–æ–≥–æ–¥–Ω—ñ –º–æ–∂–µ –¥–æ—Ä—ñ–≤–Ω—é–≤–∞—Ç–∏ –º–∏–Ω—É–ª–æ—Ä—ñ—á–Ω—ñ–π —Ñ—Ä–æ–Ω—Ç–∏—Ä–Ω—ñ–π"
            ]
          },
          "related": [
            "Models"
          ]
        },
        {
          "slug": "data-classification",
          "title": {
            "en": "Data Type Classification",
            "uk": "–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∑–∞ —Ç–∏–ø–æ–º –¥–∞–Ω–∏—Ö"
          },
          "desc": {
            "en": "Categorizing AI models by what data types they handle as input and output.",
            "uk": "–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π –®–Ü –∑–∞ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–∏—Ö, —è–∫—ñ –≤–æ–Ω–∏ –æ–±—Ä–æ–±–ª—è—é—Ç—å."
          },
          "overview": {
            "en": [
              "AI models can be categorized by the types of data they process as inputs and produce as outputs. Understanding this classification helps you choose the right model for each task. A text-to-text model (LLM) handles different tasks than an image-to-text model (captioning) or a text-to-image model (diffusion).",
              "Modern models increasingly blur these boundaries ‚Äî multimodal foundation models can handle multiple data types in a single conversation. But understanding the underlying classification helps you design effective AI pipelines and choose appropriate APIs."
            ],
            "uk": [
              "–ú–æ–¥–µ–ª—ñ –®–Ü –º–æ–∂–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑—É–≤–∞—Ç–∏ –∑–∞ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–∏—Ö, —è–∫—ñ –≤–æ–Ω–∏ –æ–±—Ä–æ–±–ª—è—é—Ç—å —è–∫ –≤—Ö–æ–¥–∏ —Ç–∞ –≤–∏–¥–∞—é—Ç—å —è–∫ –≤–∏—Ö–æ–¥–∏. –†–æ–∑—É–º—ñ–Ω–Ω—è —Ü—ñ—î—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –¥–æ–ø–æ–º–∞–≥–∞—î –æ–±—Ä–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—É –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–∂–Ω–æ—ó –∑–∞–¥–∞—á—ñ. –¢–µ–∫—Å—Ç-–≤-—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª—å (LLM) –æ–±—Ä–æ–±–ª—è—î —ñ–Ω—à—ñ –∑–∞–¥–∞—á—ñ –Ω—ñ–∂ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è-–≤-—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª—å (–ø—ñ–¥–ø–∏—Å–∏) –∞–±–æ —Ç–µ–∫—Å—Ç-–≤-–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—å (–¥–∏—Ñ—É–∑—ñ—è).",
              "–°—É—á–∞—Å–Ω—ñ –º–æ–¥–µ–ª—ñ –≤—Å–µ –±—ñ–ª—å—à–µ —Ä–æ–∑–º–∏–≤–∞—é—Ç—å —Ü—ñ –º–µ–∂—ñ ‚Äî –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ –º–æ–∂—É—Ç—å –æ–±—Ä–æ–±–ª—è—Ç–∏ –∫—ñ–ª—å–∫–∞ —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö –≤ –æ–¥–Ω—ñ–π —Ä–æ–∑–º–æ–≤—ñ. –ê–ª–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –±–∞–∑–æ–≤–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –¥–æ–ø–æ–º–∞–≥–∞—î –ø—Ä–æ—î–∫—Ç—É–≤–∞—Ç–∏ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ –®–Ü-–ø–∞–π–ø–ª–∞–π–Ω–∏ —Ç–∞ –æ–±–∏—Ä–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ API."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Text ‚Üí Text (LLMs)",
                "desc": "Chat, writing, analysis, translation, summarization. Models: GPT-4, Claude, Gemini, Llama. The most mature and widely used category of generative AI.",
                "links": [
                  {
                    "title": "LLM and GPT",
                    "href": "llm-and-gpt.html"
                  }
                ]
              },
              {
                "text": "Text ‚Üí Image (Diffusion)",
                "desc": "Generate images from text descriptions. Models: DALL-E 3, Midjourney, Stable Diffusion, Flux. Quality has improved from abstract art to photorealistic outputs in just 2 years.",
                "links": [
                  {
                    "title": "Diffusion Models",
                    "href": "diffusion-models.html"
                  }
                ]
              },
              {
                "text": "Image ‚Üí Text (Vision)",
                "desc": "Captioning, OCR, visual question-answering. Models: GPT-4V, Claude Vision, Gemini Pro Vision. Enables AI to \"see\" and reason about images, screenshots, documents.",
                "links": [
                  {
                    "title": "Multimodality",
                    "href": "multimodality.html"
                  }
                ]
              },
              {
                "text": "Text ‚Üí Audio (TTS)",
                "desc": "Voice synthesis from text. Models: ElevenLabs, OpenAI TTS, Bark. Modern TTS produces near-human quality speech with emotion, accents, and multiple languages.",
                "links": []
              },
              {
                "text": "Audio ‚Üí Text (Speech Recognition)",
                "desc": "Transcription and speech-to-text. Models: Whisper, AssemblyAI, Deepgram. Enables voice interfaces, meeting transcription, and accessibility features.",
                "links": []
              },
              {
                "text": "Text ‚Üí Video",
                "desc": "Generate video clips from text descriptions. Models: Sora, Runway Gen-3, Kling, Pika. The newest frontier ‚Äî quality is improving rapidly but still limited to short clips.",
                "links": []
              },
              {
                "text": "Text ‚Üí Code",
                "desc": "Code generation and completion from natural language. Models: GPT-4, Claude, Codex, StarCoder. Powers tools like GitHub Copilot, Cursor, and Claude Code.",
                "links": [
                  {
                    "title": "Vibecoding",
                    "href": "../level-2/vibecoding.html"
                  }
                ]
              },
              {
                "text": "Code ‚Üí Text",
                "desc": "Code explanation, documentation generation, and review. All major LLMs excel at reading and explaining code, making it one of the highest-value AI applications.",
                "links": []
              },
              {
                "text": "Image ‚Üí Image",
                "desc": "Image editing, style transfer, super-resolution, inpainting. Models: ControlNet, Instruct-Pix2Pix. Transform existing images rather than generating from scratch.",
                "links": [
                  {
                    "title": "Diffusion Models",
                    "href": "diffusion-models.html"
                  }
                ]
              },
              {
                "text": "Audio ‚Üí Audio",
                "desc": "Voice conversion, music remixing, noise removal, audio enhancement. Specialized models that transform audio inputs without going through text as an intermediate.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "–¢–µ–∫—Å—Ç ‚Üí –¢–µ–∫—Å—Ç (LLM)",
                "desc": "–ß–∞—Ç, –Ω–∞–ø–∏—Å–∞–Ω–Ω—è, –∞–Ω–∞–ª—ñ–∑, –ø–µ—Ä–µ–∫–ª–∞–¥, —Ä–µ–∑—é–º—É–≤–∞–Ω–Ω—è. –ú–æ–¥–µ–ª—ñ: GPT-4, Claude, Gemini, Llama. –ù–∞–π–±—ñ–ª—å—à –∑—Ä—ñ–ª–∞ —Ç–∞ —à–∏—Ä–æ–∫–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –®–Ü.",
                "links": [
                  {
                    "title": "LLM —Ç–∞ GPT",
                    "href": "llm-and-gpt.html"
                  }
                ]
              },
              {
                "text": "–¢–µ–∫—Å—Ç ‚Üí –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è (–î–∏—Ñ—É–∑—ñ—è)",
                "desc": "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–æ–±—Ä–∞–∂–µ–Ω—å –∑ —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –æ–ø–∏—Å—ñ–≤. –ú–æ–¥–µ–ª—ñ: DALL-E 3, Midjourney, Stable Diffusion, Flux. –Ø–∫—ñ—Å—Ç—å –∑—Ä–æ—Å–ª–∞ –≤—ñ–¥ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–≥–æ –º–∏—Å—Ç–µ—Ü—Ç–≤–∞ –¥–æ —Ñ–æ—Ç–æ—Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∑–∞ 2 —Ä–æ–∫–∏.",
                "links": [
                  {
                    "title": "–î–∏—Ñ—É–∑—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "diffusion-models.html"
                  }
                ]
              },
              {
                "text": "–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è ‚Üí –¢–µ–∫—Å—Ç (–ó—ñ—Ä)",
                "desc": "–ü—ñ–¥–ø–∏—Å–∏, OCR, –≤—ñ–∑—É–∞–ª—å–Ω—ñ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è-–≤—ñ–¥–ø–æ–≤—ñ–¥—ñ. –ú–æ–¥–µ–ª—ñ: GPT-4V, Claude Vision, Gemini Pro Vision. –î–æ–∑–≤–æ–ª—è—î –®–Ü \"–±–∞—á–∏—Ç–∏\" —Ç–∞ –º—ñ—Ä–∫—É–≤–∞—Ç–∏ –ø—Ä–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —Å–∫—Ä—ñ–Ω—à–æ—Ç–∏, –¥–æ–∫—É–º–µ–Ω—Ç–∏.",
                "links": [
                  {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å",
                    "href": "multimodality.html"
                  }
                ]
              },
              {
                "text": "–¢–µ–∫—Å—Ç ‚Üí –ê—É–¥—ñ–æ (TTS)",
                "desc": "–°–∏–Ω—Ç–µ–∑ –≥–æ–ª–æ—Å—É –∑ —Ç–µ–∫—Å—Ç—É. –ú–æ–¥–µ–ª—ñ: ElevenLabs, OpenAI TTS, Bark. –°—É—á–∞—Å–Ω–∏–π TTS —Å—Ç–≤–æ—Ä—é—î –º–æ–≤—É –º–∞–π–∂–µ –ª—é–¥—Å—å–∫–æ—ó —è–∫–æ—Å—Ç—ñ –∑ –µ–º–æ—Ü—ñ—è–º–∏, –∞–∫—Ü–µ–Ω—Ç–∞–º–∏ —Ç–∞ —Ä—ñ–∑–Ω–∏–º–∏ –º–æ–≤–∞–º–∏.",
                "links": []
              },
              {
                "text": "–ê—É–¥—ñ–æ ‚Üí –¢–µ–∫—Å—Ç (–†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–∏)",
                "desc": "–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è —Ç–∞ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–≤–∏ –≤ —Ç–µ–∫—Å—Ç. –ú–æ–¥–µ–ª—ñ: Whisper, AssemblyAI, Deepgram. –ó–∞–±–µ–∑–ø–µ—á—É—î –≥–æ–ª–æ—Å–æ–≤—ñ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏, —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—é –∑—É—Å—Ç—Ä—ñ—á–µ–π —Ç–∞ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å.",
                "links": []
              },
              {
                "text": "–¢–µ–∫—Å—Ç ‚Üí –í—ñ–¥–µ–æ",
                "desc": "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–µ–æ–∫–ª—ñ–ø—ñ–≤ –∑ —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –æ–ø–∏—Å—ñ–≤. –ú–æ–¥–µ–ª—ñ: Sora, Runway Gen-3, Kling, Pika. –ù–∞–π–Ω–æ–≤—ñ—à–∏–π —Ñ—Ä–æ–Ω—Ç—ñ—Ä ‚Äî —è–∫—ñ—Å—Ç—å —à–≤–∏–¥–∫–æ –ø–æ–∫—Ä–∞—â—É—î—Ç—å—Å—è, –∞–ª–µ –ø–æ–∫–∏ –æ–±–º–µ–∂–µ–Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –∫–ª—ñ–ø–∞–º–∏.",
                "links": []
              },
              {
                "text": "–¢–µ–∫—Å—Ç ‚Üí –ö–æ–¥",
                "desc": "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–∞ –¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–æ–¥—É –∑ –ø—Ä–∏—Ä–æ–¥–Ω–æ—ó –º–æ–≤–∏. –ú–æ–¥–µ–ª—ñ: GPT-4, Claude, Codex, StarCoder. –ñ–∏–≤–∏—Ç—å —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —è–∫ GitHub Copilot, Cursor —Ç–∞ Claude Code.",
                "links": [
                  {
                    "title": "–í–∞–π–±–∫–æ–¥–∏–Ω–≥",
                    "href": "../level-2/vibecoding.html"
                  }
                ]
              },
              {
                "text": "–ö–æ–¥ ‚Üí –¢–µ–∫—Å—Ç",
                "desc": "–ü–æ—è—Å–Ω–µ–Ω–Ω—è –∫–æ–¥—É, –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—ó —Ç–∞ —Ä–µ–≤—é. –£—Å—ñ –æ—Å–Ω–æ–≤–Ω—ñ LLM –≤—ñ–¥–º—ñ–Ω–Ω–æ —á–∏—Ç–∞—é—Ç—å —Ç–∞ –ø–æ—è—Å–Ω—é—é—Ç—å –∫–æ–¥, —Ä–æ–±–ª—è—á–∏ —Ü–µ –æ–¥–Ω–∏–º –∑ –Ω–∞–π—Ü—ñ–Ω–Ω—ñ—à–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å –®–Ü.",
                "links": []
              },
              {
                "text": "–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è ‚Üí –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è",
                "desc": "–†–µ–¥–∞–≥—É–≤–∞–Ω–Ω—è, –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç–∏–ª—é, —Å—É–ø–µ—Ä—Ä–æ–∑–¥—ñ–ª—å–Ω—ñ—Å—Ç—å, —ñ–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥. –ú–æ–¥–µ–ª—ñ: ControlNet, Instruct-Pix2Pix. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è —ñ—Å–Ω—É—é—á–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å –∑–∞–º—ñ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑ –Ω—É–ª—è.",
                "links": [
                  {
                    "title": "–î–∏—Ñ—É–∑—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "diffusion-models.html"
                  }
                ]
              },
              {
                "text": "–ê—É–¥—ñ–æ ‚Üí –ê—É–¥—ñ–æ",
                "desc": "–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –≥–æ–ª–æ—Å—É, —Ä–µ–º—ñ–∫—Å –º—É–∑–∏–∫–∏, –≤–∏–¥–∞–ª–µ–Ω–Ω—è —à—É–º—É, –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –∞—É–¥—ñ–æ. –°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ, —â–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º—É—é—Ç—å –∞—É–¥—ñ–æ –≤—Ö–æ–¥–∏ –±–µ–∑ –ø—Ä–æ–º—ñ–∂–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –µ—Ç–∞–ø—É.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Modality",
                "def": "The type of data a model works with: text, image, audio, video, or code."
              },
              {
                "term": "Pipeline",
                "def": "A chain of models processing data, e.g., audio‚Üítext‚Üítext‚Üíaudio for a voice chatbot."
              },
              {
                "term": "Embedding",
                "def": "A numerical representation of data (text, image) in a vector space, enabling semantic search."
              }
            ],
            "uk": [
              {
                "term": "–ú–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å",
                "def": "–¢–∏–ø –¥–∞–Ω–∏—Ö, –∑ —è–∫–∏–º –ø—Ä–∞—Ü—é—î –º–æ–¥–µ–ª—å: —Ç–µ–∫—Å—Ç, –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –∞—É–¥—ñ–æ, –≤—ñ–¥–µ–æ –∞–±–æ –∫–æ–¥."
              },
              {
                "term": "–ü–∞–π–ø–ª–∞–π–Ω",
                "def": "–õ–∞–Ω—Ü—é–≥ –º–æ–¥–µ–ª–µ–π, —â–æ –æ–±—Ä–æ–±–ª—è—é—Ç—å –¥–∞–Ω—ñ, –Ω–∞–ø—Ä., –∞—É–¥—ñ–æ‚Üí—Ç–µ–∫—Å—Ç‚Üí—Ç–µ–∫—Å—Ç‚Üí–∞—É–¥—ñ–æ –¥–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —á–∞—Ç–±–æ—Ç–∞."
              },
              {
                "term": "–ï–º–±–µ–¥—ñ–Ω–≥",
                "def": "–ß–∏—Å–ª–æ–≤–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö (—Ç–µ–∫—Å—Ç—É, –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è) —É –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ –ø–æ—à—É–∫—É."
              }
            ]
          },
          "tips": {
            "en": [
              "When choosing a model, first identify your primary data modality ‚Äî a text-to-image model is different from a text+image understanding model, even if both involve images",
              "Build pipelines of specialized models (speech-to-text ‚Üí LLM ‚Üí text-to-speech) rather than using one model for everything ‚Äî specialized models are usually better and cheaper",
              "Embedding models are underrated tools ‚Äî they enable semantic search, recommendation systems, and clustering at a fraction of the cost of LLM calls"
            ],
            "uk": [
              "–ü—Ä–∏ –≤–∏–±–æ—Ä—ñ –º–æ–¥–µ–ª—ñ —Å–ø–æ—á–∞—Ç–∫—É –≤–∏–∑–Ω–∞—á—Ç–µ –æ—Å–Ω–æ–≤–Ω—É –º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å ‚Äî –º–æ–¥–µ–ª—å —Ç–µ–∫—Å—Ç-–≤-–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –≤—ñ–¥ –º–æ–¥–µ–ª—ñ —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É+–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –æ–±–∏–¥–≤—ñ –ø—Ä–∞—Ü—é—é—Ç—å —ñ–∑ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è–º–∏",
              "–ë—É–¥—É–π—Ç–µ –ø–∞–π–ø–ª–∞–π–Ω–∏ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π (–º–æ–≤–∞-–≤-—Ç–µ–∫—Å—Ç ‚Üí LLM ‚Üí —Ç–µ–∫—Å—Ç-–≤-–º–æ–≤—É) –∑–∞–º—ñ—Å—Ç—å –æ–¥–Ω—ñ—î—ó –º–æ–¥–µ–ª—ñ –¥–ª—è –≤—Å—å–æ–≥–æ ‚Äî —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –∑–∞–∑–≤–∏—á–∞–π –∫—Ä–∞—â—ñ —Ç–∞ –¥–µ—à–µ–≤—à—ñ",
              "–ú–æ–¥–µ–ª—ñ –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ ‚Äî —Ü–µ –Ω–µ–¥–æ–æ—Ü—ñ–Ω–µ–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏: –≤–æ–Ω–∏ –¥–æ–∑–≤–æ–ª—è—é—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π–Ω—ñ —Å–∏—Å—Ç–µ–º–∏ —Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—é –∑–∞ —á–∞—Å—Ç–∫—É –≤–∞—Ä—Ç–æ—Å—Ç—ñ LLM-–≤–∏–∫–ª–∏–∫—ñ–≤"
            ]
          },
          "related": [
            "Models"
          ]
        },
        {
          "slug": "sota",
          "title": {
            "en": "State of the Art (SOTA)",
            "uk": "–°—Ç–∞–Ω –º–∏—Å—Ç–µ—Ü—Ç–≤–∞ (SOTA)"
          },
          "desc": {
            "en": "Understanding state-of-the-art benchmarks, rankings, and how to track the latest.",
            "uk": "–†–æ–∑—É–º—ñ–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫—ñ–≤, —Ä–µ–π—Ç–∏–Ω–≥—ñ–≤ —Ç–∞ —è–∫ —Å—Ç–µ–∂–∏—Ç–∏ –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ–º–∏ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è–º–∏."
          },
          "overview": {
            "en": [
              "State of the Art (SOTA) refers to the highest level of performance achieved on a specific task or benchmark at any given time. In the fast-moving AI field, SOTA changes frequently ‚Äî sometimes weekly. Understanding benchmarks and leaderboards helps you evaluate model claims and choose the right tools.",
              "However, benchmarks have significant limitations. Models may be optimized specifically for benchmark performance (overfitting), results may not reflect real-world usage, and different benchmarks measure different things. Learning to critically evaluate SOTA claims is an essential skill."
            ],
            "uk": [
              "State of the Art (SOTA) —Å—Ç–æ—Å—É—î—Ç—å—Å—è –Ω–∞–π–≤–∏—â–æ–≥–æ —Ä—ñ–≤–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ, –¥–æ—Å—è–≥–Ω—É—Ç–æ–≥–æ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ–π –∑–∞–¥–∞—á—ñ –∞–±–æ –±–µ–Ω—á–º–∞—Ä–∫—É –≤ –±—É–¥—å-—è–∫–∏–π –º–æ–º–µ–Ω—Ç —á–∞—Å—É. –£ —à–≤–∏–¥–∫–æ—Ä—É—Ö–æ–º—ñ–π –≥–∞–ª—É–∑—ñ –®–Ü SOTA –∑–º—ñ–Ω—é—î—Ç—å—Å—è —á–∞—Å—Ç–æ ‚Äî —ñ–Ω–æ–¥—ñ —â–æ—Ç–∏–∂–Ω—è. –†–æ–∑—É–º—ñ–Ω–Ω—è –±–µ–Ω—á–º–∞—Ä–∫—ñ–≤ —Ç–∞ –ª—ñ–¥–µ—Ä–±–æ—Ä–¥—ñ–≤ –¥–æ–ø–æ–º–∞–≥–∞—î –æ—Ü—ñ–Ω—é–≤–∞—Ç–∏ –∑–∞—è–≤–∏ –ø—Ä–æ –º–æ–¥–µ–ª—ñ —Ç–∞ –æ–±–∏—Ä–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏.",
              "–û–¥–Ω–∞–∫ –±–µ–Ω—á–º–∞—Ä–∫–∏ –º–∞—é—Ç—å –∑–Ω–∞—á–Ω—ñ –æ–±–º–µ–∂–µ–Ω–Ω—è. –ú–æ–¥–µ–ª—ñ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ –ø—ñ–¥ –±–µ–Ω—á–º–∞—Ä–∫–∏ (–ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è), —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –º–æ–∂—É—Ç—å –Ω–µ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—Ç–∏ —Ä–µ–∞–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è, –∞ —Ä—ñ–∑–Ω—ñ –±–µ–Ω—á–º–∞—Ä–∫–∏ –≤–∏–º—ñ—Ä—é—é—Ç—å —Ä—ñ–∑–Ω—ñ —Ä–µ—á—ñ. –ù–∞–≤—á–∏—Ç–∏—Å—è –∫—Ä–∏—Ç–∏—á–Ω–æ –æ—Ü—ñ–Ω—é–≤–∞—Ç–∏ SOTA-–∑–∞—è–≤–∏ ‚Äî —Ü–µ –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–π –Ω–∞–≤–∏–∫."
            ]
          },
          "details": {
            "en": [
              {
                "text": "What SOTA Means",
                "desc": "The best published result on a standard benchmark task at a given time. In AI, SOTA changes frequently ‚Äî sometimes weekly ‚Äî as new models and techniques are released.",
                "links": []
              },
              {
                "text": "Text & Knowledge Benchmarks",
                "desc": "MMLU (massive multitask knowledge), HellaSwag (commonsense reasoning), ARC (science questions), TruthfulQA (factual accuracy). These measure how well models understand and reason about language.",
                "links": []
              },
              {
                "text": "Code Benchmarks",
                "desc": "HumanEval, MBPP (basic programming), SWE-bench (real-world software engineering tasks), LiveCodeBench (fresh problems). Code benchmarks test practical programming capability.",
                "links": [
                  {
                    "title": "Vibecoding",
                    "href": "../level-2/vibecoding.html"
                  }
                ]
              },
              {
                "text": "Math Benchmarks",
                "desc": "MATH (competition math), GSM8K (grade school math), Olympiad-level problems. Mathematical reasoning is one of the hardest capabilities for LLMs and a key differentiator between models.",
                "links": [
                  {
                    "title": "Reasoning",
                    "href": "reasoning.html"
                  }
                ]
              },
              {
                "text": "Reasoning Benchmarks",
                "desc": "ARC-AGI (abstract reasoning), Big-Bench Hard (challenging diverse tasks), GPQA (graduate-level questions). These push the boundaries of what models can figure out.",
                "links": [
                  {
                    "title": "Reasoning",
                    "href": "reasoning.html"
                  }
                ]
              },
              {
                "text": "Human Preference Leaderboards",
                "desc": "Chatbot Arena (LMSYS) ‚Äî real users vote between anonymous model outputs. Widely considered the most reliable ranking because it reflects actual user satisfaction, not just benchmark scores.",
                "links": []
              },
              {
                "text": "Open LLM Leaderboard",
                "desc": "Hugging Face's automated benchmark suite for open-weight models. Useful for comparing open-source options but scores can be gamed through benchmark-specific optimization.",
                "links": [
                  {
                    "title": "The Big Players",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "Evaluating Model Claims",
                "desc": "Look beyond headline numbers: check benchmark methodology, compare across multiple benchmarks, test on your own tasks. Marketing cherry-picks the best scores.",
                "links": []
              },
              {
                "text": "Benchmark Contamination",
                "desc": "When benchmark test data leaks into training data (accidentally or deliberately), scores become artificially inflated. This is a growing problem as training datasets expand.",
                "links": [
                  {
                    "title": "Data to Model",
                    "href": "../level-3/data-to-model.html"
                  }
                ]
              },
              {
                "text": "Where to Follow AI Progress",
                "desc": "AI Twitter/X for breaking news, Papers With Code for SOTA tracking, Hugging Face for models, arXiv for papers, AI newsletters (The Batch, Import AI) for curated summaries.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "–©–æ –æ–∑–Ω–∞—á–∞—î SOTA",
                "desc": "–ù–∞–π–∫—Ä–∞—â–∏–π –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ–π –±–µ–Ω—á–º–∞—Ä–∫–æ–≤—ñ–π –∑–∞–¥–∞—á—ñ –≤ –¥–∞–Ω–∏–π –º–æ–º–µ–Ω—Ç. –£ –®–Ü SOTA –∑–º—ñ–Ω—é—î—Ç—å—Å—è —á–∞—Å—Ç–æ ‚Äî —ñ–Ω–æ–¥—ñ —â–æ—Ç–∏–∂–Ω—è ‚Äî –∫–æ–ª–∏ –≤–∏–ø—É—Å–∫–∞—é—Ç—å—Å—è –Ω–æ–≤—ñ –º–æ–¥–µ–ª—ñ —Ç–∞ —Ç–µ—Ö–Ω—ñ–∫–∏.",
                "links": []
              },
              {
                "text": "–¢–µ–∫—Å—Ç–æ–≤—ñ —Ç–∞ –∑–Ω–∞–Ω–Ω—î–≤—ñ –±–µ–Ω—á–º–∞—Ä–∫–∏",
                "desc": "MMLU (–º–∞—Å–∏–≤–Ω–µ –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–µ –∑–Ω–∞–Ω–Ω—è), HellaSwag (–∑–¥–æ—Ä–æ–≤–∏–π –≥–ª—É–∑–¥), ARC (–Ω–∞—É–∫–æ–≤—ñ –ø–∏—Ç–∞–Ω–Ω—è), TruthfulQA (—Ñ–∞–∫—Ç–∏—á–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å). –í–∏–º—ñ—Ä—é—é—Ç—å —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ç–∞ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –ø—Ä–æ –º–æ–≤—É.",
                "links": []
              },
              {
                "text": "–ö–æ–¥–æ–≤—ñ –±–µ–Ω—á–º–∞—Ä–∫–∏",
                "desc": "HumanEval, MBPP (–±–∞–∑–æ–≤–µ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è), SWE-bench (—Ä–µ–∞–ª—å–Ω—ñ –∑–∞–¥–∞—á—ñ —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—ó –ü–ó), LiveCodeBench (—Å–≤—ñ–∂—ñ –∑–∞–¥–∞—á—ñ). –¢–µ—Å—Ç—É—é—Ç—å –ø—Ä–∞–∫—Ç–∏—á–Ω—É –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–í–∞–π–±–∫–æ–¥–∏–Ω–≥",
                    "href": "../level-2/vibecoding.html"
                  }
                ]
              },
              {
                "text": "–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω—ñ –±–µ–Ω—á–º–∞—Ä–∫–∏",
                "desc": "MATH (–æ–ª—ñ–º–ø—ñ–∞–¥–Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞), GSM8K (–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –ø–æ—á–∞—Ç–∫–æ–≤–æ—ó —à–∫–æ–ª–∏), –∑–∞–¥–∞—á—ñ –æ–ª—ñ–º–ø—ñ–∞–¥–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è. –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–µ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è ‚Äî –æ–¥–Ω–∞ –∑ –Ω–∞–π—Å–∫–ª–∞–¥–Ω—ñ—à–∏—Ö –∑–¥—ñ–±–Ω–æ—Å—Ç–µ–π LLM.",
                "links": [
                  {
                    "title": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                    "href": "reasoning.html"
                  }
                ]
              },
              {
                "text": "–ë–µ–Ω—á–º–∞—Ä–∫–∏ –º—ñ—Ä–∫—É–≤–∞–Ω—å",
                "desc": "ARC-AGI (–∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–µ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è), Big-Bench Hard (—Å–∫–ª–∞–¥–Ω—ñ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ –∑–∞–¥–∞—á—ñ), GPQA (–∑–∞–ø–∏—Ç–∞–Ω–Ω—è –∞—Å–ø—ñ—Ä–∞–Ω—Ç—Å—å–∫–æ–≥–æ —Ä—ñ–≤–Ω—è). –ü—ñ–¥—à—Ç–æ–≤—Ö—É—é—Ç—å –º–µ–∂—ñ —Ç–æ–≥–æ, —â–æ –º–æ–¥–µ–ª—ñ –º–æ–∂—É—Ç—å –∑—Ä–æ–∑—É–º—ñ—Ç–∏.",
                "links": [
                  {
                    "title": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                    "href": "reasoning.html"
                  }
                ]
              },
              {
                "text": "–õ—ñ–¥–µ—Ä–±–æ—Ä–¥–∏ –ª—é–¥—Å—å–∫–∏—Ö –ø–µ—Ä–µ–≤–∞–≥",
                "desc": "Chatbot Arena (LMSYS) ‚Äî —Ä–µ–∞–ª—å–Ω—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ –≥–æ–ª–æ—Å—É—é—Ç—å –º—ñ–∂ –∞–Ω–æ–Ω—ñ–º–Ω–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –º–æ–¥–µ–ª–µ–π. –í–≤–∞–∂–∞—î—Ç—å—Å—è –Ω–∞–π–Ω–∞–¥—ñ–π–Ω—ñ—à–∏–º —Ä–µ–π—Ç–∏–Ω–≥–æ–º, –±–æ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—î —Ä–µ–∞–ª—å–Ω–µ –∑–∞–¥–æ–≤–æ–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤.",
                "links": []
              },
              {
                "text": "Open LLM Leaderboard",
                "desc": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–æ–≤–∞–Ω–∏–π –Ω–∞–±—ñ—Ä –±–µ–Ω—á–º–∞—Ä–∫—ñ–≤ Hugging Face –¥–ª—è –º–æ–¥–µ–ª–µ–π –∑ –≤—ñ–¥–∫—Ä–∏—Ç–∏–º–∏ –≤–∞–≥–∞–º–∏. –ö–æ—Ä–∏—Å–Ω–∏–π –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—ñ–¥–∫—Ä–∏—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π, –∞–ª–µ –æ—Ü—ñ–Ω–∫–∏ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –∑–∞–≤–∏—â–µ–Ω—ñ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—î—é –ø—ñ–¥ –±–µ–Ω—á–º–∞—Ä–∫–∏.",
                "links": [
                  {
                    "title": "–í–µ–ª–∏–∫—ñ –≥—Ä–∞–≤—Ü—ñ",
                    "href": "big-players.html"
                  }
                ]
              },
              {
                "text": "–û—Ü—ñ–Ω–∫–∞ –∑–∞—è–≤ –ø—Ä–æ –º–æ–¥–µ–ª—ñ",
                "desc": "–î–∏–≤—ñ—Ç—å—Å—è –∑–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤—ñ —á–∏—Å–ª–∞: –ø–µ—Ä–µ–≤—ñ—Ä—è–π—Ç–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥—ñ—é, –ø–æ—Ä—ñ–≤–Ω—é–π—Ç–µ —á–µ—Ä–µ–∑ –∫—ñ–ª—å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫—ñ–≤, —Ç–µ—Å—Ç—É–π—Ç–µ –Ω–∞ –≤–ª–∞—Å–Ω–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥ –æ–±–∏—Ä–∞—î –Ω–∞–π–∫—Ä–∞—â—ñ –æ—Ü—ñ–Ω–∫–∏.",
                "links": []
              },
              {
                "text": "–ö–æ–Ω—Ç–∞–º—ñ–Ω–∞—Ü—ñ—è –±–µ–Ω—á–º–∞—Ä–∫—ñ–≤",
                "desc": "–ö–æ–ª–∏ —Ç–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ –ø–æ—Ç—Ä–∞–ø–ª—è—é—Ç—å —É –Ω–∞–≤—á–∞–ª—å–Ω—ñ (–≤–∏–ø–∞–¥–∫–æ–≤–æ —á–∏ –Ω–∞–≤–º–∏—Å–Ω–æ), –æ—Ü—ñ–Ω–∫–∏ —à—Ç—É—á–Ω–æ –∑–∞–≤–∏—â—É—é—Ç—å—Å—è. –ó—Ä–æ—Å—Ç–∞—é—á–∞ –ø—Ä–æ–±–ª–µ–º–∞ –ø–æ –º—ñ—Ä—ñ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤.",
                "links": [
                  {
                    "title": "–í—ñ–¥ –¥–∞–Ω–∏—Ö –¥–æ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-3/data-to-model.html"
                  }
                ]
              },
              {
                "text": "–î–µ —Å–ª—ñ–¥–∫—É–≤–∞—Ç–∏ –∑–∞ –ø—Ä–æ–≥—Ä–µ—Å–æ–º –®–Ü",
                "desc": "AI Twitter/X –¥–ª—è –Ω–æ–≤–∏–Ω, Papers With Code –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è SOTA, Hugging Face –¥–ª—è –º–æ–¥–µ–ª–µ–π, arXiv –¥–ª—è —Å—Ç–∞—Ç–µ–π, –®–Ü-—Ä–æ–∑—Å–∏–ª–∫–∏ (The Batch, Import AI) –¥–ª—è –∫—É—Ä–∞—Ç–æ—Ä—Å—å–∫–∏—Ö —Ä–µ–∑—é–º–µ.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "SOTA",
                "def": "State of the Art ‚Äî the best performance achieved on a benchmark at a given time."
              },
              {
                "term": "Benchmark",
                "def": "A standardized test used to measure and compare model performance on specific tasks."
              },
              {
                "term": "Leaderboard",
                "def": "A ranking of models by performance on one or more benchmarks."
              },
              {
                "term": "Contamination",
                "def": "When benchmark test data appears in training data, making scores unreliable."
              },
              {
                "term": "Chatbot Arena",
                "def": "Human preference leaderboard where real users blindly compare model outputs."
              }
            ],
            "uk": [
              {
                "term": "SOTA",
                "def": "State of the Art ‚Äî –Ω–∞–π–∫—Ä–∞—â–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫—É –≤ –¥–∞–Ω–∏–π –º–æ–º–µ–Ω—Ç —á–∞—Å—É."
              },
              {
                "term": "–ë–µ–Ω—á–º–∞—Ä–∫",
                "def": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–∏–π —Ç–µ—Å—Ç –¥–ª—è –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è —Ç–∞ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª–µ–π."
              },
              {
                "term": "–õ—ñ–¥–µ—Ä–±–æ—Ä–¥",
                "def": "–†–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π –∑–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—é –Ω–∞ –æ–¥–Ω–æ–º—É –∞–±–æ –∫—ñ–ª—å–∫–æ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö."
              },
              {
                "term": "–ö–æ–Ω—Ç–∞–º—ñ–Ω–∞—Ü—ñ—è",
                "def": "–ö–æ–ª–∏ —Ç–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ –±–µ–Ω—á–º–∞—Ä–∫—É –∑'—è–≤–ª—è—é—Ç—å—Å—è —É –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö, —Ä–æ–±–ª—è—á–∏ –æ—Ü—ñ–Ω–∫–∏ –Ω–µ–Ω–∞–¥—ñ–π–Ω–∏–º–∏."
              },
              {
                "term": "Chatbot Arena",
                "def": "–õ—ñ–¥–µ—Ä–±–æ—Ä–¥ –ª—é–¥—Å—å–∫–∏—Ö –ø–µ—Ä–µ–≤–∞–≥, –¥–µ —Ä–µ–∞–ª—å–Ω—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ —Å–ª—ñ–ø–æ –ø–æ—Ä—ñ–≤–Ω—é—é—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –º–æ–¥–µ–ª–µ–π."
              }
            ]
          },
          "tips": {
            "en": [
              "Always check Chatbot Arena for the most reliable model rankings ‚Äî it uses real human preferences",
              "Don't trust a single benchmark score. Look at performance across multiple diverse benchmarks",
              "The best model on benchmarks may not be the best for your specific use case ‚Äî always test yourself"
            ],
            "uk": [
              "–ó–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–≤—ñ—Ä—è–π—Ç–µ Chatbot Arena –¥–ª—è –Ω–∞–π–Ω–∞–¥—ñ–π–Ω—ñ—à–∏—Ö —Ä–µ–π—Ç–∏–Ω–≥—ñ–≤ –º–æ–¥–µ–ª–µ–π ‚Äî –≤—ñ–Ω –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ä–µ–∞–ª—å–Ω—ñ –ª—é–¥—Å—å–∫—ñ –ø–µ—Ä–µ–≤–∞–≥–∏",
              "–ù–µ –¥–æ–≤—ñ—Ä—è–π—Ç–µ –æ–¥–Ω–æ–º—É –±–µ–Ω—á–º–∞—Ä–∫–æ–≤–æ–º—É –±–∞–ª—É. –î–∏–≤—ñ—Ç—å—Å—è –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —á–µ—Ä–µ–∑ –º–Ω–æ–∂–∏–Ω–Ω—ñ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ –±–µ–Ω—á–º–∞—Ä–∫–∏",
              "–ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –º–æ–∂–µ –Ω–µ –±—É—Ç–∏ –Ω–∞–π–∫—Ä–∞—â–æ—é –¥–ª—è –≤–∞—à–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤–∏–ø–∞–¥–∫—É ‚Äî –∑–∞–≤–∂–¥–∏ —Ç–µ—Å—Ç—É–π—Ç–µ —Å–∞–º—ñ"
            ]
          },
          "related": [
            "Feed",
            "AI Digest"
          ]
        }
      ]
    },
    {
      "num": 2,
      "emoji": "üí°",
      "title": {
        "en": "User",
        "uk": "–ö–æ—Ä–∏—Å—Ç—É–≤–∞—á"
      },
      "desc": {
        "en": "Core concepts for effectively using AI models: prompts, tokens, context, and common pitfalls.",
        "uk": "–û—Å–Ω–æ–≤–Ω—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –®–Ü: –ø—Ä–æ–º–ø—Ç–∏, —Ç–æ–∫–µ–Ω–∏, –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–∞ —Ç–∏–ø–æ–≤—ñ –ø–æ–º–∏–ª–∫–∏."
      },
      "topics": [
        {
          "slug": "prompt",
          "title": {
            "en": "Prompt",
            "uk": "–ü—Ä–æ–º–ø—Ç"
          },
          "desc": {
            "en": "Understanding prompts - the primary interface between humans and AI models.",
            "uk": "–†–æ–∑—É–º—ñ–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤ ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É –º—ñ–∂ –ª—é–¥—å–º–∏ —Ç–∞ –º–æ–¥–µ–ª—è–º–∏ –®–Ü."
          },
          "overview": {
            "en": [
              "A prompt is the text input you send to an AI model ‚Äî it is the primary interface for communicating your intent. The quality of your prompt directly determines the quality of the output. Learning to write effective prompts is the single most impactful skill for anyone working with AI.",
              "Prompts have structure: a system prompt sets the model's behavior and role, user messages provide specific requests, and the conversation history gives context. Understanding this structure and writing clear, specific instructions with relevant context is the foundation of \"prompt engineering.\""
            ],
            "uk": [
              "–ü—Ä–æ–º–ø—Ç ‚Äî —Ü–µ —Ç–µ–∫—Å—Ç–æ–≤–∏–π –≤—Ö—ñ–¥, —è–∫–∏–π –≤–∏ –Ω–∞–¥—Å–∏–ª–∞—î—Ç–µ –º–æ–¥–µ–ª—ñ –®–Ü ‚Äî —Ü–µ –æ—Å–Ω–æ–≤–Ω–∏–π —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–µ—Ä–µ–¥–∞—á—ñ –≤–∞—à–æ–≥–æ –Ω–∞–º—ñ—Ä—É. –Ø–∫—ñ—Å—Ç—å –≤–∞—à–æ–≥–æ –ø—Ä–æ–º–ø—Ç—É –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ –≤–∏–∑–Ω–∞—á–∞—î —è–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É. –ù–∞–≤—á–∏—Ç–∏—Å—è –ø–∏—Å–∞—Ç–∏ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ –ø—Ä–æ–º–ø—Ç–∏ ‚Äî —Ü–µ –Ω–∞–π–±—ñ–ª—å—à –≤–ø–ª–∏–≤–æ–≤–∏–π –Ω–∞–≤–∏–∫ –¥–ª—è –±—É–¥—å-–∫–æ–≥–æ, —Ö—Ç–æ –ø—Ä–∞—Ü—é—î –∑ –®–Ü.",
              "–ü—Ä–æ–º–ø—Ç–∏ –º–∞—é—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É: —Å–∏—Å—Ç–µ–º–Ω–∏–π –ø—Ä–æ–º–ø—Ç –≤—Å—Ç–∞–Ω–æ–≤–ª—é—î –ø–æ–≤–µ–¥—ñ–Ω–∫—É —Ç–∞ —Ä–æ–ª—å –º–æ–¥–µ–ª—ñ, –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –Ω–∞–¥–∞—é—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –∑–∞–ø–∏—Ç–∏, –∞ —ñ—Å—Ç–æ—Ä—ñ—è —Ä–æ–∑–º–æ–≤–∏ –¥–∞—î –∫–æ–Ω—Ç–µ–∫—Å—Ç. –†–æ–∑—É–º—ñ–Ω–Ω—è —Ü—ñ—î—ó —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ —Ç–∞ –Ω–∞–ø–∏—Å–∞–Ω–Ω—è —á—ñ—Ç–∫–∏—Ö, –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ–π –∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —î –æ—Å–Ω–æ–≤–æ—é \"–ø—Ä–æ–º–ø—Ç-—ñ–Ω–∂–µ–Ω–µ—Ä—ñ—ó.\""
            ]
          },
          "details": {
            "en": [
              {
                "text": "What Is a Prompt",
                "desc": "Your text instructions to an AI model ‚Äî the primary interface for communicating intent. The quality of your prompt directly determines the quality of the output.",
                "links": [
                  {
                    "title": "Generative AI",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "System Prompts",
                "desc": "Hidden instructions that set model personality, rules, and behavior boundaries. They persist across the entire conversation and define the model's \"character.\"",
                "links": []
              },
              {
                "text": "User Prompts",
                "desc": "Specific requests and questions within a conversation. Each user message is a prompt that builds on the conversation history and system prompt context.",
                "links": [
                  {
                    "title": "Context",
                    "href": "context.html"
                  }
                ]
              },
              {
                "text": "Prompt Structure",
                "desc": "Effective prompts have structure: instruction (what to do) + context (background info) + examples (desired format) + constraints (rules/limits) + output format (JSON, markdown, etc.).",
                "links": [
                  {
                    "title": "Prompting Techniques",
                    "href": "../level-4/prompting-techniques.html"
                  }
                ]
              },
              {
                "text": "Writing Clear Prompts",
                "desc": "Be specific (\"summarize in 3 bullet points\" not \"summarize\"), provide relevant context, state format expectations explicitly. Clarity beats cleverness every time.",
                "links": []
              },
              {
                "text": "Role Prompting",
                "desc": "\"You are an expert in...\" activates domain-specific knowledge and communication style. Role prompts help models adopt appropriate tone, vocabulary, and depth for the task.",
                "links": []
              },
              {
                "text": "Few-Shot Prompting",
                "desc": "Providing 2-5 examples of desired input/output pairs in the prompt. Shows the model exactly what format, style, and quality you expect without any fine-tuning.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Common Prompt Templates",
                "desc": "Analysis (break down X), summarization (summarize for Y audience), code generation (write a function that...), creative writing (write in the style of...). Templates save time and improve consistency.",
                "links": []
              },
              {
                "text": "Iterative Refinement",
                "desc": "Improving prompts based on output quality feedback. Start simple, identify what's wrong, add constraints or examples to fix it. Prompt engineering is iterative, not one-shot.",
                "links": []
              },
              {
                "text": "Anti-Patterns & Risks",
                "desc": "Vague instructions (\"make it better\"), contradictory constraints, overly long prompts that dilute focus, and prompt injection risks where malicious text overrides your instructions.",
                "links": [
                  {
                    "title": "Hallucinations",
                    "href": "hallucination.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–©–æ —Ç–∞–∫–µ –ø—Ä–æ–º–ø—Ç",
                "desc": "–í–∞—à—ñ —Ç–µ–∫—Å—Ç–æ–≤—ñ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó –º–æ–¥–µ–ª—ñ –®–Ü ‚Äî –æ—Å–Ω–æ–≤–Ω–∏–π —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–µ—Ä–µ–¥–∞—á—ñ –Ω–∞–º—ñ—Ä—É. –Ø–∫—ñ—Å—Ç—å –ø—Ä–æ–º–ø—Ç—É –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ –≤–∏–∑–Ω–∞—á–∞—î —è–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É.",
                "links": [
                  {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "–°–∏—Å—Ç–µ–º–Ω—ñ –ø—Ä–æ–º–ø—Ç–∏",
                "desc": "–ü—Ä–∏—Ö–æ–≤–∞–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó, —â–æ –≤—Å—Ç–∞–Ω–æ–≤–ª—é—é—Ç—å –æ—Å–æ–±–∏—Å—Ç—ñ—Å—Ç—å, –ø—Ä–∞–≤–∏–ª–∞ —Ç–∞ –º–µ–∂—ñ –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –º–æ–¥–µ–ª—ñ. –í–æ–Ω–∏ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –ø—Ä–æ—Ç—è–≥–æ–º —É—Å—ñ—î—ó —Ä–æ–∑–º–æ–≤–∏ —ñ –≤–∏–∑–Ω–∞—á–∞—é—Ç—å \"—Ö–∞—Ä–∞–∫—Ç–µ—Ä\" –º–æ–¥–µ–ª—ñ.",
                "links": []
              },
              {
                "text": "–ö–æ—Ä–∏—Å—Ç—É–≤–∞—Ü—å–∫—ñ –ø—Ä–æ–º–ø—Ç–∏",
                "desc": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –∑–∞–ø–∏—Ç–∏ —Ç–∞ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è –≤ —Ä–∞–º–∫–∞—Ö —Ä–æ–∑–º–æ–≤–∏. –ö–æ–∂–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ ‚Äî —Ü–µ –ø—Ä–æ–º–ø—Ç, —â–æ –±—É–¥—É—î—Ç—å—Å—è –Ω–∞ —ñ—Å—Ç–æ—Ä—ñ—ó —Ä–æ–∑–º–æ–≤–∏ —Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç—É.",
                "links": [
                  {
                    "title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç",
                    "href": "context.html"
                  }
                ]
              },
              {
                "text": "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–º–ø—Ç—É",
                "desc": "–ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ –ø—Ä–æ–º–ø—Ç–∏ –º–∞—é—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É: —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è (—â–æ —Ä–æ–±–∏—Ç–∏) + –∫–æ–Ω—Ç–µ–∫—Å—Ç (—Ñ–æ–Ω–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è) + –ø—Ä–∏–∫–ª–∞–¥–∏ (–±–∞–∂–∞–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç) + –æ–±–º–µ–∂–µ–Ω–Ω—è (–ø—Ä–∞–≤–∏–ª–∞/–ª—ñ–º—ñ—Ç–∏) + —Ñ–æ—Ä–º–∞—Ç –≤–∏—Ö–æ–¥—É (JSON, markdown —Ç–æ—â–æ).",
                "links": [
                  {
                    "title": "–¢–µ—Ö–Ω—ñ–∫–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É",
                    "href": "../level-4/prompting-techniques.html"
                  }
                ]
              },
              {
                "text": "–ù–∞–ø–∏—Å–∞–Ω–Ω—è —á—ñ—Ç–∫–∏—Ö –ø—Ä–æ–º–ø—Ç—ñ–≤",
                "desc": "–ë—É–¥—å—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–º–∏ (\"—Ä–µ–∑—é–º—É–π —É 3 –ø—É–Ω–∫—Ç–∞—Ö\" –∞ –Ω–µ \"—Ä–µ–∑—é–º—É–π\"), –Ω–∞–¥–∞–≤–∞–π—Ç–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —è–≤–Ω–æ –≤–∫–∞–∑—É–π—Ç–µ –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è —Ñ–æ—Ä–º–∞—Ç—É. –ß—ñ—Ç–∫—ñ—Å—Ç—å –ø–µ—Ä–µ–º–∞–≥–∞—î —Ö–∏—Ç—Ä—ñ—Å—Ç—å –∫–æ–∂–Ω–æ–≥–æ —Ä–∞–∑—É.",
                "links": []
              },
              {
                "text": "–†–æ–ª—å–æ–≤–∏–π –ø—Ä–æ–º–ø—Ç–∏–Ω–≥",
                "desc": "\"–í–∏ —î –µ–∫—Å–ø–µ—Ä—Ç–æ–º —É...\" –∞–∫—Ç–∏–≤—É—î –¥–æ–º–µ–Ω–Ω–æ-—Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –∑–Ω–∞–Ω–Ω—è —Ç–∞ —Å—Ç–∏–ª—å –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—ó. –†–æ–ª—å–æ–≤—ñ –ø—Ä–æ–º–ø—Ç–∏ –¥–æ–ø–æ–º–∞–≥–∞—é—Ç—å –º–æ–¥–µ–ª—è–º –ø—Ä–∏–π–Ω—è—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π —Ç–æ–Ω —Ç–∞ –≥–ª–∏–±–∏–Ω—É.",
                "links": []
              },
              {
                "text": "–ü—Ä–æ–º–ø—Ç–∏–Ω–≥ –∑ –∫—ñ–ª—å–∫–æ–º–∞ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏",
                "desc": "–ù–∞–¥–∞–Ω–Ω—è 2-5 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –±–∞–∂–∞–Ω–∏—Ö –ø–∞—Ä –≤—Ö—ñ–¥/–≤–∏—Ö—ñ–¥ —É –ø—Ä–æ–º–ø—Ç—ñ. –ü–æ–∫–∞–∑—É—î –º–æ–¥–µ–ª—ñ —Ç–æ—á–Ω–æ —è–∫–∏–π —Ñ–æ—Ä–º–∞—Ç, —Å—Ç–∏–ª—å —Ç–∞ —è–∫—ñ—Å—Ç—å –≤–∏ –æ—á—ñ–∫—É—î—Ç–µ –±–µ–∑ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥—É.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "–ü–æ—à–∏—Ä–µ–Ω—ñ —à–∞–±–ª–æ–Ω–∏ –ø—Ä–æ–º–ø—Ç—ñ–≤",
                "desc": "–ê–Ω–∞–ª—ñ–∑ (—Ä–æ–∑–±–µ—Ä–∏ X), —Ä–µ–∑—é–º—É–≤–∞–Ω–Ω—è (—Ä–µ–∑—é–º—É–π –¥–ª—è Y –∞—É–¥–∏—Ç–æ—Ä—ñ—ó), –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–¥—É (–Ω–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü—ñ—é —â–æ...), —Ç–≤–æ—Ä—á–µ –Ω–∞–ø–∏—Å–∞–Ω–Ω—è (–Ω–∞–ø–∏—à–∏ —É —Å—Ç–∏–ª—ñ...). –®–∞–±–ª–æ–Ω–∏ –µ–∫–æ–Ω–æ–º–ª—è—Ç—å —á–∞—Å —Ç–∞ –ø–æ–∫—Ä–∞—â—É—é—Ç—å —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å.",
                "links": []
              },
              {
                "text": "–Ü—Ç–µ—Ä–∞—Ç–∏–≤–Ω–µ –≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è",
                "desc": "–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –∑–≤'—è–∑–∫—É –ø—Ä–æ —è–∫—ñ—Å—Ç—å. –ü–æ—á–∏–Ω–∞–π—Ç–µ –ø—Ä–æ—Å—Ç–æ, –≤–∏–∑–Ω–∞—á—Ç–µ —â–æ –Ω–µ —Ç–∞–∫, –¥–æ–¥–∞–π—Ç–µ –æ–±–º–µ–∂–µ–Ω–Ω—è —á–∏ –ø—Ä–∏–∫–ª–∞–¥–∏. –ü—Ä–æ–º–ø—Ç-—ñ–Ω–∂–µ–Ω–µ—Ä—ñ—è —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞, –Ω–µ –æ–¥–Ω–æ—Ä–∞–∑–æ–≤–∞.",
                "links": []
              },
              {
                "text": "–ê–Ω—Ç–∏–ø–∞—Ç–µ—Ä–Ω–∏ —Ç–∞ —Ä–∏–∑–∏–∫–∏",
                "desc": "–†–æ–∑–º–∏—Ç—ñ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó (\"–∑—Ä–æ–±–∏ –∫—Ä–∞—â–µ\"), —Å—É–ø–µ—Ä–µ—á–ª–∏–≤—ñ –æ–±–º–µ–∂–µ–Ω–Ω—è, –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥—ñ –ø—Ä–æ–º–ø—Ç–∏ —â–æ —Ä–æ–∑–ø–æ—Ä–æ—à—É—é—Ç—å —Ñ–æ–∫—É—Å, —Ç–∞ —Ä–∏–∑–∏–∫–∏ –ø—Ä–æ–º–ø—Ç-—ñ–Ω'—î–∫—Ü—ñ–π –¥–µ –∑–ª–æ–≤–º–∏—Å–Ω–∏–π —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–≤–∏–∑–Ω–∞—á–∞—î –≤–∞—à—ñ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó.",
                "links": [
                  {
                    "title": "–ì–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó",
                    "href": "hallucination.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "System Prompt",
                "def": "Hidden instructions that set model behavior, personality, and rules for the conversation."
              },
              {
                "term": "Few-Shot",
                "def": "Providing examples in the prompt to show the model what output format/style you want."
              },
              {
                "term": "Prompt Engineering",
                "def": "The practice of crafting effective prompts to get desired results from AI models."
              },
              {
                "term": "Prompt Injection",
                "def": "An attack where malicious instructions are embedded to override the system prompt."
              }
            ],
            "uk": [
              {
                "term": "–°–∏—Å—Ç–µ–º–Ω–∏–π –ø—Ä–æ–º–ø—Ç",
                "def": "–ü—Ä–∏—Ö–æ–≤–∞–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó, —â–æ –≤—Å—Ç–∞–Ω–æ–≤–ª—é—é—Ç—å –ø–æ–≤–µ–¥—ñ–Ω–∫—É, –æ—Å–æ–±–∏—Å—Ç—ñ—Å—Ç—å —Ç–∞ –ø—Ä–∞–≤–∏–ª–∞ –º–æ–¥–µ–ª—ñ –¥–ª—è —Ä–æ–∑–º–æ–≤–∏."
              },
              {
                "term": "Few-Shot",
                "def": "–ù–∞–¥–∞–Ω–Ω—è –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —É –ø—Ä–æ–º–ø—Ç—ñ –¥–ª—è –ø–æ–∫–∞–∑—É –º–æ–¥–µ–ª—ñ –±–∞–∂–∞–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—É/—Å—Ç–∏–ª—é –≤–∏—Ö–æ–¥—É."
              },
              {
                "term": "–ü—Ä–æ–º–ø—Ç-—ñ–Ω–∂–µ–Ω–µ—Ä—ñ—è",
                "def": "–ü—Ä–∞–∫—Ç–∏–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏—Ö –ø—Ä–æ–º–ø—Ç—ñ–≤ –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –±–∞–∂–∞–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –≤—ñ–¥ –º–æ–¥–µ–ª–µ–π –®–Ü."
              },
              {
                "term": "–ü—Ä–æ–º–ø—Ç-—ñ–Ω'—î–∫—Ü—ñ—è",
                "def": "–ê—Ç–∞–∫–∞, –¥–µ –∑–ª–æ–≤–º–∏—Å–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó –≤–±—É–¥–æ–≤—É—é—Ç—å—Å—è –¥–ª—è –ø–µ—Ä–µ–≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç—É."
              }
            ]
          },
          "tips": {
            "en": [
              "Start with a clear role and task description, then add constraints and examples",
              "When output isn't right, refine the prompt incrementally rather than rewriting from scratch",
              "Specifying the output format explicitly (JSON, markdown, bullet points) dramatically improves results"
            ],
            "uk": [
              "–ü–æ—á–∏–Ω–∞–π—Ç–µ –∑ —á—ñ—Ç–∫–æ–≥–æ –æ–ø–∏—Å—É —Ä–æ–ª—ñ —Ç–∞ –∑–∞–¥–∞—á—ñ, –ø–æ—Ç—ñ–º –¥–æ–¥–∞–≤–∞–π—Ç–µ –æ–±–º–µ–∂–µ–Ω–Ω—è —Ç–∞ –ø—Ä–∏–∫–ª–∞–¥–∏",
              "–ö–æ–ª–∏ –≤–∏—Ö—ñ–¥ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π, –≤–¥–æ—Å–∫–æ–Ω–∞–ª—é–π—Ç–µ –ø—Ä–æ–º–ø—Ç —ñ–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ, –∞ –Ω–µ –ø–µ—Ä–µ–ø–∏—Å—É–π—Ç–µ –∑ –Ω—É–ª—è",
              "–Ø–≤–Ω–µ –≤–∫–∞–∑—É–≤–∞–Ω–Ω—è —Ñ–æ—Ä–º–∞—Ç—É –≤–∏—Ö–æ–¥—É (JSON, markdown, –º–∞—Ä–∫–µ—Ä–∏) –¥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø–æ–∫—Ä–∞—â—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏"
            ]
          },
          "related": [
            "Feed",
            "Agents & Tools"
          ]
        },
        {
          "slug": "token",
          "title": {
            "en": "Token",
            "uk": "–¢–æ–∫–µ–Ω"
          },
          "desc": {
            "en": "How models process text through tokenization - the fundamental unit of LLM computation.",
            "uk": "–Ø–∫ –º–æ–¥–µ–ª—ñ –æ–±—Ä–æ–±–ª—è—é—Ç—å —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—é ‚Äî –±–∞–∑–æ–≤—É –æ–¥–∏–Ω–∏—Ü—é –æ–±—á–∏—Å–ª–µ–Ω—å LLM."
          },
          "overview": {
            "en": [
              "Tokens are the fundamental units that LLMs work with. They are not characters, not words, but subword pieces ‚Äî typically 3-4 characters of English text. The word \"tokenization\" becomes roughly [\"token\", \"ization\"]. Understanding tokens is critical because they determine costs, context window limits, and model behavior.",
              "Every interaction with an AI model involves counting tokens: your input is measured in tokens, the model's output is counted in tokens, and you pay per token. The context window ‚Äî how much text the model can \"see\" at once ‚Äî is measured in tokens. A typical page of English text is about 500 tokens."
            ],
            "uk": [
              "–¢–æ–∫–µ–Ω–∏ ‚Äî —Ü–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –æ–¥–∏–Ω–∏—Ü—ñ, –∑ —è–∫–∏–º–∏ –ø—Ä–∞—Ü—é—é—Ç—å LLM. –í–æ–Ω–∏ –Ω–µ —Å–∏–º–≤–æ–ª–∏, –Ω–µ —Å–ª–æ–≤–∞, –∞ –ø—ñ–¥—Å–ª—ñ–≤–Ω—ñ —á–∞—Å—Ç–∏–Ω–∏ ‚Äî –∑–∞–∑–≤–∏—á–∞–π 3-4 —Å–∏–º–≤–æ–ª–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç—É. –°–ª–æ–≤–æ \"tokenization\" —Å—Ç–∞—î –ø—Ä–∏–±–ª–∏–∑–Ω–æ [\"token\", \"ization\"]. –†–æ–∑—É–º—ñ–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–≤ –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–µ, –±–æ –≤–æ–Ω–∏ –≤–∏–∑–Ω–∞—á–∞—é—Ç—å –≤–∏—Ç—Ä–∞—Ç–∏, –ª—ñ–º—ñ—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –≤—ñ–∫–Ω–∞ —Ç–∞ –ø–æ–≤–µ–¥—ñ–Ω–∫—É –º–æ–¥–µ–ª—ñ.",
              "–ö–æ–∂–Ω–∞ –≤–∑–∞—î–º–æ–¥—ñ—è –∑ –º–æ–¥–µ–ª–ª—é –®–Ü –≤–∫–ª—é—á–∞—î –ø—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Ç–æ–∫–µ–Ω—ñ–≤: –≤–∞—à –≤—Ö—ñ–¥ –≤–∏–º—ñ—Ä—é—î—Ç—å—Å—è –≤ —Ç–æ–∫–µ–Ω–∞—Ö, –≤–∏—Ö—ñ–¥ –º–æ–¥–µ–ª—ñ –ø—ñ–¥—Ä–∞—Ö–æ–≤—É—î—Ç—å—Å—è –≤ —Ç–æ–∫–µ–Ω–∞—Ö, —ñ –≤–∏ –ø–ª–∞—Ç–∏—Ç–µ –∑–∞ —Ç–æ–∫–µ–Ω. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–µ –≤—ñ–∫–Ω–æ ‚Äî —Å–∫—ñ–ª—å–∫–∏ —Ç–µ–∫—Å—Ç—É –º–æ–¥–µ–ª—å –º–æ–∂–µ \"–±–∞—á–∏—Ç–∏\" –æ–¥–Ω–æ—á–∞—Å–Ω–æ ‚Äî –≤–∏–º—ñ—Ä—é—î—Ç—å—Å—è –≤ —Ç–æ–∫–µ–Ω–∞—Ö. –¢–∏–ø–æ–≤–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç—É ‚Äî —Ü–µ –ø—Ä–∏–±–ª–∏–∑–Ω–æ 500 —Ç–æ–∫–µ–Ω—ñ–≤."
            ]
          },
          "details": {
            "en": [
              {
                "text": "What Is a Token",
                "desc": "Subword units ‚Äî not characters or words. \"Hello world\" is 2 tokens. \"Tokenization\" becomes [\"token\", \"ization\"]. Typically 3-4 characters of English text per token.",
                "links": []
              },
              {
                "text": "Tokenization Algorithms",
                "desc": "BPE (Byte Pair Encoding) iteratively merges frequent character pairs. SentencePiece handles any language. tiktoken is OpenAI's fast tokenizer. Each model family uses its own tokenizer.",
                "links": []
              },
              {
                "text": "Context Window Sizes",
                "desc": "4K tokens (early GPT-3.5) ‚Üí 128K (GPT-4) ‚Üí 200K (Claude) ‚Üí 1M+ (Gemini). Context windows have grown 250x in just 2 years, dramatically expanding what models can process.",
                "links": [
                  {
                    "title": "Context",
                    "href": "context.html"
                  }
                ]
              },
              {
                "text": "Token Pricing",
                "desc": "Typical costs: $1-30 per million tokens depending on model tier. Claude Haiku ~$0.25/M input, GPT-4o ~$2.50/M input, Claude Opus ~$15/M input. Understanding pricing enables cost optimization.",
                "links": [
                  {
                    "title": "API Providers",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              },
              {
                "text": "Language Differences",
                "desc": "Ukrainian, Chinese, Arabic, and other non-Latin scripts use 2-3x more tokens than English for equivalent content. This directly impacts costs and effective context window size.",
                "links": []
              },
              {
                "text": "Special Tokens",
                "desc": "Control tokens like <|im_start|>, <|im_end|>, [PAD], [SEP] are used internally by models to mark message boundaries, roles, and sequence structure. You rarely see them but they consume context.",
                "links": []
              },
              {
                "text": "Token Counting Tools",
                "desc": "tiktoken (OpenAI), Anthropic tokenizer, Hugging Face tokenizers ‚Äî use these to predict costs and check if your prompt fits within the context window before sending.",
                "links": []
              },
              {
                "text": "Cost Optimization",
                "desc": "Shorter prompts = cheaper, but too short = worse quality. The art is finding the minimum effective prompt length. Removing unnecessary context and boilerplate saves money at scale.",
                "links": [
                  {
                    "title": "Prompt",
                    "href": "prompt.html"
                  }
                ]
              },
              {
                "text": "Prompt Caching",
                "desc": "Many APIs cache common prompt prefixes to reduce costs on repeated calls. Anthropic and OpenAI both offer caching that can reduce input costs by 90% for repeated system prompts.",
                "links": [
                  {
                    "title": "API Providers",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              },
              {
                "text": "Input vs Output Token Pricing",
                "desc": "Output tokens are typically 2-5x more expensive than input tokens. Generating text costs more than reading it. This incentivizes concise outputs and affects application design decisions.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "–©–æ —Ç–∞–∫–µ —Ç–æ–∫–µ–Ω",
                "desc": "–ü—ñ–¥—Å–ª—ñ–≤–Ω—ñ –æ–¥–∏–Ω–∏—Ü—ñ ‚Äî –Ω–µ —Å–∏–º–≤–æ–ª–∏ —á–∏ —Å–ª–æ–≤–∞. \"Hello world\" —Ü–µ 2 —Ç–æ–∫–µ–Ω–∏. \"Tokenization\" —Å—Ç–∞—î [\"token\", \"ization\"]. –ó–∞–∑–≤–∏—á–∞–π 3-4 —Å–∏–º–≤–æ–ª–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç—É –Ω–∞ —Ç–æ–∫–µ–Ω.",
                "links": []
              },
              {
                "text": "–ê–ª–≥–æ—Ä–∏—Ç–º–∏ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—ó",
                "desc": "BPE (Byte Pair Encoding) —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –∑–ª–∏–≤–∞—î —á–∞—Å—Ç—ñ –ø–∞—Ä–∏ —Å–∏–º–≤–æ–ª—ñ–≤. SentencePiece –ø—Ä–∞—Ü—é—î –∑ –±—É–¥—å-—è–∫–æ—é –º–æ–≤–æ—é. tiktoken ‚Äî —à–≤–∏–¥–∫–∏–π —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä OpenAI. –ö–æ–∂–Ω–∞ —Å—ñ–º'—è –º–æ–¥–µ–ª–µ–π –º–∞—î —Å–≤—ñ–π —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä.",
                "links": []
              },
              {
                "text": "–†–æ–∑–º—ñ—Ä–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏—Ö –≤—ñ–∫–æ–Ω",
                "desc": "4K —Ç–æ–∫–µ–Ω—ñ–≤ (—Ä–∞–Ω–Ω—ñ–π GPT-3.5) ‚Üí 128K (GPT-4) ‚Üí 200K (Claude) ‚Üí 1M+ (Gemini). –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ñ –≤—ñ–∫–Ω–∞ –∑—Ä–æ—Å–ª–∏ —É 250 —Ä–∞–∑—ñ–≤ –∑–∞ 2 —Ä–æ–∫–∏, —Ä—ñ–∑–∫–æ —Ä–æ–∑—à–∏—Ä—é—é—á–∏ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–±—Ä–æ–±–∫–∏.",
                "links": [
                  {
                    "title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç",
                    "href": "context.html"
                  }
                ]
              },
              {
                "text": "–¶—ñ–Ω–æ—É—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–≤",
                "desc": "–¢–∏–ø–æ–≤—ñ –≤–∏—Ç—Ä–∞—Ç–∏: $1-30 –∑–∞ –º—ñ–ª—å–π–æ–Ω —Ç–æ–∫–µ–Ω—ñ–≤ –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ä—ñ–≤–Ω—è –º–æ–¥–µ–ª—ñ. Claude Haiku ~$0.25/M –≤—Ö—ñ–¥, GPT-4o ~$2.50/M –≤—Ö—ñ–¥, Claude Opus ~$15/M –≤—Ö—ñ–¥. –†–æ–∑—É–º—ñ–Ω–Ω—è —Ü—ñ–Ω –¥–æ–∑–≤–æ–ª—è—î –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—é.",
                "links": [
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              },
              {
                "text": "–ú–æ–≤–Ω—ñ –≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç—ñ",
                "desc": "–£–∫—Ä–∞—ó–Ω—Å—å–∫—ñ, –∫–∏—Ç–∞–π—Å—å–∫—ñ, –∞—Ä–∞–±—Å—å–∫—ñ —Ç–∞ —ñ–Ω—à—ñ –Ω–µ–ª–∞—Ç–∏–Ω—Å—å–∫—ñ —Å–∫—Ä–∏–ø—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —É 2-3 —Ä–∞–∑–∏ –±—ñ–ª—å—à–µ —Ç–æ–∫–µ–Ω—ñ–≤ –∑–∞ –µ–∫–≤—ñ–≤–∞–ª–µ–Ω—Ç–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç. –¶–µ –ø—Ä—è–º–æ –≤–ø–ª–∏–≤–∞—î –Ω–∞ –≤–∏—Ç—Ä–∞—Ç–∏ —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.",
                "links": []
              },
              {
                "text": "–°–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Ç–æ–∫–µ–Ω–∏",
                "desc": "–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ñ —Ç–æ–∫–µ–Ω–∏ —è–∫ <|im_start|>, <|im_end|>, [PAD], [SEP] –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –≤–Ω—É—Ç—Ä—ñ—à–Ω—å–æ –¥–ª—è —Ä–æ–∑–º—ñ—Ç–∫–∏ –º–µ–∂ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å, —Ä–æ–ª–µ–π —Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏. –í–∏ —ó—Ö —Ä—ñ–¥–∫–æ –±–∞—á–∏—Ç–µ, –∞–ª–µ –≤–æ–Ω–∏ —Å–ø–æ–∂–∏–≤–∞—é—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç.",
                "links": []
              },
              {
                "text": "–Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É —Ç–æ–∫–µ–Ω—ñ–≤",
                "desc": "tiktoken (OpenAI), Anthropic tokenizer, Hugging Face tokenizers ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ —ó—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É –≤–∏—Ç—Ä–∞—Ç —Ç–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —á–∏ –ø—Ä–æ–º–ø—Ç –≤–º—ñ—â—É—î—Ç—å—Å—è —É –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–µ –≤—ñ–∫–Ω–æ.",
                "links": []
              },
              {
                "text": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –≤–∏—Ç—Ä–∞—Ç",
                "desc": "–ö–æ—Ä–æ—Ç—à—ñ –ø—Ä–æ–º–ø—Ç–∏ = –¥–µ—à–µ–≤—à–µ, –∞–ª–µ –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–æ = –≥—ñ—Ä—à–∞ —è–∫—ñ—Å—Ç—å. –ú–∏—Å—Ç–µ—Ü—Ç–≤–æ ‚Äî –∑–Ω–∞–π—Ç–∏ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—É –µ—Ñ–µ–∫—Ç–∏–≤–Ω—É –¥–æ–≤–∂–∏–Ω—É –ø—Ä–æ–º–ø—Ç—É. –í–∏–¥–∞–ª–µ–Ω–Ω—è –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –µ–∫–æ–Ω–æ–º–∏—Ç—å –≥—Ä–æ—à—ñ.",
                "links": [
                  {
                    "title": "–ü—Ä–æ–º–ø—Ç",
                    "href": "prompt.html"
                  }
                ]
              },
              {
                "text": "–ö–µ—à—É–≤–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤",
                "desc": "–ë–∞–≥–∞—Ç–æ API –∫–µ—à—É—é—Ç—å —Å–ø—ñ–ª—å–Ω—ñ –ø—Ä–µ—Ñ—ñ–∫—Å–∏ –ø—Ä–æ–º–ø—Ç—ñ–≤. Anthropic —Ç–∞ OpenAI –ø—Ä–æ–ø–æ–Ω—É—é—Ç—å –∫–µ—à—É–≤–∞–Ω–Ω—è, —â–æ –º–æ–∂–µ –∑–º–µ–Ω—à–∏—Ç–∏ –≤–∏—Ç—Ä–∞—Ç–∏ –Ω–∞ –≤—Ö—ñ–¥ –Ω–∞ 90% –¥–ª—è –ø–æ–≤—Ç–æ—Ä—é–≤–∞–Ω–∏—Ö —Å–∏—Å—Ç–µ–º–Ω–∏—Ö –ø—Ä–æ–º–ø—Ç—ñ–≤.",
                "links": [
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              },
              {
                "text": "–¶—ñ–Ω–∏ –≤—Ö—ñ–¥–Ω–∏—Ö vs –≤–∏—Ö—ñ–¥–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤",
                "desc": "–í–∏—Ö—ñ–¥–Ω—ñ —Ç–æ–∫–µ–Ω–∏ –∑–∞–∑–≤–∏—á–∞–π —É 2-5 —Ä–∞–∑—ñ–≤ –¥–æ—Ä–æ–∂—á—ñ –∑–∞ –≤—Ö—ñ–¥–Ω—ñ. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É –∫–æ—à—Ç—É—î –±—ñ–ª—å—à–µ –Ω—ñ–∂ —á–∏—Ç–∞–Ω–Ω—è. –¶–µ —Å—Ç–∏–º—É–ª—é—î —Å—Ç–∏—Å–ª—ñ –≤–∏—Ö–æ–¥–∏ —Ç–∞ –≤–ø–ª–∏–≤–∞—î –Ω–∞ –ø—Ä–æ—î–∫—Ç—É–≤–∞–Ω–Ω—è –¥–æ–¥–∞—Ç–∫—ñ–≤.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Token",
                "def": "The basic unit of text that LLMs process ‚Äî a subword piece typically 3-4 characters long."
              },
              {
                "term": "BPE",
                "def": "Byte Pair Encoding ‚Äî a tokenization algorithm that iteratively merges the most frequent character pairs."
              },
              {
                "term": "Context Window",
                "def": "The maximum number of tokens a model can process in a single request (input + output combined)."
              },
              {
                "term": "Prompt Caching",
                "def": "API feature that caches common prompt prefixes to reduce cost on repeated similar requests."
              }
            ],
            "uk": [
              {
                "term": "–¢–æ–∫–µ–Ω",
                "def": "–ë–∞–∑–æ–≤–∞ –æ–¥–∏–Ω–∏—Ü—è —Ç–µ–∫—Å—Ç—É, —è–∫—É –æ–±—Ä–æ–±–ª—è—é—Ç—å LLM ‚Äî –ø—ñ–¥—Å–ª—ñ–≤–Ω–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∑–∞–∑–≤–∏—á–∞–π 3-4 —Å–∏–º–≤–æ–ª–∏."
              },
              {
                "term": "BPE",
                "def": "Byte Pair Encoding ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—ó, —â–æ —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –∑–ª–∏–≤–∞—î –Ω–∞–π—á–∞—Å—Ç—ñ—à—ñ –ø–∞—Ä–∏ —Å–∏–º–≤–æ–ª—ñ–≤."
              },
              {
                "term": "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–µ –≤—ñ–∫–Ω–æ",
                "def": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤, —è–∫—É –º–æ–¥–µ–ª—å –º–æ–∂–µ –æ–±—Ä–æ–±–∏—Ç–∏ –≤ –æ–¥–Ω–æ–º—É –∑–∞–ø–∏—Ç—ñ (–≤—Ö—ñ–¥ + –≤–∏—Ö—ñ–¥ —Ä–∞–∑–æ–º)."
              },
              {
                "term": "–ö–µ—à—É–≤–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤",
                "def": "–§—É–Ω–∫—Ü—ñ—è API, —â–æ –∫–µ—à—É—î —Å–ø—ñ–ª—å–Ω—ñ –ø—Ä–µ—Ñ—ñ–∫—Å–∏ –ø—Ä–æ–º–ø—Ç—ñ–≤ –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è –≤–∏—Ç—Ä–∞—Ç –Ω–∞ –ø–æ–≤—Ç–æ—Ä–Ω—ñ –∑–∞–ø–∏—Ç–∏."
              }
            ]
          },
          "tips": {
            "en": [
              "Estimate token counts before making API calls: 1 token is roughly 4 English characters or 0.75 words ‚Äî a page of text is about 500 tokens",
              "Non-Latin scripts (Ukrainian, Chinese, Arabic) use more tokens per word ‚Äî budget 2-3x more tokens for multilingual applications",
              "To reduce costs, compress system prompts and reuse cached prefixes ‚Äî Anthropic and OpenAI both support prompt caching for significant savings"
            ],
            "uk": [
              "–û—Ü—ñ–Ω—é–π—Ç–µ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ –ø–µ—Ä–µ–¥ API-–≤–∏–∫–ª–∏–∫–∞–º–∏: 1 —Ç–æ–∫–µ–Ω ‚Äî —Ü–µ –ø—Ä–∏–±–ª–∏–∑–Ω–æ 4 –∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ —Å–∏–º–≤–æ–ª–∏ –∞–±–æ 0.75 —Å–ª–æ–≤–∞ ‚Äî —Å—Ç–æ—Ä—ñ–Ω–∫–∞ —Ç–µ–∫—Å—Ç—É —Ü–µ ~500 —Ç–æ–∫–µ–Ω—ñ–≤",
              "–ù–µ–ª–∞—Ç–∏–Ω—Å—å–∫—ñ —Å–∫—Ä–∏–ø—Ç–∏ (—É–∫—Ä–∞—ó–Ω—Å—å–∫–∞, –∫–∏—Ç–∞–π—Å—å–∫–∞, –∞—Ä–∞–±—Å—å–∫–∞) –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –±—ñ–ª—å—à–µ —Ç–æ–∫–µ–Ω—ñ–≤ –Ω–∞ —Å–ª–æ–≤–æ ‚Äî –∑–∞–∫–ª–∞–¥–∞–π—Ç–µ 2-3x –±—ñ–ª—å—à–µ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–≤–Ω–∏—Ö –¥–æ–¥–∞—Ç–∫—ñ–≤",
              "–î–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è –≤–∏—Ç—Ä–∞—Ç —Å—Ç–∏—Å–∫–∞–π—Ç–µ —Å–∏—Å—Ç–µ–º–Ω—ñ –ø—Ä–æ–º–ø—Ç–∏ —Ç–∞ –ø–æ–≤—Ç–æ—Ä–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –∫–µ—à–æ–≤–∞–Ω—ñ –ø—Ä–µ—Ñ—ñ–∫—Å–∏ ‚Äî Anthropic —ñ OpenAI –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å –∫–µ—à—É–≤–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤"
            ]
          },
          "related": [
            "Agents & Tools"
          ]
        },
        {
          "slug": "context",
          "title": {
            "en": "Context",
            "uk": "–ö–æ–Ω—Ç–µ–∫—Å—Ç"
          },
          "desc": {
            "en": "Context windows, how models use context, and managing context effectively.",
            "uk": "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ñ –≤—ñ–∫–Ω–∞, —è–∫ –º–æ–¥–µ–ª—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–µ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."
          },
          "overview": {
            "en": [
              "The context window is the total amount of text (measured in tokens) that a model can process in a single request ‚Äî including both your input and the model's output. Think of it as the model's \"working memory.\" Anything outside the context window simply doesn't exist for the model.",
              "Context windows have grown dramatically: from 4K tokens in early GPT-3.5 to 200K (Claude) and 1M+ (Gemini). But bigger isn't always better ‚Äî models often struggle to effectively use information in the middle of very long contexts. Understanding these dynamics is key to building effective AI applications."
            ],
            "uk": [
              "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–µ –≤—ñ–∫–Ω–æ ‚Äî —Ü–µ –∑–∞–≥–∞–ª—å–Ω–∏–π –æ–±—Å—è–≥ —Ç–µ–∫—Å—Ç—É (–≤–∏–º—ñ—Ä—è–Ω–∏–π —É —Ç–æ–∫–µ–Ω–∞—Ö), —è–∫–∏–π –º–æ–¥–µ–ª—å –º–æ–∂–µ –æ–±—Ä–æ–±–∏—Ç–∏ –≤ –æ–¥–Ω–æ–º—É –∑–∞–ø–∏—Ç—ñ ‚Äî –≤–∫–ª—é—á–∞—é—á–∏ —è–∫ –≤–∞—à –≤—Ö—ñ–¥, —Ç–∞–∫ —ñ –≤–∏—Ö—ñ–¥ –º–æ–¥–µ–ª—ñ. –î—É–º–∞–π—Ç–µ –ø—Ä–æ —Ü–µ —è–∫ –ø—Ä–æ \"—Ä–æ–±–æ—á—É –ø–∞–º'—è—Ç—å\" –º–æ–¥–µ–ª—ñ. –í—Å–µ, —â–æ –ø–æ–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏–º –≤—ñ–∫–Ω–æ–º, –ø—Ä–æ—Å—Ç–æ –Ω–µ —ñ—Å–Ω—É—î –¥–ª—è –º–æ–¥–µ–ª—ñ.",
              "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ñ –≤—ñ–∫–Ω–∞ –¥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –∑—Ä–æ—Å–ª–∏: –≤—ñ–¥ 4K —Ç–æ–∫–µ–Ω—ñ–≤ —É —Ä–∞–Ω–Ω—å–æ–º—É GPT-3.5 –¥–æ 200K (Claude) —Ç–∞ 1M+ (Gemini). –ê–ª–µ –±—ñ–ª—å—à–µ ‚Äî –Ω–µ –∑–∞–≤–∂–¥–∏ –∫—Ä–∞—â–µ ‚Äî –º–æ–¥–µ–ª—ñ —á–∞—Å—Ç–æ –º–∞—é—Ç—å –ø—Ä–æ–±–ª–µ–º–∏ –∑ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏–º –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó —É —Å–µ—Ä–µ–¥–∏–Ω—ñ –¥—É–∂–µ –¥–æ–≤–≥–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ–≤. –†–æ–∑—É–º—ñ–Ω–Ω—è —Ü—ñ—î—ó –¥–∏–Ω–∞–º—ñ–∫–∏ —î –∫–ª—é—á–µ–º –¥–æ –ø–æ–±—É–¥–æ–≤–∏ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏—Ö –®–Ü-–¥–æ–¥–∞—Ç–∫—ñ–≤."
            ]
          },
          "details": {
            "en": [
              {
                "text": "What Is a Context Window",
                "desc": "The total tokens (input + output) a model processes in one request ‚Äî its working memory. Everything outside the context window simply doesn't exist for the model.",
                "links": [
                  {
                    "title": "Token",
                    "href": "token.html"
                  }
                ]
              },
              {
                "text": "Context Window Sizes",
                "desc": "GPT-4 (128K), Claude (200K), Gemini (1M+), open models (8K-128K). Bigger context means more information available, but cost and latency increase with context size.",
                "links": [
                  {
                    "title": "The Big Players",
                    "href": "../level-1/big-players.html"
                  }
                ]
              },
              {
                "text": "How Attention Works",
                "desc": "Each token \"attends\" to every other token ‚Äî computational cost grows quadratically (O(n^2)). This is why very long contexts are expensive and why efficient attention methods matter.",
                "links": [
                  {
                    "title": "Neural Networks",
                    "href": "../level-3/neural-networks.html"
                  }
                ]
              },
              {
                "text": "Lost-in-the-Middle Problem",
                "desc": "Models attend better to the beginning and end of context than the middle. Important information placed in the middle of a long context may be overlooked or given less weight.",
                "links": []
              },
              {
                "text": "Context Management Strategies",
                "desc": "Summarization (compress older messages), chunking (process documents in pieces), prioritization (put most relevant info first/last). Essential skills for building production AI apps.",
                "links": []
              },
              {
                "text": "RAG (Retrieval-Augmented Generation)",
                "desc": "Pull relevant documents into context on demand rather than stuffing everything in. A search retrieves the most relevant chunks, which are then added to the prompt before generation.",
                "links": [
                  {
                    "title": "RAG",
                    "href": "../level-4/rag.html"
                  }
                ]
              },
              {
                "text": "Conversation Memory",
                "desc": "Chatbots simulate long-term memory by managing context: summarizing old messages, maintaining key facts, and selectively including relevant history in each new request.",
                "links": []
              },
              {
                "text": "Context Engineering",
                "desc": "Deliberate structuring of what goes into the context window ‚Äî what to include, what to summarize, what to omit. Arguably more important than prompt engineering for complex applications.",
                "links": [
                  {
                    "title": "Prompt",
                    "href": "prompt.html"
                  }
                ]
              },
              {
                "text": "Sliding Window Processing",
                "desc": "For documents longer than the context window, process in overlapping chunks that \"slide\" through the content. Each chunk shares some overlap with the previous for continuity.",
                "links": []
              },
              {
                "text": "Multi-Turn Conversation Costs",
                "desc": "Each message in a conversation consumes context. As conversations grow, old messages get truncated or summarized. Understanding this helps you design chatbots that remain coherent over time.",
                "links": [
                  {
                    "title": "Token",
                    "href": "token.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–©–æ —Ç–∞–∫–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–µ –≤—ñ–∫–Ω–æ",
                "desc": "–ó–∞–≥–∞–ª—å–Ω—ñ —Ç–æ–∫–µ–Ω–∏ (–≤—Ö—ñ–¥ + –≤–∏—Ö—ñ–¥), —â–æ –º–æ–¥–µ–ª—å –æ–±—Ä–æ–±–ª—è—î –≤ –æ–¥–Ω–æ–º—É –∑–∞–ø–∏—Ç—ñ ‚Äî —ó—ó —Ä–æ–±–æ—á–∞ –ø–∞–º'—è—Ç—å. –í—Å–µ –ø–æ–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏–º –≤—ñ–∫–Ω–æ–º –ø—Ä–æ—Å—Ç–æ –Ω–µ —ñ—Å–Ω—É—î –¥–ª—è –º–æ–¥–µ–ª—ñ.",
                "links": [
                  {
                    "title": "–¢–æ–∫–µ–Ω",
                    "href": "token.html"
                  }
                ]
              },
              {
                "text": "–†–æ–∑–º—ñ—Ä–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏—Ö –≤—ñ–∫–æ–Ω",
                "desc": "GPT-4 (128K), Claude (200K), Gemini (1M+), –≤—ñ–¥–∫—Ä–∏—Ç—ñ –º–æ–¥–µ–ª—ñ (8K-128K). –ë—ñ–ª—å—à–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Äî –±—ñ–ª—å—à–µ –¥–æ—Å—Ç—É–ø–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó, –∞–ª–µ –∑—Ä–æ—Å—Ç–∞—é—Ç—å –≤–∏—Ç—Ä–∞—Ç–∏ —Ç–∞ –ª–∞—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å.",
                "links": [
                  {
                    "title": "–í–µ–ª–∏–∫—ñ –≥—Ä–∞–≤—Ü—ñ",
                    "href": "../level-1/big-players.html"
                  }
                ]
              },
              {
                "text": "–Ø–∫ –ø—Ä–∞—Ü—é—î —É–≤–∞–≥–∞",
                "desc": "–ö–æ–∂–µ–Ω —Ç–æ–∫–µ–Ω \"–∑–≤–µ—Ä—Ç–∞—î —É–≤–∞–≥—É\" –Ω–∞ –∫–æ–∂–µ–Ω —ñ–Ω—à–∏–π ‚Äî –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –∑—Ä–æ—Å—Ç–∞—î –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ (O(n^2)). –¢–æ–º—É –¥—É–∂–µ –¥–æ–≤–≥—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏ –¥–æ—Ä–æ–≥—ñ —ñ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ –º–µ—Ç–æ–¥–∏ —É–≤–∞–≥–∏ –≤–∞–∂–ª–∏–≤—ñ.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂",
                    "href": "../level-3/neural-networks.html"
                  }
                ]
              },
              {
                "text": "–ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–≥—É–±–ª–µ–Ω–æ—Å—Ç—ñ —É —Å–µ—Ä–µ–¥–∏–Ω—ñ",
                "desc": "–ú–æ–¥–µ–ª—ñ –∫—Ä–∞—â–µ –∑–≤–µ—Ä—Ç–∞—é—Ç—å —É–≤–∞–≥—É –Ω–∞ –ø–æ—á–∞—Ç–æ–∫ —Ç–∞ –∫—ñ–Ω–µ—Ü—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –Ω—ñ–∂ –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω—É. –í–∞–∂–ª–∏–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è —É —Å–µ—Ä–µ–¥–∏–Ω—ñ –¥–æ–≤–≥–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –º–æ–∂–µ –±—É—Ç–∏ –ø—Ä–æ–ø—É—â–µ–Ω–∞ –∞–±–æ –æ—Ç—Ä–∏–º–∞—Ç–∏ –º–µ–Ω—à–µ –≤–∞–≥–∏.",
                "links": []
              },
              {
                "text": "–°—Ç—Ä–∞—Ç–µ–≥—ñ—ó —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º",
                "desc": "–†–µ–∑—é–º—É–≤–∞–Ω–Ω—è (—Å—Ç–∏—Å–Ω–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å), —á–∞–Ω–∫—ñ–Ω–≥ (–æ–±—Ä–æ–±–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —á–∞—Å—Ç–∏–Ω–∞–º–∏), –ø—Ä—ñ–æ—Ä–∏—Ç–∏–∑–∞—Ü—ñ—è (–Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ –Ω–∞ –ø–æ—á–∞—Ç–æ–∫/–∫—ñ–Ω–µ—Ü—å). –ù–µ–æ–±—Ö—ñ–¥–Ω—ñ –Ω–∞–≤–∏—á–∫–∏ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω –®–Ü-–¥–æ–¥–∞—Ç–∫—ñ–≤.",
                "links": []
              },
              {
                "text": "RAG (–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ –ø–æ—à—É–∫–æ–≤–∏–º –¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è–º)",
                "desc": "–í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞ –∑–∞–ø–∏—Ç–æ–º –∑–∞–º—ñ—Å—Ç—å –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –≤—Å—å–æ–≥–æ. –ü–æ—à—É–∫ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–∞–π—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—à—ñ —á–∞–Ω–∫–∏, —â–æ –¥–æ–¥–∞—é—Ç—å—Å—è —É –ø—Ä–æ–º–ø—Ç –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—î—é.",
                "links": [
                  {
                    "title": "RAG",
                    "href": "../level-4/rag.html"
                  }
                ]
              },
              {
                "text": "–ü–∞–º'—è—Ç—å —Ä–æ–∑–º–æ–≤–∏",
                "desc": "–ß–∞—Ç–±–æ—Ç–∏ —ñ–º—ñ—Ç—É—é—Ç—å –¥–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤—É –ø–∞–º'—è—Ç—å —á–µ—Ä–µ–∑ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: —Ä–µ–∑—é–º—É–≤–∞–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å, –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö —Ñ–∞–∫—Ç—ñ–≤ —Ç–∞ –≤–∏–±—ñ—Ä–∫–æ–≤–µ –≤–∫–ª—é—á–µ–Ω–Ω—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—ó —ñ—Å—Ç–æ—Ä—ñ—ó.",
                "links": []
              },
              {
                "text": "–ö–æ–Ω—Ç–µ–∫—Å—Ç-—ñ–Ω–∂–µ–Ω–µ—Ä—ñ—è",
                "desc": "–ù–∞–≤–º–∏—Å–Ω–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É–≤–∞–Ω–Ω—è —Ç–æ–≥–æ, —â–æ –ø–æ—Ç—Ä–∞–ø–ª—è—î –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–µ –≤—ñ–∫–Ω–æ ‚Äî —â–æ –≤–∫–ª—é—á–∏—Ç–∏, —â–æ —Ä–µ–∑—é–º—É–≤–∞—Ç–∏, —â–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏. –ú–æ–∂–ª–∏–≤–æ, –≤–∞–∂–ª–∏–≤—ñ—à–µ –∑–∞ –ø—Ä–æ–º–ø—Ç-—ñ–Ω–∂–µ–Ω–µ—Ä—ñ—é –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –¥–æ–¥–∞—Ç–∫—ñ–≤.",
                "links": [
                  {
                    "title": "–ü—Ä–æ–º–ø—Ç",
                    "href": "prompt.html"
                  }
                ]
              },
              {
                "text": "–û–±—Ä–æ–±–∫–∞ –∫–æ–≤–∑–Ω–∏–º –≤—ñ–∫–Ω–æ–º",
                "desc": "–î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –¥–æ–≤—à–∏—Ö –∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–µ –≤—ñ–∫–Ω–æ ‚Äî –æ–±—Ä–æ–±–∫–∞ —á–∞—Å—Ç–∏–Ω–∞–º–∏, —â–æ \"–∫–æ–≤–∑–∞—é—Ç—å\" —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç –∑ –ø–µ—Ä–µ—Ç–∏–Ω–∞–º–∏ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –±–µ–∑–ø–µ—Ä–µ—Ä–≤–Ω–æ—Å—Ç—ñ.",
                "links": []
              },
              {
                "text": "–í–∏—Ç—Ä–∞—Ç–∏ –±–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤–∏—Ö —Ä–æ–∑–º–æ–≤",
                "desc": "–ö–æ–∂–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤ —Ä–æ–∑–º–æ–≤—ñ —Å–ø–æ–∂–∏–≤–∞—î –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ü–æ –º—ñ—Ä—ñ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è —Ä–æ–∑–º–æ–≤–∏ —Å—Ç–∞—Ä—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –æ–±—Ä—ñ–∑–∞—é—Ç—å—Å—è —á–∏ —Ä–µ–∑—é–º—É—é—Ç—å—Å—è. –†–æ–∑—É–º—ñ–Ω–Ω—è —Ü—å–æ–≥–æ –¥–æ–ø–æ–º–∞–≥–∞—î –ø—Ä–æ—î–∫—Ç—É–≤–∞—Ç–∏ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–∏—Ö —á–∞—Ç–±–æ—Ç—ñ–≤.",
                "links": [
                  {
                    "title": "–¢–æ–∫–µ–Ω",
                    "href": "token.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Context Window",
                "def": "Maximum tokens a model processes at once ‚Äî its working memory for a single request."
              },
              {
                "term": "Lost-in-the-Middle",
                "def": "Models attend better to start and end of context, often missing information in the middle."
              },
              {
                "term": "RAG",
                "def": "Retrieval-Augmented Generation ‚Äî dynamically retrieving relevant documents to add to the context."
              },
              {
                "term": "Context Engineering",
                "def": "The practice of deliberately structuring and managing what information enters the model's context."
              }
            ],
            "uk": [
              {
                "term": "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–µ –≤—ñ–∫–Ω–æ",
                "def": "–ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω—ñ–≤, —â–æ –º–æ–¥–µ–ª—å –æ–±—Ä–æ–±–ª—è—î –∑–∞ —Ä–∞–∑ ‚Äî —ó—ó —Ä–æ–±–æ—á–∞ –ø–∞–º'—è—Ç—å –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø–∏—Ç—É."
              },
              {
                "term": "–ó–∞–≥—É–±–ª–µ–Ω—ñ—Å—Ç—å —É —Å–µ—Ä–µ–¥–∏–Ω—ñ",
                "def": "–ú–æ–¥–µ–ª—ñ –∫—Ä–∞—â–µ –∑–≤–µ—Ä—Ç–∞—é—Ç—å —É–≤–∞–≥—É –Ω–∞ –ø–æ—á–∞—Ç–æ–∫ —Ç–∞ –∫—ñ–Ω–µ—Ü—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É, —á–∞—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞—é—á–∏ —Å–µ—Ä–µ–¥–∏–Ω—É."
              },
              {
                "term": "RAG",
                "def": "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ –ø–æ—à—É–∫–æ–≤–∏–º –¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è–º ‚Äî –¥–∏–Ω–∞–º—ñ—á–Ω–µ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç."
              },
              {
                "term": "–ö–æ–Ω—Ç–µ–∫—Å—Ç-—ñ–Ω–∂–µ–Ω–µ—Ä—ñ—è",
                "def": "–ü—Ä–∞–∫—Ç–∏–∫–∞ –Ω–∞–≤–º–∏—Å–Ω–æ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É–≤–∞–Ω–Ω—è —Ç–∞ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é, —â–æ –ø–æ—Ç—Ä–∞–ø–ª—è—î –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª—ñ."
              }
            ]
          },
          "tips": {
            "en": [
              "Put the most important information at the beginning and end of your prompt ‚Äî models pay less attention to the middle (the \"lost in the middle\" effect)",
              "For long-context tasks, chunk your input and summarize irrelevant sections rather than feeding everything in ‚Äî focused context produces better results than exhaustive context",
              "Use RAG instead of stuffing everything into context ‚Äî retrieve only what's relevant for the specific query, even if the model has a huge context window"
            ],
            "uk": [
              "–†–æ–∑–º—ñ—â—É–π—Ç–µ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ç–∞ –≤ –∫—ñ–Ω—Ü—ñ –ø—Ä–æ–º–ø—Ç—É ‚Äî –º–æ–¥–µ–ª—ñ –º–µ–Ω—à–µ —É–≤–∞–≥–∏ –ø—Ä–∏–¥—ñ–ª—è—é—Ç—å —Å–µ—Ä–µ–¥–∏–Ω—ñ (–µ—Ñ–µ–∫—Ç \"–∑–∞–≥—É–±–ª–µ–Ω–æ—Å—Ç—ñ –≤ —Å–µ—Ä–µ–¥–∏–Ω—ñ\")",
              "–î–ª—è –∑–∞–¥–∞—á –∑ –¥–æ–≤–≥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —á–∞–Ω–∫—É–π—Ç–µ –≤—Ö—ñ–¥ —Ç–∞ —Å—É–º–∞—Ä–∏–∑—É–π—Ç–µ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ —Å–µ–∫—Ü—ñ—ó ‚Äî —Ñ–æ–∫—É—Å–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∞—î –∫—Ä–∞—â—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –Ω—ñ–∂ –≤–∏—á–µ—Ä–ø–Ω–∏–π",
              "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ RAG –∑–∞–º—ñ—Å—Ç—å –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –≤—Å—å–æ–≥–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Äî –≤–∏—Ç—è–≥—É–π—Ç–µ –ª–∏—à–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–ø–∏—Ç—É, –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –º–æ–¥–µ–ª—å –º–∞—î –≤–µ–ª–∏—á–µ–∑–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–µ –≤—ñ–∫–Ω–æ"
            ]
          },
          "related": [
            "Agents & Tools",
            "Feed"
          ]
        },
        {
          "slug": "hallucination",
          "title": {
            "en": "Hallucinations",
            "uk": "–ì–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó"
          },
          "desc": {
            "en": "Why LLMs generate false information and how to detect and mitigate it.",
            "uk": "–ß–æ–º—É LLM –≥–µ–Ω–µ—Ä—É—é—Ç—å —Ö–∏–±–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é —Ç–∞ —è–∫ —ó—ó –≤–∏—è–≤–∏—Ç–∏ –π –∑–º–µ–Ω—à–∏—Ç–∏."
          },
          "overview": {
            "en": [
              "Hallucinations (also called confabulations) occur when AI models generate plausible-sounding but factually incorrect information. This is not a bug ‚Äî it is an inherent property of how LLMs work. Since they generate text by predicting likely next tokens rather than looking up facts, they can produce confident-sounding nonsense.",
              "Understanding hallucinations is critical for using AI safely. They range from minor factual errors to completely fabricated citations, invented APIs, and non-existent functions. The key mitigation strategies include grounding responses in retrieved data (RAG), using tool calls for fact-checking, and always verifying critical information independently."
            ],
            "uk": [
              "–ì–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó (—Ç–∞–∫–æ–∂ –∫–æ–Ω—Ñ–∞–±—É–ª—è—Ü—ñ—ó) –≤–∏–Ω–∏–∫–∞—é—Ç—å, –∫–æ–ª–∏ –º–æ–¥–µ–ª—ñ –®–Ü –≥–µ–Ω–µ—Ä—É—é—Ç—å –ø—Ä–∞–≤–¥–æ–ø–æ–¥—ñ–±–Ω–æ –∑–≤—É—á–Ω—É, –∞–ª–µ —Ñ–∞–∫—Ç–∏—á–Ω–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é. –¶–µ –Ω–µ –±–∞–≥ ‚Äî —Ü–µ –Ω–µ–≤—ñ–¥'—î–º–Ω–∞ –≤–ª–∞—Å—Ç–∏–≤—ñ—Å—Ç—å —Ç–æ–≥–æ, —è–∫ –ø—Ä–∞—Ü—é—é—Ç—å LLM. –û—Å–∫—ñ–ª—å–∫–∏ –≤–æ–Ω–∏ –≥–µ–Ω–µ—Ä—É—é—Ç—å —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è–º –Ω–∞–π—ñ–º–æ–≤—ñ—Ä–Ω—ñ—à–∏—Ö –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤, –∞ –Ω–µ –ø–æ—à—É–∫–æ–º —Ñ–∞–∫—Ç—ñ–≤, –≤–æ–Ω–∏ –º–æ–∂—É—Ç—å –≤–∏–¥–∞–≤–∞—Ç–∏ –≤–ø–µ–≤–Ω–µ–Ω–æ –∑–≤—É—á–Ω—É –Ω—ñ—Å–µ–Ω—ñ—Ç–Ω–∏—Ü—é.",
              "–†–æ–∑—É–º—ñ–Ω–Ω—è –≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ–π –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–µ –¥–ª—è –±–µ–∑–ø–µ—á–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –®–Ü. –í–æ–Ω–∏ –≤–∞—Ä—ñ—é—é—Ç—å—Å—è –≤—ñ–¥ –Ω–µ–∑–Ω–∞—á–Ω–∏—Ö —Ñ–∞–∫—Ç–∏—á–Ω–∏—Ö –ø–æ–º–∏–ª–æ–∫ –¥–æ –ø–æ–≤–Ω—ñ—Å—Ç—é –≤–∏–≥–∞–¥–∞–Ω–∏—Ö —Ü–∏—Ç–∞—Ç, –≤–∏–≥–∞–¥–∞–Ω–∏—Ö API —Ç–∞ –Ω–µ—ñ—Å–Ω—É—é—á–∏—Ö —Ñ—É–Ω–∫—Ü—ñ–π. –ö–ª—é—á–æ–≤—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –∑–º–µ–Ω—à–µ–Ω–Ω—è –≤–∫–ª—é—á–∞—é—Ç—å –∑–∞–∑–µ–º–ª–µ–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –Ω–∞ –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö (RAG), –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤–∏–∫–ª–∏–∫—ñ–≤ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ñ–∞–∫—Ç—ñ–≤ —Ç–∞ –∑–∞–≤–∂–¥–∏ –Ω–µ–∑–∞–ª–µ–∂–Ω—É –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—é –∫—Ä–∏—Ç–∏—á–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó."
            ]
          },
          "details": {
            "en": [
              {
                "text": "What Are Hallucinations",
                "desc": "Confident-sounding false statements generated by AI. The model states something as fact that is partially or completely fabricated ‚Äî and it sounds just as convincing as its correct outputs.",
                "links": [
                  {
                    "title": "Generative AI",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "Why They Happen",
                "desc": "LLMs predict likely text, not factual text ‚Äî they have no internal fact database. They generate what \"sounds right\" based on training patterns, which sometimes produces plausible-sounding nonsense.",
                "links": [
                  {
                    "title": "LLM and GPT",
                    "href": "../level-1/llm-and-gpt.html"
                  }
                ]
              },
              {
                "text": "Factual Hallucinations",
                "desc": "Wrong dates, made-up statistics, incorrect attributions. The model confidently states \"Einstein published relativity in 1903\" or invents plausible-sounding but non-existent facts.",
                "links": []
              },
              {
                "text": "Fabricated References",
                "desc": "Invented academic papers, fake URLs, non-existent API endpoints or function names. Particularly dangerous because they look legitimate and are hard to spot without verification.",
                "links": []
              },
              {
                "text": "Logical Hallucinations",
                "desc": "Correct-sounding reasoning chains that reach wrong conclusions. Each step seems reasonable but the overall logic is flawed ‚Äî especially dangerous because it \"shows its work.\"",
                "links": [
                  {
                    "title": "Reasoning",
                    "href": "../level-1/reasoning.html"
                  }
                ]
              },
              {
                "text": "Detection Strategies",
                "desc": "Cross-reference with authoritative sources, run generated code to verify it works, check citations actually exist, use search tools to verify facts. Trust but verify.",
                "links": []
              },
              {
                "text": "RAG Grounding",
                "desc": "Providing retrieved real documents reduces hallucination by giving models actual data to cite. Instead of generating from memory, the model references specific source material.",
                "links": [
                  {
                    "title": "RAG",
                    "href": "../level-4/rag.html"
                  }
                ]
              },
              {
                "text": "Tool Use Grounding",
                "desc": "Letting models call search APIs, databases, or calculators to verify facts before answering. External tools provide ground truth that the model can reference instead of guessing.",
                "links": [
                  {
                    "title": "Tool Use",
                    "href": "../level-4/tool-use.html"
                  }
                ]
              },
              {
                "text": "When Hallucinations Are Dangerous",
                "desc": "Medical/legal/financial advice, API documentation, safety-critical systems, academic research. In these domains, a confident wrong answer can cause real harm.",
                "links": []
              },
              {
                "text": "When They're Acceptable",
                "desc": "Creative writing, brainstorming, generating placeholder content, exploring ideas. In creative contexts, the model's ability to generate novel combinations is a feature, not a bug.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "–©–æ —Ç–∞–∫–µ –≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó",
                "desc": "–í–ø–µ–≤–Ω–µ–Ω–æ –∑–≤—É—á–Ω—ñ —Ö–∏–±–Ω—ñ —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è, –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ –®–Ü. –ú–æ–¥–µ–ª—å —Å—Ç–≤–µ—Ä–¥–∂—É—î —â–æ—Å—å —è–∫ —Ñ–∞–∫—Ç, —â–æ —á–∞—Å—Ç–∫–æ–≤–æ –∞–±–æ –ø–æ–≤–Ω—ñ—Å—Ç—é –≤–∏–≥–∞–¥–∞–Ω–æ ‚Äî —ñ —Ü–µ –∑–≤—É—á–∏—Ç—å —Ç–∞–∫ —Å–∞–º–æ –ø–µ—Ä–µ–∫–æ–Ω–ª–∏–≤–æ —è–∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.",
                "links": [
                  {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "–ß–æ–º—É —Ü–µ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è",
                "desc": "LLM –ø–µ—Ä–µ–¥–±–∞—á–∞—é—Ç—å —ñ–º–æ–≤—ñ—Ä–Ω–∏–π —Ç–µ–∫—Å—Ç, –Ω–µ —Ñ–∞–∫—Ç–∏—á–Ω–∏–π ‚Äî –≤–æ–Ω–∏ –Ω–µ –º–∞—é—Ç—å –≤–Ω—É—Ç—Ä—ñ—à–Ω—å–æ—ó –±–∞–∑–∏ —Ñ–∞–∫—Ç—ñ–≤. –í–æ–Ω–∏ –≥–µ–Ω–µ—Ä—É—é—Ç—å —Ç–µ, —â–æ \"–∑–≤—É—á–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ\" –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –ø–∞—Ç–µ—Ä–Ω—ñ–≤.",
                "links": [
                  {
                    "title": "LLM —Ç–∞ GPT",
                    "href": "../level-1/llm-and-gpt.html"
                  }
                ]
              },
              {
                "text": "–§–∞–∫—Ç–∏—á–Ω—ñ –≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó",
                "desc": "–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ñ –¥–∞—Ç–∏, –≤–∏–≥–∞–¥–∞–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω—ñ –∞—Ç—Ä–∏–±—É—Ü—ñ—ó. –ú–æ–¥–µ–ª—å –≤–ø–µ–≤–Ω–µ–Ω–æ —Å—Ç–≤–µ—Ä–¥–∂—É—î \"–ï–π–Ω—à—Ç–µ–π–Ω –æ–ø—É–±–ª—ñ–∫—É–≤–∞–≤ —Ç–µ–æ—Ä—ñ—é –≤—ñ–¥–Ω–æ—Å–Ω–æ—Å—Ç—ñ —É 1903\" –∞–±–æ –≤–∏–≥–∞–¥—É—î –ø—Ä–∞–≤–¥–æ–ø–æ–¥—ñ–±–Ω—ñ, –∞–ª–µ –Ω–µ—ñ—Å–Ω—É—é—á—ñ —Ñ–∞–∫—Ç–∏.",
                "links": []
              },
              {
                "text": "–í–∏–≥–∞–¥–∞–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è",
                "desc": "–í–∏–≥–∞–¥–∞–Ω—ñ –Ω–∞—É–∫–æ–≤—ñ —Å—Ç–∞—Ç—Ç—ñ, —Ñ–µ–π–∫–æ–≤—ñ URL, –Ω–µ—ñ—Å–Ω—É—é—á—ñ API-–µ–Ω–¥–ø–æ—ñ–Ω—Ç–∏ –∞–±–æ —ñ–º–µ–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ–π. –û—Å–æ–±–ª–∏–≤–æ –Ω–µ–±–µ–∑–ø–µ—á–Ω—ñ, –±–æ –≤–∏–≥–ª—è–¥–∞—é—Ç—å –ª–µ–≥—ñ—Ç–∏–º–Ω–æ —ñ –≤–∞–∂–∫–æ –ø–æ–º—ñ—Ç–∏—Ç–∏ –±–µ–∑ –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—ó.",
                "links": []
              },
              {
                "text": "–õ–æ–≥—ñ—á–Ω—ñ –≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó",
                "desc": "–ü—Ä–∞–≤–∏–ª—å–Ω–æ –∑–≤—É—á–Ω—ñ –ª–∞–Ω—Ü—é–≥–∏ –º—ñ—Ä–∫—É–≤–∞–Ω—å, —â–æ –¥–æ—Å—è–≥–∞—é—Ç—å —Ö–∏–±–Ω–∏—Ö –≤–∏—Å–Ω–æ–≤–∫—ñ–≤. –ö–æ–∂–µ–Ω –∫—Ä–æ–∫ –∑–¥–∞—î—Ç—å—Å—è —Ä–æ–∑—É–º–Ω–∏–º, –∞–ª–µ –∑–∞–≥–∞–ª—å–Ω–∞ –ª–æ–≥—ñ–∫–∞ —Ö–∏–±–Ω–∞ ‚Äî –æ—Å–æ–±–ª–∏–≤–æ –Ω–µ–±–µ–∑–ø–µ—á–Ω–æ, –±–æ \"–ø–æ–∫–∞–∑—É—î —Ä–æ–±–æ—Ç—É.\"",
                "links": [
                  {
                    "title": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                    "href": "../level-1/reasoning.html"
                  }
                ]
              },
              {
                "text": "–°—Ç—Ä–∞—Ç–µ–≥—ñ—ó –≤–∏—è–≤–ª–µ–Ω–Ω—è",
                "desc": "–ü–µ—Ä–µ—Ö—Ä–µ—Å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–∏–º–∏ –¥–∂–µ—Ä–µ–ª–∞–º–∏, –∑–∞–ø—É—Å–∫ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ–≥–æ –∫–æ–¥—É, –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è —Ü–∏—Ç–∞—Ç, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–æ—à—É–∫—É –¥–ª—è –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Ñ–∞–∫—Ç—ñ–≤. –î–æ–≤—ñ—Ä—è–π, –∞–ª–µ –ø–µ—Ä–µ–≤—ñ—Ä—è–π.",
                "links": []
              },
              {
                "text": "–ó–∞–∑–µ–º–ª–µ–Ω–Ω—è RAG",
                "desc": "–ù–∞–¥–∞–Ω–Ω—è —Ä–µ–∞–ª—å–Ω–∏—Ö –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑–º–µ–Ω—à—É—î –≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó, –¥–∞—é—á–∏ –º–æ–¥–µ–ª—è–º —Ñ–∞–∫—Ç–∏—á–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è —Ü–∏—Ç—É–≤–∞–Ω–Ω—è. –ó–∞–º—ñ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑ –ø–∞–º'—è—Ç—ñ –º–æ–¥–µ–ª—å –ø–æ—Å–∏–ª–∞—î—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π –≤–∏—Ö—ñ–¥–Ω–∏–π –º–∞—Ç–µ—Ä—ñ–∞–ª.",
                "links": [
                  {
                    "title": "RAG",
                    "href": "../level-4/rag.html"
                  }
                ]
              },
              {
                "text": "–ó–∞–∑–µ–º–ª–µ–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏",
                "desc": "–î–æ–∑–≤—ñ–ª –º–æ–¥–µ–ª—è–º –≤–∏–∫–ª–∏–∫–∞—Ç–∏ API –ø–æ—à—É–∫—É, –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –∞–±–æ –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∏ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ñ–∞–∫—Ç—ñ–≤. –ó–æ–≤–Ω—ñ—à–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –Ω–∞–¥–∞—é—Ç—å –æ—Å–Ω–æ–≤—É —ñ—Å—Ç–∏–Ω–∏.",
                "links": [
                  {
                    "title": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤",
                    "href": "../level-4/tool-use.html"
                  }
                ]
              },
              {
                "text": "–ö–æ–ª–∏ –≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó –Ω–µ–±–µ–∑–ø–µ—á–Ω—ñ",
                "desc": "–ú–µ–¥–∏—á–Ω–∞/—é—Ä–∏–¥–∏—á–Ω–∞/—Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∞ –ø–æ—Ä–∞–¥–∞, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è API, —Å–∏—Å—Ç–µ–º–∏ –±–µ–∑–ø–µ–∫–∏, –∞–∫–∞–¥–µ–º—ñ—á–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è. –£ —Ü–∏—Ö –¥–æ–º–µ–Ω–∞—Ö –≤–ø–µ–≤–Ω–µ–Ω–∞ —Ö–∏–±–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –º–æ–∂–µ –∑–∞–≤–¥–∞—Ç–∏ —Ä–µ–∞–ª—å–Ω–æ—ó —à–∫–æ–¥–∏.",
                "links": []
              },
              {
                "text": "–ö–æ–ª–∏ –≤–æ–Ω–∏ –ø—Ä–∏–π–Ω—è—Ç–Ω—ñ",
                "desc": "–¢–≤–æ—Ä—á–µ –ø–∏—Å—å–º–æ, –º–æ–∑–∫–æ–≤–∏–π —à—Ç—É—Ä–º, –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–∞–ø–æ–≤–Ω—é–≤–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É, –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —ñ–¥–µ–π. –£ —Ç–≤–æ—Ä—á–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó ‚Äî —Ü–µ –ø–µ—Ä–µ–≤–∞–≥–∞.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Hallucination",
                "def": "When an AI model generates false information that sounds confident and plausible."
              },
              {
                "term": "Confabulation",
                "def": "Alternative term for hallucination, emphasizing the model is \"filling in gaps\" rather than lying."
              },
              {
                "term": "Grounding",
                "def": "Providing real data (via RAG or tools) so the model bases answers on facts, not just training patterns."
              },
              {
                "term": "Calibration",
                "def": "How well a model's confidence matches its actual accuracy ‚Äî well-calibrated models know when they're uncertain."
              }
            ],
            "uk": [
              {
                "term": "–ì–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—è",
                "def": "–ö–æ–ª–∏ –º–æ–¥–µ–ª—å –®–Ü –≥–µ–Ω–µ—Ä—É—î —Ö–∏–±–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é, —â–æ –∑–≤—É—á–∏—Ç—å –≤–ø–µ–≤–Ω–µ–Ω–æ —Ç–∞ –ø—Ä–∞–≤–¥–æ–ø–æ–¥—ñ–±–Ω–æ."
              },
              {
                "term": "–ö–æ–Ω—Ñ–∞–±—É–ª—è—Ü—ñ—è",
                "def": "–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π —Ç–µ—Ä–º—ñ–Ω –¥–ª—è –≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó, –ø—ñ–¥–∫—Ä–µ—Å–ª—é—é—á–∏ —â–æ –º–æ–¥–µ–ª—å \"–∑–∞–ø–æ–≤–Ω—é—î –ø—Ä–æ–≥–∞–ª–∏–Ω–∏\", –∞ –Ω–µ –±—Ä–µ—à–µ."
              },
              {
                "term": "–ó–∞–∑–µ–º–ª–µ–Ω–Ω—è",
                "def": "–ù–∞–¥–∞–Ω–Ω—è —Ä–µ–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö (—á–µ—Ä–µ–∑ RAG –∞–±–æ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏), —â–æ–± –º–æ–¥–µ–ª—å –±–∞–∑—É–≤–∞–ª–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ —Ñ–∞–∫—Ç–∞—Ö."
              },
              {
                "term": "–ö–∞–ª—ñ–±—Ä—É–≤–∞–Ω–Ω—è",
                "def": "–ù–∞—Å–∫—ñ–ª—å–∫–∏ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î —ó—ó —Ä–µ–∞–ª—å–Ω—ñ–π —Ç–æ—á–Ω–æ—Å—Ç—ñ ‚Äî –¥–æ–±—Ä–µ –∫–∞–ª—ñ–±—Ä–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ –∑–Ω–∞—é—Ç—å, –∫–æ–ª–∏ –Ω–µ–≤–ø–µ–≤–Ω–µ–Ω—ñ."
              }
            ]
          },
          "tips": {
            "en": [
              "Always verify AI-generated facts, especially numbers, dates, URLs, and citations ‚Äî these are the most common hallucination categories",
              "Ask the model \"Are you certain about this?\" or \"What are you least confident about?\" ‚Äî models can sometimes self-identify uncertain claims",
              "For high-stakes tasks, use RAG with verified sources rather than relying on the model's parametric knowledge ‚Äî grounded responses hallucinate far less"
            ],
            "uk": [
              "–ó–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–≤—ñ—Ä—è–π—Ç–µ —Ñ–∞–∫—Ç–∏, –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ –®–Ü, –æ—Å–æ–±–ª–∏–≤–æ —á–∏—Å–ª–∞, –¥–∞—Ç–∏, URL —Ç–∞ —Ü–∏—Ç–∞—Ç–∏ ‚Äî —Ü–µ –Ω–∞–π–ø–æ—à–∏—Ä–µ–Ω—ñ—à—ñ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ–π",
              "–ó–∞–ø–∏—Ç–∞–π—Ç–µ –º–æ–¥–µ–ª—å \"–í–∏ –≤–ø–µ–≤–Ω–µ–Ω—ñ –≤ —Ü—å–æ–º—É?\" –∞–±–æ \"–í —á–æ–º—É –≤–∏ –Ω–∞–π–º–µ–Ω—à –≤–ø–µ–≤–Ω–µ–Ω—ñ?\" ‚Äî –º–æ–¥–µ–ª—ñ —ñ–Ω–æ–¥—ñ –º–æ–∂—É—Ç—å —Å–∞–º–æ—Å—Ç—ñ–π–Ω–æ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ –Ω–µ–≤–ø–µ–≤–Ω–µ–Ω—ñ —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è",
              "–î–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –∑–∞–¥–∞—á –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ RAG –∑ –≤–µ—Ä–∏—Ñ—ñ–∫–æ–≤–∞–Ω–∏–º–∏ –¥–∂–µ—Ä–µ–ª–∞–º–∏ –∑–∞–º—ñ—Å—Ç—å –ø–æ–∫–ª–∞–¥–∞–Ω–Ω—è –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–Ω—ñ –∑–Ω–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ ‚Äî –∑–∞–∑–µ–º–ª–µ–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –≥–∞–ª—é—Ü–∏–Ω—É—é—Ç—å –∑–Ω–∞—á–Ω–æ –º–µ–Ω—à–µ"
            ]
          },
          "related": [
            "Feed"
          ]
        },
        {
          "slug": "vibecoding",
          "title": {
            "en": "Vibecoding",
            "uk": "–í–∞–π–±–∫–æ–¥–∏–Ω–≥"
          },
          "desc": {
            "en": "The new paradigm of coding by intent - describing what you want and letting AI write the code.",
            "uk": "–ù–æ–≤–∞ –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è –∑–∞ –Ω–∞–º—ñ—Ä–æ–º ‚Äî –æ–ø–∏—Å—É—î—à —â–æ —Ö–æ—á–µ—à, –∞ –®–Ü –ø–∏—à–µ –∫–æ–¥."
          },
          "overview": {
            "en": [
              "Vibecoding is a term coined by Andrej Karpathy in early 2025 to describe a new programming paradigm: instead of manually writing every line of code, you describe what you want in natural language and let AI generate the implementation. You guide the process through conversation, accepting or rejecting generated code based on whether the result \"feels right.\"",
              "This approach works surprisingly well for prototypes, scripts, and web apps. Tools like Cursor, Claude Code, GitHub Copilot, and Windsurf enable vibecoding workflows. However, it has clear limitations for complex systems ‚Äî understanding what the code does remains important for debugging, security, and maintenance."
            ],
            "uk": [
              "–í–∞–π–±–∫–æ–¥–∏–Ω–≥ ‚Äî —Ü–µ —Ç–µ—Ä–º—ñ–Ω, –∑–∞–ø—Ä–æ–ø–æ–Ω–æ–≤–∞–Ω–∏–π –ê–Ω–¥—Ä–µ—î–º –ö–∞—Ä–ø–∞—Ç—ñ –Ω–∞ –ø–æ—á–∞—Ç–∫—É 2025 —Ä–æ–∫—É –¥–ª—è –æ–ø–∏—Å—É –Ω–æ–≤–æ—ó –ø–∞—Ä–∞–¥–∏–≥–º–∏ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è: –∑–∞–º—ñ—Å—Ç—å —Ä—É—á–Ω–æ–≥–æ –Ω–∞–ø–∏—Å–∞–Ω–Ω—è –∫–æ–∂–Ω–æ–≥–æ —Ä—è–¥–∫–∞ –∫–æ–¥—É, –≤–∏ –æ–ø–∏—Å—É—î—Ç–µ —â–æ —Ö–æ—á–µ—Ç–µ –ø—Ä–∏—Ä–æ–¥–Ω–æ—é –º–æ–≤–æ—é —ñ –¥–æ–∑–≤–æ–ª—è—î—Ç–µ –®–Ü –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—é. –í–∏ –∫–µ—Ä—É—î—Ç–µ –ø—Ä–æ—Ü–µ—Å–æ–º —á–µ—Ä–µ–∑ —Ä–æ–∑–º–æ–≤—É, –ø—Ä–∏–π–º–∞—é—á–∏ –∞–±–æ –≤—ñ–¥—Ö–∏–ª—è—é—á–∏ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –∫–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç–æ–≥–æ, —á–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç \"–≤—ñ–¥—á—É–≤–∞—î—Ç—å—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º.\"",
              "–¶–µ–π –ø—ñ–¥—Ö—ñ–¥ –ø—Ä–∞—Ü—é—î –Ω–∞–ø—Ä–æ—á—É–¥ –¥–æ–±—Ä–µ –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø—ñ–≤, —Å–∫—Ä–∏–ø—Ç—ñ–≤ —Ç–∞ –≤–µ–±-–¥–æ–¥–∞—Ç–∫—ñ–≤. –Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —è–∫ Cursor, Claude Code, GitHub Copilot —Ç–∞ Windsurf –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å –≤–æ—Ä–∫—Ñ–ª–æ—É –≤–∞–π–±–∫–æ–¥–∏–Ω–≥—É. –û–¥–Ω–∞–∫ –≤—ñ–Ω –º–∞—î —á—ñ—Ç–∫—ñ –æ–±–º–µ–∂–µ–Ω–Ω—è –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö —Å–∏—Å—Ç–µ–º ‚Äî —Ä–æ–∑—É–º—ñ–Ω–Ω—è —â–æ –∫–æ–¥ —Ä–æ–±–∏—Ç—å –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –≤–∞–∂–ª–∏–≤–∏–º –¥–ª—è –¥–µ–±–∞–≥—ñ–Ω–≥—É, –±–µ–∑–ø–µ–∫–∏ —Ç–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∏."
            ]
          },
          "details": {
            "en": [
              {
                "text": "What Is Vibecoding",
                "desc": "Describing what you want in natural language and letting AI write the code. You guide the process through conversation, accepting or rejecting output based on whether it \"feels right.\"",
                "links": [
                  {
                    "title": "Generative AI",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "Origin of the Term",
                "desc": "Andrej Karpathy coined \"vibecoding\" in February 2025 to describe programming by intent rather than syntax. The term went viral and now defines a new programming paradigm.",
                "links": []
              },
              {
                "text": "The Coding Spectrum",
                "desc": "Manual coding ‚Üí assisted (Copilot autocomplete) ‚Üí vibecoding (describe intent, AI implements) ‚Üí fully autonomous agents (AI plans and executes entire features). We're moving right.",
                "links": [
                  {
                    "title": "Agents",
                    "href": "../level-4/agents.html"
                  }
                ]
              },
              {
                "text": "Key Vibecoding Tools",
                "desc": "Cursor (AI-first IDE), Claude Code (CLI agent), GitHub Copilot (inline suggestions), Windsurf (AI IDE), Bolt/v0 (web app generators). Each tool suits different workflows.",
                "links": [
                  {
                    "title": "Tools & Libraries",
                    "href": "../level-4/base-tools.html"
                  }
                ]
              },
              {
                "text": "When It Works Well",
                "desc": "Prototypes, scripts, standard web apps (CRUD, dashboards, forms), well-defined tasks with clear requirements. Vibecoding excels when patterns are common and well-represented in training data.",
                "links": []
              },
              {
                "text": "When It Fails",
                "desc": "Complex architectures, novel algorithms, security-critical code, performance optimization, and anything requiring deep domain expertise. Understanding limitations prevents dangerous overreliance.",
                "links": [
                  {
                    "title": "Hallucinations",
                    "href": "hallucination.html"
                  }
                ]
              },
              {
                "text": "Best Practices",
                "desc": "Clear requirements upfront, iterative refinement (don't accept first output), always review generated code for correctness and security. The developer's role shifts from writing to directing and reviewing.",
                "links": [
                  {
                    "title": "Prompt",
                    "href": "prompt.html"
                  }
                ]
              },
              {
                "text": "Spec-Driven Vibecoding",
                "desc": "Writing detailed specifications and requirements first, then letting AI implement them. A good spec + AI is more effective than a vague request + AI. The spec IS the new code.",
                "links": []
              },
              {
                "text": "Testing Is Essential",
                "desc": "Vibecoded projects need comprehensive tests since you may not fully understand every line of generated code. Tests serve as your verification layer ‚Äî they're your safety net.",
                "links": []
              },
              {
                "text": "The Future of Coding",
                "desc": "Programming evolves from writing syntax to directing intent, reviewing output, and designing systems. Understanding architecture and requirements becomes more important than memorizing APIs.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–©–æ —Ç–∞–∫–µ –≤–∞–π–±–∫–æ–¥–∏–Ω–≥",
                "desc": "–û–ø–∏—Å —Ç–æ–≥–æ, —â–æ –≤–∏ —Ö–æ—á–µ—Ç–µ, –ø—Ä–∏—Ä–æ–¥–Ω–æ—é –º–æ–≤–æ—é, –¥–µ –®–Ü –ø–∏—à–µ –∫–æ–¥. –í–∏ –∫–µ—Ä—É—î—Ç–µ –ø—Ä–æ—Ü–µ—Å–æ–º —á–µ—Ä–µ–∑ —Ä–æ–∑–º–æ–≤—É, –ø—Ä–∏–π–º–∞—é—á–∏ –∞–±–æ –≤—ñ–¥—Ö–∏–ª—è—é—á–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ç–æ–≥–æ, —á–∏ –≤—ñ–Ω \"–≤—ñ–¥—á—É–≤–∞—î—Ç—å—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º.\"",
                "links": [
                  {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "–ü–æ—Ö–æ–¥–∂–µ–Ω–Ω—è —Ç–µ—Ä–º—ñ–Ω—É",
                "desc": "–ê–Ω–¥—Ä–µ–π –ö–∞—Ä–ø–∞—Ç—ñ –∑–∞–ø—Ä–æ–ø–æ–Ω—É–≤–∞–≤ \"–≤–∞–π–±–∫–æ–¥–∏–Ω–≥\" —É –ª—é—Ç–æ–º—É 2025 –¥–ª—è –æ–ø–∏—Å—É –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è –∑–∞ –Ω–∞–º—ñ—Ä–æ–º, –∞ –Ω–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–æ–º. –¢–µ—Ä–º—ñ–Ω —Å—Ç–∞–≤ –≤—ñ—Ä—É—Å–Ω–∏–º —ñ —Ç–µ–ø–µ—Ä –≤–∏–∑–Ω–∞—á–∞—î –Ω–æ–≤—É –ø–∞—Ä–∞–¥–∏–≥–º—É –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è.",
                "links": []
              },
              {
                "text": "–°–ø–µ–∫—Ç—Ä –∫–æ–¥—É–≤–∞–Ω–Ω—è",
                "desc": "–†—É—á–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è ‚Üí –∞—Å–∏—Å—Ç–æ–≤–∞–Ω–µ (Copilot) ‚Üí –≤–∞–π–±–∫–æ–¥–∏–Ω–≥ (–æ–ø–∏—Å –Ω–∞–º—ñ—Ä—É, –®–Ü —Ä–µ–∞–ª—ñ–∑—É—î) ‚Üí –ø–æ–≤–Ω—ñ—Å—Ç—é –∞–≤—Ç–æ–Ω–æ–º–Ω—ñ –∞–≥–µ–Ω—Ç–∏ (–®–Ü –ø–ª–∞–Ω—É—î —Ç–∞ –≤–∏–∫–æ–Ω—É—î —Ü—ñ–ª—ñ —Ñ—ñ—á—ñ). –ú–∏ —Ä—É—Ö–∞—î–º–æ—Å—å –≤–ø—Ä–∞–≤–æ.",
                "links": [
                  {
                    "title": "–ê–≥–µ–Ω—Ç–∏",
                    "href": "../level-4/agents.html"
                  }
                ]
              },
              {
                "text": "–ö–ª—é—á–æ–≤—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –≤–∞–π–±–∫–æ–¥–∏–Ω–≥—É",
                "desc": "Cursor (AI-first IDE), Claude Code (CLI –∞–≥–µ–Ω—Ç), GitHub Copilot (—ñ–Ω–ª–∞–π–Ω –ø—ñ–¥–∫–∞–∑–∫–∏), Windsurf (–®–Ü IDE), Bolt/v0 (–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∏ –≤–µ–±-–¥–æ–¥–∞—Ç–∫—ñ–≤). –ö–æ–∂–µ–Ω —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –≤–æ—Ä–∫—Ñ–ª–æ—É.",
                "links": [
                  {
                    "title": "–Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —Ç–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏",
                    "href": "../level-4/base-tools.html"
                  }
                ]
              },
              {
                "text": "–ö–æ–ª–∏ –ø—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ",
                "desc": "–ü—Ä–æ—Ç–æ—Ç–∏–ø–∏, —Å–∫—Ä–∏–ø—Ç–∏, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –≤–µ–±-–¥–æ–¥–∞—Ç–∫–∏ (CRUD, –¥–∞—à–±–æ—Ä–¥–∏, —Ñ–æ—Ä–º–∏), —á—ñ—Ç–∫–æ –≤–∏–∑–Ω–∞—á–µ–Ω—ñ –∑–∞–¥–∞—á—ñ. –í–∞–π–±–∫–æ–¥–∏–Ω–≥ –≤—ñ–¥–º—ñ–Ω–Ω–∏–π, –∫–æ–ª–∏ –ø–∞—Ç–µ—Ä–Ω–∏ –ø–æ—à–∏—Ä–µ–Ω—ñ —Ç–∞ –¥–æ–±—Ä–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ñ —É –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö.",
                "links": []
              },
              {
                "text": "–ö–æ–ª–∏ –∑–±–æ—ó—Ç—å",
                "desc": "–°–∫–ª–∞–¥–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏, –Ω–æ–≤—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏, –∫—Ä–∏—Ç–∏—á–Ω–∏–π –¥–ª—è –±–µ–∑–ø–µ–∫–∏ –∫–æ–¥, –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ —Ç–∞ –≤—Å–µ, —â–æ –≤–∏–º–∞–≥–∞—î –≥–ª–∏–±–æ–∫–æ—ó –¥–æ–º–µ–Ω–Ω–æ—ó –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏. –†–æ–∑—É–º—ñ–Ω–Ω—è –æ–±–º–µ–∂–µ–Ω—å –∑–∞–ø–æ–±—ñ–≥–∞—î –Ω–µ–±–µ–∑–ø–µ—á–Ω—ñ–π –Ω–∞–¥–º—ñ—Ä–Ω—ñ–π –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ.",
                "links": [
                  {
                    "title": "–ì–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó",
                    "href": "hallucination.html"
                  }
                ]
              },
              {
                "text": "–ù–∞–π–∫—Ä–∞—â—ñ –ø—Ä–∞–∫—Ç–∏–∫–∏",
                "desc": "–ß—ñ—Ç–∫—ñ –≤–∏–º–æ–≥–∏ –Ω–∞–ø–µ—Ä–µ–¥, —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–µ –≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è (–Ω–µ –ø—Ä–∏–π–º–∞–π—Ç–µ –ø–µ—Ä—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç), –∑–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–≥–ª—è–¥–∞–π—Ç–µ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –∫–æ–¥. –†–æ–ª—å —Ä–æ–∑—Ä–æ–±–Ω–∏–∫–∞ –∑–º—ñ—â—É—î—Ç—å—Å—è –≤—ñ–¥ –Ω–∞–ø–∏—Å–∞–Ω–Ω—è –¥–æ –∫–µ—Ä—É–≤–∞–Ω–Ω—è —Ç–∞ —Ä–µ–≤—é.",
                "links": [
                  {
                    "title": "–ü—Ä–æ–º–ø—Ç",
                    "href": "prompt.html"
                  }
                ]
              },
              {
                "text": "–°–ø–µ–∫-–∫–µ—Ä–æ–≤–∞–Ω–∏–π –≤–∞–π–±–∫–æ–¥–∏–Ω–≥",
                "desc": "–°–ø–æ—á–∞—Ç–∫—É –Ω–∞–ø–∏—Å–∞–Ω–Ω—è –¥–µ—Ç–∞–ª—å–Ω–∏—Ö —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ–π —Ç–∞ –≤–∏–º–æ–≥, –ø–æ—Ç—ñ–º –®–Ü —Ä–µ–∞–ª—ñ–∑—É—î. –•–æ—Ä–æ—à–∞ —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ—è + –®–Ü –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—à–µ –Ω—ñ–∂ —Ä–æ–∑–º–∏—Ç–∏–π –∑–∞–ø–∏—Ç + –®–Ü. –°–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ—è ‚Äî —Ü–µ –Ω–æ–≤–∏–π –∫–æ–¥.",
                "links": []
              },
              {
                "text": "–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–µ–æ–±—Ö—ñ–¥–Ω–µ",
                "desc": "–ü—Ä–æ—î–∫—Ç–∏ –∑ –≤–∞–π–±–∫–æ–¥–∏–Ω–≥–æ–º –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω–∏—Ö —Ç–µ—Å—Ç—ñ–≤, –±–æ –≤–∏ –º–æ–∂–µ—Ç–µ –Ω–µ —Ä–æ–∑—É–º—ñ—Ç–∏ –∫–æ–∂–µ–Ω —Ä—è–¥–æ–∫ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ–≥–æ –∫–æ–¥—É. –¢–µ—Å—Ç–∏ ‚Äî –≤–∞—à —Ä—ñ–≤–µ–Ω—å –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Ç–∞ —Å—Ç—Ä–∞—Ö—É–≤–∞–ª—å–Ω–∞ —Å—ñ—Ç–∫–∞.",
                "links": []
              },
              {
                "text": "–ú–∞–π–±—É—Ç–Ω—î –∫–æ–¥—É–≤–∞–Ω–Ω—è",
                "desc": "–ü—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è –µ–≤–æ–ª—é—Ü—ñ–æ–Ω—É—î –≤—ñ–¥ –Ω–∞–ø–∏—Å–∞–Ω–Ω—è —Å–∏–Ω—Ç–∞–∫—Å–∏—Å—É –¥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –Ω–∞–º—ñ—Ä—É, —Ä–µ–≤—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —Ç–∞ –ø—Ä–æ—î–∫—Ç—É–≤–∞–Ω–Ω—è —Å–∏—Å—Ç–µ–º. –†–æ–∑—É–º—ñ–Ω–Ω—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ —Ç–∞ –≤–∏–º–æ–≥ —Å—Ç–∞—î –≤–∞–∂–ª–∏–≤—ñ—à–∏–º –∑–∞ –∑–∞–ø–∞–º'—è—Ç–æ–≤—É–≤–∞–Ω–Ω—è API.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Vibecoding",
                "def": "Programming by describing what you want in natural language and letting AI write the code."
              },
              {
                "term": "AI-First IDE",
                "def": "Development environment built around AI code generation (Cursor, Windsurf) rather than traditional text editing."
              },
              {
                "term": "Spec-Driven Development",
                "def": "Writing detailed requirements/specs first, then using AI to generate the implementation."
              },
              {
                "term": "Agentic Coding",
                "def": "AI coding assistants that can autonomously plan, write, test, and debug code with minimal guidance."
              }
            ],
            "uk": [
              {
                "term": "–í–∞–π–±–∫–æ–¥–∏–Ω–≥",
                "def": "–ü—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è —á–µ—Ä–µ–∑ –æ–ø–∏—Å –±–∞–∂–∞–Ω–æ–≥–æ –ø—Ä–∏—Ä–æ–¥–Ω–æ—é –º–æ–≤–æ—é, –¥–µ –®–Ü –ø–∏—à–µ –∫–æ–¥."
              },
              {
                "term": "AI-First IDE",
                "def": "–°–µ—Ä–µ–¥–æ–≤–∏—â–µ —Ä–æ–∑—Ä–æ–±–∫–∏, –ø–æ–±—É–¥–æ–≤–∞–Ω–µ –Ω–∞–≤–∫–æ–ª–æ –®–Ü-–≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–¥—É (Cursor, Windsurf)."
              },
              {
                "term": "–°–ø–µ–∫-–∫–µ—Ä–æ–≤–∞–Ω–∞ —Ä–æ–∑—Ä–æ–±–∫–∞",
                "def": "–ù–∞–ø–∏—Å–∞–Ω–Ω—è –¥–µ—Ç–∞–ª—å–Ω–∏—Ö –≤–∏–º–æ–≥/—Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ–π —Å–ø–æ—á–∞—Ç–∫—É, –ø–æ—Ç—ñ–º –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –®–Ü –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó."
              },
              {
                "term": "–ê–≥–µ–Ω—Ç–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è",
                "def": "–®–Ü-–∞—Å–∏—Å—Ç–µ–Ω—Ç–∏, —â–æ –º–æ–∂—É—Ç—å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –ø–ª–∞–Ω—É–≤–∞—Ç–∏, –ø–∏—Å–∞—Ç–∏, —Ç–µ—Å—Ç—É–≤–∞—Ç–∏ —Ç–∞ –¥–µ–±–∞–∂–∏—Ç–∏ –∫–æ–¥."
              }
            ]
          },
          "tips": {
            "en": [
              "Write clear specifications before vibecoding ‚Äî the better your description, the better the AI-generated code. A vague request produces vague code",
              "Always review and understand generated code before committing ‚Äî vibecoding is faster than manual coding, but blind acceptance creates hidden technical debt",
              "Use vibecoding for prototypes and standard patterns, switch to manual coding for performance-critical or security-sensitive components"
            ],
            "uk": [
              "–ü–∏—à—ñ—Ç—å —á—ñ—Ç–∫—ñ —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –ø–µ—Ä–µ–¥ –≤–∞–π–±–∫–æ–¥–∏–Ω–≥–æ–º ‚Äî —á–∏–º –∫—Ä–∞—â–∏–π –≤–∞—à –æ–ø–∏—Å, —Ç–∏–º –∫—Ä–∞—â–∏–π –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –∫–æ–¥. –†–æ–∑–º–∏—Ç–∏–π –∑–∞–ø–∏—Ç —Å—Ç–≤–æ—Ä—é—î —Ä–æ–∑–º–∏—Ç–∏–π –∫–æ–¥",
              "–ó–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–≥–ª—è–¥–∞–π—Ç–µ —Ç–∞ —Ä–æ–∑—É–º—ñ–π—Ç–µ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –∫–æ–¥ –ø–µ—Ä–µ–¥ –∫–æ–º—ñ—Ç–æ–º ‚Äî –≤–∞–π–±–∫–æ–¥–∏–Ω–≥ —à–≤–∏–¥—à–∏–π –∑–∞ —Ä—É—á–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è, –∞–ª–µ —Å–ª—ñ–ø–µ –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Å—Ç–≤–æ—Ä—é—î –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏–π —Ç–µ—Ö–Ω—ñ—á–Ω–∏–π –±–æ—Ä–≥",
              "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –≤–∞–π–±–∫–æ–¥–∏–Ω–≥ –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø—ñ–≤ —Ç–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏—Ö –ø–∞—Ç–µ—Ä–Ω—ñ–≤, –ø–µ—Ä–µ—Ö–æ–¥—å—Ç–µ –Ω–∞ —Ä—É—á–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –¥–æ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –∞–±–æ –±–µ–∑–ø–µ–∫–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤"
            ]
          },
          "related": [
            "Feed",
            "Video Content"
          ]
        }
      ]
    },
    {
      "num": 3,
      "emoji": "‚ö°",
      "title": {
        "en": "Professional",
        "uk": "–ü—Ä–æ—Ñ–µ—Å—ñ–æ–Ω–∞–ª"
      },
      "desc": {
        "en": "Deep technical understanding of neural networks, model training, optimization, and architecture.",
        "uk": "–ì–ª–∏–±–æ–∫–µ —Ç–µ—Ö–Ω—ñ—á–Ω–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂, –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π, –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó —Ç–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏."
      },
      "topics": [
        {
          "slug": "neural-networks",
          "title": {
            "en": "Neural Network Fundamentals",
            "uk": "–û—Å–Ω–æ–≤–∏ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂"
          },
          "desc": {
            "en": "Architecture of neural networks - layers, activation functions, and how learning happens.",
            "uk": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂ ‚Äî —à–∞—Ä–∏, —Ñ—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó —Ç–∞ —è–∫ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –Ω–∞–≤—á–∞–Ω–Ω—è."
          },
          "overview": {
            "en": [
              "Neural networks are the mathematical foundation underlying all modern AI. They are loosely inspired by biological neurons but in practice are systems of matrix multiplications and nonlinear functions organized into layers. Understanding how they work ‚Äî forward propagation, loss computation, and backpropagation ‚Äî is essential for anyone wanting to go beyond surface-level AI usage.",
              "The field has evolved from simple perceptrons in the 1950s to today's trillion-parameter transformer networks. Each architectural breakthrough ‚Äî from convolutional layers for vision to attention mechanisms for language ‚Äî expanded what neural networks could do. Knowing these fundamentals helps you understand why certain models excel at certain tasks and what the actual limitations of \"AI\" are."
            ],
            "uk": [
              "–ù–µ–π—Ä–æ–Ω–Ω—ñ –º–µ—Ä–µ–∂—ñ ‚Äî —Ü–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∞ –æ—Å–Ω–æ–≤–∞, —â–æ –ª–µ–∂–∏—Ç—å –ø—ñ–¥ —É—Å—ñ–º —Å—É—á–∞—Å–Ω–∏–º –®–Ü. –í–æ–Ω–∏ –Ω–∞—Ç—Ö–Ω–µ–Ω–Ω—ñ –±—ñ–æ–ª–æ–≥—ñ—á–Ω–∏–º–∏ –Ω–µ–π—Ä–æ–Ω–∞–º–∏, –∞–ª–µ –Ω–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ —î —Å–∏—Å—Ç–µ–º–∞–º–∏ –º–∞—Ç—Ä–∏—á–Ω–∏—Ö –º–Ω–æ–∂–µ–Ω—å —Ç–∞ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö —Ñ—É–Ω–∫—Ü—ñ–π, –æ—Ä–≥–∞–Ω—ñ–∑–æ–≤–∞–Ω–∏—Ö —É —à–∞—Ä–∏. –†–æ–∑—É–º—ñ–Ω–Ω—è —ó—Ö —Ä–æ–±–æ—Ç–∏ ‚Äî –ø—Ä—è–º–æ–≥–æ –ø–æ—à–∏—Ä–µ–Ω–Ω—è, –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç —Ç–∞ –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –ø–æ—à–∏—Ä–µ–Ω–Ω—è ‚Äî —î –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–º –¥–ª—è —Ç–∏—Ö, —Ö—Ç–æ —Ö–æ—á–µ –≤–∏–π—Ç–∏ –∑–∞ –º–µ–∂—ñ –ø–æ–≤–µ—Ä—Ö–Ω–µ–≤–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –®–Ü.",
              "–°—Ñ–µ—Ä–∞ –µ–≤–æ–ª—é—Ü—ñ–æ–Ω—É–≤–∞–ª–∞ –≤—ñ–¥ –ø—Ä–æ—Å—Ç–∏—Ö –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω—ñ–≤ 1950-—Ö –¥–æ —Å—É—á–∞—Å–Ω–∏—Ö —Ç—Ä–∏–ª—å–π–æ–Ω-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤–∏—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∏—Ö –º–µ—Ä–µ–∂. –ö–æ–∂–µ–Ω –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–∏–π –ø—Ä–æ—Ä–∏–≤ ‚Äî –≤—ñ–¥ –∫–æ–Ω–≤–æ–ª—é—Ü—ñ–π–Ω–∏—Ö —à–∞—Ä—ñ–≤ –¥–ª—è –∑–æ—Ä—É –¥–æ –º–µ—Ö–∞–Ω—ñ–∑–º—ñ–≤ —É–≤–∞–≥–∏ –¥–ª—è –º–æ–≤–∏ ‚Äî —Ä–æ–∑—à–∏—Ä—é–≤–∞–≤ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂. –ó–Ω–∞–Ω–Ω—è —Ü–∏—Ö –æ—Å–Ω–æ–≤ –¥–æ–ø–æ–º–∞–≥–∞—î –∑—Ä–æ–∑—É–º—ñ—Ç–∏, —á–æ–º—É –ø–µ–≤–Ω—ñ –º–æ–¥–µ–ª—ñ –≤—ñ–¥–º—ñ–Ω–Ω—ñ –≤ –ø–µ–≤–Ω–∏—Ö –∑–∞–¥–∞—á–∞—Ö —ñ —è–∫—ñ —Å–ø—Ä–∞–≤–∂–Ω—ñ –æ–±–º–µ–∂–µ–Ω–Ω—è \"–®–Ü\"."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Neurons and Weighted Sums",
                "desc": "A neuron computes a weighted sum of its inputs, adds a bias term, then passes the result through an activation function. This simple operation, repeated billions of times across layers, is how neural networks compute.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Network Layers",
                "desc": "Input layer receives raw data, hidden layers extract increasingly abstract features, output layer produces the final prediction. \"Deep\" learning means many hidden layers ‚Äî modern LLMs have 80-120+ layers.",
                "links": [
                  {
                    "title": "LLM",
                    "href": "../level-2/llm.html"
                  }
                ]
              },
              {
                "text": "Activation Functions",
                "desc": "ReLU (most common, simple max(0,x)), GELU (used in transformers, smoother), Sigmoid (squashes to 0-1), Softmax (outputs probability distribution). These introduce nonlinearity ‚Äî without them, the entire network would collapse to a single linear transformation.",
                "links": []
              },
              {
                "text": "Forward Propagation",
                "desc": "Data flows through the network layer by layer ‚Äî each layer transforms its input and passes the result to the next. The final output is a prediction that can be compared to the true answer to compute error.",
                "links": []
              },
              {
                "text": "Loss Functions",
                "desc": "Measuring how wrong the prediction is. Cross-entropy loss for classification, MSE for regression, next-token prediction loss for language models. The entire training process is about minimizing this loss function.",
                "links": [
                  {
                    "title": "Data to Model",
                    "href": "data-to-model.html"
                  }
                ]
              },
              {
                "text": "Backpropagation",
                "desc": "The algorithm that makes learning possible. It computes how much each weight contributed to the error by applying the chain rule of calculus backwards through the network ‚Äî hence \"back\" propagation.",
                "links": [
                  {
                    "title": "Training & Fine-tuning",
                    "href": "training-finetuning.html"
                  }
                ]
              },
              {
                "text": "Gradient Descent Optimization",
                "desc": "Adjusting weights in the direction that reduces loss. Adam optimizer (used by almost all modern models) adapts learning rates per-parameter. Learning rate scheduling, warmup, and weight decay are critical training hyperparameters.",
                "links": [
                  {
                    "title": "Model Optimization",
                    "href": "model-optimization.html"
                  }
                ]
              },
              {
                "text": "Convolutional Networks (CNNs)",
                "desc": "Specialized for spatial data like images. Convolutional filters slide across the input detecting edges, textures, and patterns. Still used in vision AI but increasingly replaced by Vision Transformers (ViT).",
                "links": [
                  {
                    "title": "Multimodal AI",
                    "href": "../level-1/multimodal-ai.html"
                  }
                ]
              },
              {
                "text": "Recurrent Networks (RNNs, LSTMs)",
                "desc": "Designed for sequential data ‚Äî text, time series, audio. They maintain a hidden state that carries information across time steps. Largely replaced by Transformers which process sequences in parallel.",
                "links": [
                  {
                    "title": "Model Types",
                    "href": "model-types.html"
                  }
                ]
              },
              {
                "text": "The Transformer Architecture",
                "desc": "The 2017 breakthrough that powers all modern LLMs. Self-attention allows each token to attend to every other token in the sequence, capturing long-range dependencies that RNNs struggled with. Multi-head attention runs several attention computations in parallel.",
                "links": [
                  {
                    "title": "Model Types",
                    "href": "model-types.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–ù–µ–π—Ä–æ–Ω–∏ —Ç–∞ –∑–≤–∞–∂–µ–Ω—ñ —Å—É–º–∏",
                "desc": "–ù–µ–π—Ä–æ–Ω –æ–±—á–∏—Å–ª—é—î –∑–≤–∞–∂–µ–Ω—É —Å—É–º—É –≤—Ö–æ–¥—ñ–≤, –¥–æ–¥–∞—î –∑–º—ñ—â–µ–Ω–Ω—è, –ø–æ—Ç—ñ–º –ø—Ä–æ–ø—É—Å–∫–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü—ñ—é –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó. –¶—è –ø—Ä–æ—Å—Ç–∞ –æ–ø–µ—Ä–∞—Ü—ñ—è, –ø–æ–≤—Ç–æ—Ä–µ–Ω–∞ –º—ñ–ª—å—è—Ä–¥–∏ —Ä–∞–∑—ñ–≤ —á–µ—Ä–µ–∑ —à–∞—Ä–∏, —î –æ—Å–Ω–æ–≤–æ—é –æ–±—á–∏—Å–ª–µ–Ω—å –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "–®–∞—Ä–∏ –º–µ—Ä–µ–∂—ñ",
                "desc": "–í—Ö—ñ–¥–Ω–∏–π —à–∞—Ä –ø—Ä–∏–π–º–∞—î —Å–∏—Ä—ñ –¥–∞–Ω—ñ, –ø—Ä–∏—Ö–æ–≤–∞–Ω—ñ —à–∞—Ä–∏ –≤–∏—Ç—è–≥—É—é—Ç—å –¥–µ–¥–∞–ª—ñ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ñ—à—ñ –æ–∑–Ω–∞–∫–∏, –≤–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä –≤–∏–¥–∞—î —Ñ—ñ–Ω–∞–ª—å–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è. \"–ì–ª–∏–±–æ–∫–µ\" –Ω–∞–≤—á–∞–Ω–Ω—è –æ–∑–Ω–∞—á–∞—î –±–∞–≥–∞—Ç–æ –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —à–∞—Ä—ñ–≤ ‚Äî —Å—É—á–∞—Å–Ω—ñ LLM –º–∞—é—Ç—å 80-120+ —à–∞—Ä—ñ–≤.",
                "links": [
                  {
                    "title": "LLM",
                    "href": "../level-2/llm.html"
                  }
                ]
              },
              {
                "text": "–§—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó",
                "desc": "ReLU (–Ω–∞–π–ø–æ—à–∏—Ä–µ–Ω—ñ—à–∞, –ø—Ä–æ—Å—Ç–∏–π max(0,x)), GELU (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö, –ø–ª–∞–≤–Ω—ñ—à–∞), Sigmoid (—Å—Ç–∏—Å–∫–∞—î –¥–æ 0-1), Softmax (–≤–∏–¥–∞—î —Ä–æ–∑–ø–æ–¥—ñ–ª –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π). –í–æ–Ω–∏ –≤–≤–æ–¥—è—Ç—å –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å ‚Äî –±–µ–∑ –Ω–∏—Ö –≤—Å—è –º–µ—Ä–µ–∂–∞ –∑–≤–µ–ª–∞—Å—å –±–∏ –¥–æ –æ–¥–Ω–æ–≥–æ –ª—ñ–Ω—ñ–π–Ω–æ–≥–æ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è.",
                "links": []
              },
              {
                "text": "–ü—Ä—è–º–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è",
                "desc": "–î–∞–Ω—ñ –ø—Ä–æ—Ö–æ–¥—è—Ç—å —á–µ—Ä–µ–∑ –º–µ—Ä–µ–∂—É —à–∞—Ä –∑–∞ —à–∞—Ä–æ–º ‚Äî –∫–æ–∂–µ–Ω —à–∞—Ä —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º—É—î —Å–≤—ñ–π –≤—Ö—ñ–¥ —ñ –ø–µ—Ä–µ–¥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É. –§—ñ–Ω–∞–ª—å–Ω–∏–π –≤–∏—Ö—ñ–¥ ‚Äî –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è, —è–∫–µ –ø–æ—Ä—ñ–≤–Ω—é—î—Ç—å—Å—è –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏.",
                "links": []
              },
              {
                "text": "–§—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç",
                "desc": "–í–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è —Ö–∏–±–Ω–µ. Cross-entropy –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó, MSE –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó, –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –º–æ–≤–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π. –í–µ—Å—å –ø—Ä–æ—Ü–µ—Å –Ω–∞–≤—á–∞–Ω–Ω—è ‚Äî —Ü–µ –º—ñ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—è —Ü—ñ—î—ó —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç.",
                "links": [
                  {
                    "title": "–í—ñ–¥ –¥–∞–Ω–∏—Ö –¥–æ –º–æ–¥–µ–ª—ñ",
                    "href": "data-to-model.html"
                  }
                ]
              },
              {
                "text": "–ó–≤–æ—Ä–æ—Ç–Ω–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è",
                "desc": "–ê–ª–≥–æ—Ä–∏—Ç–º, —â–æ —Ä–æ–±–∏—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–∂–ª–∏–≤–∏–º. –í—ñ–Ω –æ–±—á–∏—Å–ª—é—î –≤–Ω–µ—Å–æ–∫ –∫–æ–∂–Ω–æ—ó –≤–∞–≥–∏ –≤ –ø–æ–º–∏–ª–∫—É, –∑–∞—Å—Ç–æ—Å–æ–≤—É—é—á–∏ –ª–∞–Ω—Ü—é–≥–æ–≤–µ –ø—Ä–∞–≤–∏–ª–æ —á–∏—Å–ª–µ–Ω–Ω—è –Ω–∞–∑–∞–¥ —á–µ—Ä–µ–∑ –º–µ—Ä–µ–∂—É ‚Äî –∑–≤—ñ–¥—Å–∏ \"–∑–≤–æ—Ä–æ—Ç–Ω–µ\" –ø–æ—à–∏—Ä–µ–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥",
                    "href": "training-finetuning.html"
                  }
                ]
              },
              {
                "text": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–º —Å–ø—É—Å–∫–æ–º",
                "desc": "–ö–æ—Ä–µ–∫—Ü—ñ—è –≤–∞–≥ —É –Ω–∞–ø—Ä—è–º–∫—É –∑–º–µ–Ω—à–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç. –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä Adam (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –º–∞–π–∂–µ –≤—Å—ñ–º–∞ —Å—É—á–∞—Å–Ω–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏) –∞–¥–∞–ø—Ç—É—î —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞. –ü–ª–∞–Ω—É–≤–∞–Ω–Ω—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ, –ø—Ä–æ–≥—Ä—ñ–≤ —Ç–∞ –∑–∞—Ç—É—Ö–∞–Ω–Ω—è –≤–∞–≥ ‚Äî –∫—Ä–∏—Ç–∏—á–Ω—ñ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏.",
                "links": [
                  {
                    "title": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π",
                    "href": "model-optimization.html"
                  }
                ]
              },
              {
                "text": "–ö–æ–Ω–≤–æ–ª—é—Ü—ñ–π–Ω—ñ –º–µ—Ä–µ–∂—ñ (CNN)",
                "desc": "–°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö —è–∫ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è. –ö–æ–Ω–≤–æ–ª—é—Ü—ñ–π–Ω—ñ —Ñ—ñ–ª—å—Ç—Ä–∏ –∫–æ–≤–∑–∞—é—Ç—å –ø–æ –≤—Ö–æ–¥—É, –≤–∏—è–≤–ª—è—é—á–∏ –∫—Ä–∞—ó, —Ç–µ–∫—Å—Ç—É—Ä–∏ —Ç–∞ –ø–∞—Ç–µ—Ä–Ω–∏. –í—Å–µ —â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è —É –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–æ–º—É –∑–æ—Ä—ñ, –∞–ª–µ –∑–∞–º—ñ–Ω—é—é—Ç—å—Å—è Vision Transformers (ViT).",
                "links": [
                  {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∏–π –®–Ü",
                    "href": "../level-1/multimodal-ai.html"
                  }
                ]
              },
              {
                "text": "–†–µ–∫—É—Ä–µ–Ω—Ç–Ω—ñ –º–µ—Ä–µ–∂—ñ (RNN, LSTM)",
                "desc": "–°—Ç–≤–æ—Ä–µ–Ω—ñ –¥–ª—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∏—Ö –¥–∞–Ω–∏—Ö ‚Äî —Ç–µ–∫—Å—Ç, —á–∞—Å–æ–≤—ñ —Ä—è–¥–∏, –∞—É–¥—ñ–æ. –í–æ–Ω–∏ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏–π —Å—Ç–∞–Ω, —â–æ –Ω–µ—Å–µ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é —á–µ—Ä–µ–∑ —á–∞—Å–æ–≤—ñ –∫—Ä–æ–∫–∏. –ó–¥–µ–±—ñ–ª—å—à–æ–≥–æ –∑–∞–º—ñ–Ω–µ–Ω—ñ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º–∏, —è–∫—ñ –æ–±—Ä–æ–±–ª—è—é—Ç—å –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ.",
                "links": [
                  {
                    "title": "–¢–∏–ø–∏ –º–æ–¥–µ–ª–µ–π",
                    "href": "model-types.html"
                  }
                ]
              },
              {
                "text": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ Transformer",
                "desc": "–ü—Ä–æ—Ä–∏–≤ 2017 —Ä–æ–∫—É, —â–æ –ª–µ–∂–∏—Ç—å –≤ –æ—Å–Ω–æ–≤—ñ –≤—Å—ñ—Ö —Å—É—á–∞—Å–Ω–∏—Ö LLM. –°–∞–º–æ—É–≤–∞–≥–∞ –¥–æ–∑–≤–æ–ª—è—î –∫–æ–∂–Ω–æ–º—É —Ç–æ–∫–µ–Ω—É –∑–≤–µ—Ä—Ç–∞—Ç–∏ —É–≤–∞–≥—É –Ω–∞ –∫–æ–∂–µ–Ω —ñ–Ω—à–∏–π –≤ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ, –∑–∞—Ö–æ–ø–ª—é—é—á–∏ –¥–∞–ª–µ–∫–æ—Å—è–∂–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ, –∑ —è–∫–∏–º–∏ RNN –º–∞–ª–∏ –ø—Ä–æ–±–ª–µ–º–∏. –ë–∞–≥–∞—Ç–æ–≥–æ–ª–æ–≤–∞ —É–≤–∞–≥–∞ –∑–∞–ø—É—Å–∫–∞—î –∫—ñ–ª—å–∫–∞ –æ–±—á–∏—Å–ª–µ–Ω—å —É–≤–∞–≥–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ.",
                "links": [
                  {
                    "title": "–¢–∏–ø–∏ –º–æ–¥–µ–ª–µ–π",
                    "href": "model-types.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Backpropagation",
                "def": "Algorithm for computing how each weight contributes to the error by applying the chain rule backwards through the network."
              },
              {
                "term": "Gradient Descent",
                "def": "Optimization algorithm that iteratively adjusts weights in the direction that reduces error, using computed gradients."
              },
              {
                "term": "Self-Attention",
                "def": "Mechanism where each element in a sequence computes relevance scores with every other element, enabling context-aware processing."
              },
              {
                "term": "Activation Function",
                "def": "Nonlinear function applied after weighted sums ‚Äî without it, neural networks could only model linear relationships."
              }
            ],
            "uk": [
              {
                "term": "–ó–≤–æ—Ä–æ—Ç–Ω–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è",
                "def": "–ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≤–Ω–µ—Å–∫—É –∫–æ–∂–Ω–æ—ó –≤–∞–≥–∏ –≤ –ø–æ–º–∏–ª–∫—É —á–µ—Ä–µ–∑ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ª–∞–Ω—Ü—é–≥–æ–≤–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞ –Ω–∞–∑–∞–¥ —á–µ—Ä–µ–∑ –º–µ—Ä–µ–∂—É."
              },
              {
                "term": "–ì—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫",
                "def": "–ê–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó, —â–æ —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –∫–æ—Ä–µ–≥—É—î –≤–∞–≥–∏ —É –Ω–∞–ø—Ä—è–º–∫—É –∑–º–µ–Ω—à–µ–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –æ–±—á–∏—Å–ª–µ–Ω—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏."
              },
              {
                "term": "–°–∞–º–æ—É–≤–∞–≥–∞",
                "def": "–ú–µ—Ö–∞–Ω—ñ–∑–º, –¥–µ –∫–æ–∂–µ–Ω –µ–ª–µ–º–µ–Ω—Ç –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –æ–±—á–∏—Å–ª—é—î –æ—Ü—ñ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ –∑ –∫–æ–∂–Ω–∏–º —ñ–Ω—à–∏–º –µ–ª–µ–º–µ–Ω—Ç–æ–º –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–ª–µ–∂–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏."
              },
              {
                "term": "–§—É–Ω–∫—Ü—ñ—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó",
                "def": "–ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è, —â–æ –∑–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –ø—ñ—Å–ª—è –∑–≤–∞–∂–µ–Ω–∏—Ö —Å—É–º ‚Äî –±–µ–∑ –Ω–µ—ó –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂—ñ –º–æ–≥–ª–∏ –± –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ –ª–∏—à–µ –ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ."
              }
            ]
          },
          "tips": {
            "en": [
              "You don't need to code neural networks from scratch ‚Äî but understanding backpropagation and gradient descent explains why models sometimes fail to learn",
              "When a model \"forgets\" things during fine-tuning (catastrophic forgetting), it's because gradient updates for new data override weights learned for old data",
              "Visualize neural network layers with tools like Netron or TensorBoard to build intuition about how data transforms through the architecture"
            ],
            "uk": [
              "–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–∏—Å–∞—Ç–∏ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂—ñ –∑ –Ω—É–ª—è ‚Äî –∞–ª–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –ø–æ—à–∏—Ä–µ–Ω–Ω—è —Ç–∞ –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫—É –ø–æ—è—Å–Ω—é—î, —á–æ–º—É –º–æ–¥–µ–ª—ñ —ñ–Ω–æ–¥—ñ –Ω–µ –Ω–∞–≤—á–∞—é—Ç—å—Å—è",
              "–ö–æ–ª–∏ –º–æ–¥–µ–ª—å \"–∑–∞–±—É–≤–∞—î\" –ø—Ä–∏ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥—É (–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ñ—á–Ω–µ –∑–∞–±—É–≤–∞–Ω–Ω—è), —Ü–µ —Ç–æ–º—É —â–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ –¥–ª—è –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö –ø–µ—Ä–µ–∑–∞–ø–∏—Å—É—é—Ç—å –≤–∞–≥–∏ —Å—Ç–∞—Ä–∏—Ö",
              "–í—ñ–∑—É–∞–ª—ñ–∑—É–π—Ç–µ —à–∞—Ä–∏ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —è–∫ Netron –∞–±–æ TensorBoard –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ —ñ–Ω—Ç—É—ó—Ü—ñ—ó –ø—Ä–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É"
            ]
          },
          "related": [
            "Models",
            "Video Content"
          ]
        },
        {
          "slug": "data-to-model",
          "title": {
            "en": "Data to Model",
            "uk": "–í—ñ–¥ –¥–∞–Ω–∏—Ö –¥–æ –º–æ–¥–µ–ª—ñ"
          },
          "desc": {
            "en": "The complete pipeline from raw data to a trained model.",
            "uk": "–ü–æ–≤–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω –≤—ñ–¥ —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö –¥–æ –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ."
          },
          "overview": {
            "en": [
              "The journey from raw data to a working AI model involves a complex pipeline of collection, cleaning, preprocessing, training, and evaluation. Data quality is often more important than model architecture ‚Äî the AI community saying \"garbage in, garbage out\" has never been more relevant. Understanding this pipeline helps you appreciate why some models are better than others and how to create effective fine-tuned models.",
              "The data pipeline is where the real competitive advantage lies. Companies like OpenAI, Anthropic, and Google invest heavily in data curation ‚Äî not just finding more data, but finding better data. Filtering, deduplication, and synthetic data generation have become entire subdisciplines as the AI community realizes that the quality ceiling of a model is set by its training data."
            ],
            "uk": [
              "–®–ª—è—Ö –≤—ñ–¥ —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö –¥–æ –ø—Ä–∞—Ü—é—é—á–æ—ó –º–æ–¥–µ–ª—ñ –®–Ü –≤–∫–ª—é—á–∞—î —Å–∫–ª–∞–¥–Ω–∏–π –ø–∞–π–ø–ª–∞–π–Ω –∑–±–æ—Ä—É, –æ—á–∏—â–µ–Ω–Ω—è, –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ—ó –æ–±—Ä–æ–±–∫–∏, –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∫–∏. –Ø–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö —á–∞—Å—Ç–æ –≤–∞–∂–ª–∏–≤—ñ—à–∞ –∑–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª—ñ ‚Äî –≤–∏—Å–ª—ñ–≤ —Å–ø—ñ–ª—å–Ω–æ—Ç–∏ –®–Ü \"—Å–º—ñ—Ç—Ç—è –Ω–∞ –≤—Ö–æ–¥—ñ ‚Äî —Å–º—ñ—Ç—Ç—è –Ω–∞ –≤–∏—Ö–æ–¥—ñ\" –Ω—ñ–∫–æ–ª–∏ –Ω–µ –±—É–≤ –±—ñ–ª—å—à –∞–∫—Ç—É–∞–ª—å–Ω–∏–º. –†–æ–∑—É–º—ñ–Ω–Ω—è —Ü—å–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω—É –¥–æ–ø–æ–º–∞–≥–∞—î –∑—Ä–æ–∑—É–º—ñ—Ç–∏, —á–æ–º—É –¥–µ—è–∫—ñ –º–æ–¥–µ–ª—ñ –∫—Ä–∞—â—ñ –∑–∞ —ñ–Ω—à–∏—Ö.",
              "–ü–∞–π–ø–ª–∞–π–Ω –¥–∞–Ω–∏—Ö ‚Äî —Ü–µ –¥–µ —Å–ø—Ä–∞–≤–∂–Ω—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞ –ø–µ—Ä–µ–≤–∞–≥–∞. –ö–æ–º–ø–∞–Ω—ñ—ó —è–∫ OpenAI, Anthropic —Ç–∞ Google —ñ–Ω–≤–µ—Å—Ç—É—é—Ç—å –º–∞—Å—à—Ç–∞–±–Ω–æ –≤ –∫—É—Ä–∞—Ü—ñ—é –¥–∞–Ω–∏—Ö ‚Äî –Ω–µ –ø—Ä–æ—Å—Ç–æ –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –±—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö, –∞ –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –∫—Ä–∞—â–∏—Ö. –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è, –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—è —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö —Å—Ç–∞–ª–∏ —Ü—ñ–ª–∏–º–∏ –ø—ñ–¥–¥–∏—Å—Ü–∏–ø–ª—ñ–Ω–∞–º–∏, –±–æ —Å–ø—ñ–ª—å–Ω–æ—Ç–∞ –®–Ü —É—Å–≤—ñ–¥–æ–º–∏–ª–∞, —â–æ —Å—Ç–µ–ª—è —è–∫–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è —ó—ó –Ω–∞–≤—á–∞–ª—å–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Data Collection at Scale",
                "desc": "Web crawling (Common Crawl ‚Äî petabytes of web pages), digitized books, GitHub code repositories, scientific papers (arXiv, PubMed), Wikipedia. The scale is staggering: trillions of tokens from billions of documents.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Data Cleaning",
                "desc": "Removing duplicates, low-quality content, boilerplate HTML, personally identifiable information (PII), and machine-generated spam. Up to 90% of raw crawled data may be discarded during cleaning.",
                "links": []
              },
              {
                "text": "Deduplication",
                "desc": "Exact and near-duplicate removal to prevent memorization and data leakage. MinHash, SimHash, and suffix array techniques identify similar content. Critical for preventing models from memorizing and regurgitating specific texts.",
                "links": []
              },
              {
                "text": "Content Filtering",
                "desc": "Removing harmful, toxic, or copyrighted content from training data. Classifier-based filtering, keyword blocklists, and domain-level decisions. Balancing thorough filtering with preserving data diversity is a core challenge.",
                "links": [
                  {
                    "title": "AI Safety",
                    "href": "../level-5/ai-safety.html"
                  }
                ]
              },
              {
                "text": "Tokenization and Preprocessing",
                "desc": "Converting raw text into tokens the model can process. BPE (Byte Pair Encoding) and SentencePiece are dominant methods. Vocabulary size (32K-100K+ tokens) trades memory for encoding efficiency. Multilingual tokenizers must balance all languages.",
                "links": [
                  {
                    "title": "Token",
                    "href": "../level-2/token.html"
                  }
                ]
              },
              {
                "text": "Dataset Formats",
                "desc": "JSONL (human-readable, line-per-example), Parquet (columnar, compressed), Arrow (in-memory, zero-copy). Efficient storage is critical when datasets reach terabytes. Hugging Face datasets library standardizes access.",
                "links": [
                  {
                    "title": "Tools & Libraries",
                    "href": "../level-4/base-tools.html"
                  }
                ]
              },
              {
                "text": "Data Quality vs Quantity",
                "desc": "Smaller high-quality datasets can outperform larger noisy ones. Microsoft's Phi models proved this: carefully curated \"textbook quality\" data trained models that punched far above their parameter count.",
                "links": [
                  {
                    "title": "Model Optimization",
                    "href": "model-optimization.html"
                  }
                ]
              },
              {
                "text": "Synthetic Data Generation",
                "desc": "Using existing AI models to generate training data for specific capabilities. Self-instruct, Evol-Instruct, and distillation pipelines create millions of instruction-response pairs. Enables training on domains where real data is scarce or expensive.",
                "links": [
                  {
                    "title": "LLM",
                    "href": "../level-2/llm.html"
                  }
                ]
              },
              {
                "text": "Human Data Annotation",
                "desc": "Human labelers creating supervised examples for fine-tuning and RLHF. Annotation quality varies widely ‚Äî detailed guidelines, multiple annotators per example, and inter-annotator agreement checks are essential.",
                "links": [
                  {
                    "title": "Training & Fine-tuning",
                    "href": "training-finetuning.html"
                  }
                ]
              },
              {
                "text": "Open Datasets",
                "desc": "The Pile (EleutherAI), RedPajama (Together AI), FineWeb (HuggingFace), SlimPajama ‚Äî open datasets that power open-source models. Understanding their composition explains model capabilities and biases.",
                "links": [
                  {
                    "title": "Data Classification",
                    "href": "../level-1/data-classification.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–ó–±—ñ—Ä –¥–∞–Ω–∏—Ö —É –º–∞—Å—à—Ç–∞–±—ñ",
                "desc": "–í–µ–±-–∫—Ä–∞—É–ª—ñ–Ω–≥ (Common Crawl ‚Äî –ø–µ—Ç–∞–±–∞–π—Ç–∏ –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–æ–∫), –æ—Ü–∏—Ñ—Ä–æ–≤–∞–Ω—ñ –∫–Ω–∏–≥–∏, —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—ó –∫–æ–¥—É GitHub, –Ω–∞—É–∫–æ–≤—ñ —Å—Ç–∞—Ç—Ç—ñ (arXiv, PubMed), –í—ñ–∫—ñ–ø–µ–¥—ñ—è. –ú–∞—Å—à—Ç–∞–± –≤—Ä–∞–∂–∞—é—á–∏–π: —Ç—Ä–∏–ª—å–π–æ–Ω–∏ —Ç–æ–∫–µ–Ω—ñ–≤ –∑ –º—ñ–ª—å—è—Ä–¥—ñ–≤ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "–û—á–∏—â–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö",
                "desc": "–í–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤, –Ω–∏–∑—å–∫–æ—è–∫—ñ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É, —à–∞–±–ª–æ–Ω–Ω–æ–≥–æ HTML, –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö (PII) —Ç–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ —Å–ø–∞–º—É. –î–æ 90% —Å–∏—Ä–∏—Ö –∫—Ä–∞—É–ª–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö –º–æ–∂–µ –±—É—Ç–∏ –≤—ñ–¥–∫–∏–Ω—É—Ç–æ –ø—ñ–¥ —á–∞—Å –æ—á–∏—â–µ–Ω–Ω—è.",
                "links": []
              },
              {
                "text": "–î–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—è",
                "desc": "–í–∏–¥–∞–ª–µ–Ω–Ω—è —Ç–æ—á–Ω–∏—Ö —Ç–∞ –±–ª–∏–∑—å–∫–∏—Ö –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è –∑–∞–ø–∞–º'—è—Ç–æ–≤—É–≤–∞–Ω–Ω—é. –¢–µ—Ö–Ω—ñ–∫–∏ MinHash, SimHash —Ç–∞ —Å—É—Ñ—ñ–∫—Å–Ω–∏—Ö –º–∞—Å–∏–≤—ñ–≤ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫—É—é—Ç—å —Å—Ö–æ–∂–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç. –ö—Ä–∏—Ç–∏—á–Ω–∞ –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è –∑–∞—É—á—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–ª—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤.",
                "links": []
              },
              {
                "text": "–§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∫–æ–Ω—Ç–µ–Ω—Ç—É",
                "desc": "–í–∏–¥–∞–ª–µ–Ω–Ω—è —à–∫—ñ–¥–ª–∏–≤–æ–≥–æ, —Ç–æ–∫—Å–∏—á–Ω–æ–≥–æ –∞–±–æ –∑–∞—Ö–∏—â–µ–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä—Å—å–∫–∏–º –ø—Ä–∞–≤–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç—É. –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–Ω–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è, –±–ª–æ–∫–ª—ñ—Å—Ç –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ —Ç–∞ —Ä—ñ—à–µ–Ω–Ω—è –Ω–∞ —Ä—ñ–≤–Ω—ñ –¥–æ–º–µ–Ω—ñ–≤. –ë–∞–ª–∞–Ω—Å —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –∑—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç—Ç—è ‚Äî –∫–ª—é—á–æ–≤–∏–π –≤–∏–∫–ª–∏–∫.",
                "links": [
                  {
                    "title": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü",
                    "href": "../level-5/ai-safety.html"
                  }
                ]
              },
              {
                "text": "–¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è —Ç–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞",
                "desc": "–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É —É —Ç–æ–∫–µ–Ω–∏. BPE (Byte Pair Encoding) —Ç–∞ SentencePiece ‚Äî –¥–æ–º—ñ–Ω—É—é—á—ñ –º–µ—Ç–æ–¥–∏. –†–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞ (32K-100K+ —Ç–æ–∫–µ–Ω—ñ–≤) –æ–±–º—ñ–Ω—é—î –ø–∞–º'—è—Ç—å –Ω–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –∫–æ–¥—É–≤–∞–Ω–Ω—è. –ú—É–ª—å—Ç–∏–º–æ–≤–Ω—ñ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–∏ –ø–æ–≤–∏–Ω–Ω—ñ –±–∞–ª–∞–Ω—Å—É–≤–∞—Ç–∏ –≤—Å—ñ –º–æ–≤–∏.",
                "links": [
                  {
                    "title": "–¢–æ–∫–µ–Ω",
                    "href": "../level-2/token.html"
                  }
                ]
              },
              {
                "text": "–§–æ—Ä–º–∞—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤",
                "desc": "JSONL (–ª—é–¥–∏–Ω–æ-—á–∏—Ç–∞–±–µ–ª—å–Ω–∏–π), Parquet (–∫–æ–ª–æ–Ω–∫–æ–≤–∏–π, —Å—Ç–∏—Å–Ω—É—Ç–∏–π), Arrow (—É –ø–∞–º'—è—Ç—ñ, zero-copy). –ï—Ñ–µ–∫—Ç–∏–≤–Ω–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –∫—Ä–∏—Ç–∏—á–Ω–µ –ø—Ä–∏ —Ç–µ—Ä–∞–±–∞–π—Ç–Ω–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö. –ë—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ Hugging Face datasets —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑—É—î –¥–æ—Å—Ç—É–ø.",
                "links": [
                  {
                    "title": "–Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —Ç–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏",
                    "href": "../level-4/base-tools.html"
                  }
                ]
              },
              {
                "text": "–Ø–∫—ñ—Å—Ç—å vs –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö",
                "desc": "–ú–µ–Ω—à—ñ —è–∫—ñ—Å–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ –º–æ–∂—É—Ç—å –ø–µ—Ä–µ–≤–µ—Ä—à–∏—Ç–∏ –±—ñ–ª—å—à—ñ –∑–∞—à—É–º–ª–µ–Ω—ñ. –ú–æ–¥–µ–ª—ñ Phi –≤—ñ–¥ Microsoft —Ü–µ –¥–æ–≤–µ–ª–∏: —Ä–µ—Ç–µ–ª—å–Ω–æ –∫—É—Ä–∞—Ç–æ—Ä—Å—å–∫—ñ –¥–∞–Ω—ñ \"–ø—ñ–¥—Ä—É—á–Ω–∏–∫–æ–≤–æ—ó —è–∫–æ—Å—Ç—ñ\" –Ω–∞–≤—á–∏–ª–∏ –º–æ–¥–µ–ª—ñ, —â–æ –ø–µ—Ä–µ–≤–µ—Ä—à—É–≤–∞–ª–∏ –∑–Ω–∞—á–Ω–æ –±—ñ–ª—å—à—ñ.",
                "links": [
                  {
                    "title": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π",
                    "href": "model-optimization.html"
                  }
                ]
              },
              {
                "text": "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö",
                "desc": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö –®–Ü-–º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö. Self-instruct, Evol-Instruct —Ç–∞ –ø–∞–π–ø–ª–∞–π–Ω–∏ –¥–∏—Å—Ç–∏–ª—è—Ü—ñ—ó —Å—Ç–≤–æ—Ä—é—é—Ç—å –º—ñ–ª—å–π–æ–Ω–∏ –ø–∞—Ä —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è-–≤—ñ–¥–ø–æ–≤—ñ–¥—å. –î–æ–∑–≤–æ–ª—è—î –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –¥–æ–º–µ–Ω–∞—Ö –¥–µ —Ä–µ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –¥–µ—Ñ—ñ—Ü–∏—Ç–Ω—ñ.",
                "links": [
                  {
                    "title": "LLM",
                    "href": "../level-2/llm.html"
                  }
                ]
              },
              {
                "text": "–õ—é–¥—Å—å–∫–∞ –∞–Ω–æ—Ç–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö",
                "desc": "–õ—é–¥—Å—å–∫—ñ —Ä–æ–∑–º—ñ—Ç–Ω–∏–∫–∏ —Å—Ç–≤–æ—Ä—é—é—Ç—å –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–ª—è —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥—É —Ç–∞ RLHF. –Ø–∫—ñ—Å—Ç—å –∞–Ω–æ—Ç–∞—Ü—ñ—ó –≤–∞—Ä—ñ—é—î—Ç—å—Å—è ‚Äî –¥–µ—Ç–∞–ª—å–Ω—ñ –≥–∞–π–¥–ª–∞–π–Ω–∏, –∫—ñ–ª—å–∫–∞ —Ä–æ–∑–º—ñ—Ç–Ω–∏–∫—ñ–≤ –Ω–∞ –ø—Ä–∏–∫–ª–∞–¥ —Ç–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —É–∑–≥–æ–¥–∂–µ–Ω–æ—Å—Ç—ñ –º—ñ–∂ —Ä–æ–∑–º—ñ—Ç–Ω–∏–∫–∞–º–∏ —î –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–º–∏.",
                "links": [
                  {
                    "title": "–ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥",
                    "href": "training-finetuning.html"
                  }
                ]
              },
              {
                "text": "–í—ñ–¥–∫—Ä–∏—Ç—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏",
                "desc": "The Pile (EleutherAI), RedPajama (Together AI), FineWeb (HuggingFace), SlimPajama ‚Äî –≤—ñ–¥–∫—Ä–∏—Ç—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ –¥–ª—è open-source –º–æ–¥–µ–ª–µ–π. –†–æ–∑—É–º—ñ–Ω–Ω—è —ó—Ö —Å–∫–ª–∞–¥—É –ø–æ—è—Å–Ω—é—î –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —Ç–∞ —É–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π.",
                "links": [
                  {
                    "title": "–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö",
                    "href": "../level-1/data-classification.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Common Crawl",
                "def": "Massive open web archive containing petabytes of web pages, used as primary data source for training most LLMs."
              },
              {
                "term": "Synthetic Data",
                "def": "Training data generated by AI models rather than collected from real sources, enabling training on scarce domains."
              },
              {
                "term": "Data Deduplication",
                "def": "Removing duplicate or near-duplicate examples using hashing techniques to prevent memorization and improve quality."
              },
              {
                "term": "BPE Tokenization",
                "def": "Byte Pair Encoding ‚Äî the dominant method for splitting text into sub-word tokens that models can process."
              }
            ],
            "uk": [
              {
                "term": "Common Crawl",
                "def": "–ú–∞—Å–∏–≤–Ω–∏–π –≤—ñ–¥–∫—Ä–∏—Ç–∏–π –≤–µ–±-–∞—Ä—Ö—ñ–≤ –∑ –ø–µ—Ç–∞–±–∞–π—Ç–∞–º–∏ –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–æ–∫, –æ—Å–Ω–æ–≤–Ω–µ –¥–∂–µ—Ä–µ–ª–æ –¥–∞–Ω–∏—Ö –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –±—ñ–ª—å—à–æ—Å—Ç—ñ LLM."
              },
              {
                "term": "–°–∏–Ω—Ç–µ—Ç–∏—á–Ω—ñ –¥–∞–Ω—ñ",
                "def": "–ù–∞–≤—á–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ, –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—è–º–∏ –®–Ü –∑–∞–º—ñ—Å—Ç—å –∑–±–æ—Ä—É –∑ —Ä–µ–∞–ª—å–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª, —â–æ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –¥–µ—Ñ—ñ—Ü–∏—Ç–Ω–∏—Ö –¥–æ–º–µ–Ω–∞—Ö."
              },
              {
                "term": "–î–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö",
                "def": "–í–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –∞–±–æ –±–ª–∏–∑—å–∫–∏—Ö –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —Ö–µ—à—É–≤–∞–Ω–Ω—è–º –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è –∑–∞–ø–∞–º'—è—Ç–æ–≤—É–≤–∞–Ω–Ω—è —Ç–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ."
              },
              {
                "term": "BPE —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è",
                "def": "Byte Pair Encoding ‚Äî –¥–æ–º—ñ–Ω—É—é—á–∏–π –º–µ—Ç–æ–¥ —Ä–æ–∑–±–∏—Ç—Ç—è —Ç–µ–∫—Å—Ç—É –Ω–∞ –ø—ñ–¥-—Å–ª–æ–≤–µ—Å–Ω—ñ —Ç–æ–∫–µ–Ω–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –º–æ–¥–µ–ª—è–º–∏."
              }
            ]
          },
          "tips": {
            "en": [
              "When fine-tuning, spend 80% of your time on data preparation and only 20% on training ‚Äî data quality determines the ceiling",
              "Always inspect your training data manually before launching a run ‚Äî even a few corrupted or mislabeled examples can derail the model",
              "For custom datasets, start with 1,000 high-quality examples rather than 100,000 noisy ones ‚Äî iterate on quality before scaling quantity"
            ],
            "uk": [
              "–ü—Ä–∏ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥—É –≤–∏—Ç—Ä–∞—á–∞–π—Ç–µ 80% —á–∞—Å—É –Ω–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–∏—Ö —ñ –ª–∏—à–µ 20% –Ω–∞ –Ω–∞–≤—á–∞–Ω–Ω—è ‚Äî —è–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö –≤–∏–∑–Ω–∞—á–∞—î —Å—Ç–µ–ª—é",
              "–ó–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–≤—ñ—Ä—è–π—Ç–µ –Ω–∞–≤—á–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –≤—Ä—É—á–Ω—É –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º ‚Äî –Ω–∞–≤—ñ—Ç—å –∫—ñ–ª—å–∫–∞ –ø–æ—à–∫–æ–¥–∂–µ–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –º–æ–∂—É—Ç—å –∑—ñ–ø—Å—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å",
              "–î–ª—è –≤–ª–∞—Å–Ω–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ –ø–æ—á–Ω—ñ—Ç—å –∑ 1000 —è–∫—ñ—Å–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –∑–∞–º—ñ—Å—Ç—å 100000 –∑–∞—à—É–º–ª–µ–Ω–∏—Ö ‚Äî —ñ—Ç–µ—Ä—É–π—Ç–µ —è–∫—ñ—Å—Ç—å –ø–µ—Ä–µ–¥ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è–º"
            ]
          },
          "related": [
            "Models",
            "Video Content"
          ]
        },
        {
          "slug": "training-finetuning",
          "title": {
            "en": "Training & Fine-tuning",
            "uk": "–ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥"
          },
          "desc": {
            "en": "How models are trained from scratch and adapted for specific tasks.",
            "uk": "–Ø–∫ –º–æ–¥–µ–ª—ñ –Ω–∞–≤—á–∞—é—Ç—å—Å—è –∑ –Ω—É–ª—è —Ç–∞ –∞–¥–∞–ø—Ç—É—é—Ç—å—Å—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –∑–∞–¥–∞—á."
          },
          "overview": {
            "en": [
              "Model training happens in stages. Pre-training teaches the model general language understanding by predicting next tokens on internet-scale data ‚Äî this costs millions of dollars and requires thousands of GPUs. Fine-tuning then adapts this general model for specific tasks using much smaller, curated datasets. Finally, alignment training (RLHF or DPO) teaches the model to be helpful, honest, and safe.",
              "The rise of parameter-efficient methods like LoRA has democratized fine-tuning ‚Äî you can now adapt a 70B parameter model on a single consumer GPU. This has created a vibrant ecosystem of community-created fine-tunes for specific domains, languages, and use cases. Understanding when to fine-tune vs when to just prompt better is a key skill for AI practitioners."
            ],
            "uk": [
              "–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –µ—Ç–∞–ø–∞–º–∏. –ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è –≤—á–∏—Ç—å –º–æ–¥–µ–ª—å –∑–∞–≥–∞–ª—å–Ω–æ–º—É —Ä–æ–∑—É–º—ñ–Ω–Ω—é –º–æ–≤–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤ –Ω–∞ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞—Å—à—Ç–∞–±–Ω–∏—Ö –¥–∞–Ω–∏—Ö ‚Äî —Ü–µ –∫–æ—à—Ç—É—î –º—ñ–ª—å–π–æ–Ω–∏ –¥–æ–ª–∞—Ä—ñ–≤ —Ç–∞ –≤–∏–º–∞–≥–∞—î —Ç–∏—Å—è—á—ñ GPU. –§–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –ø–æ—Ç—ñ–º –∞–¥–∞–ø—Ç—É—î —Ü—é –∑–∞–≥–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –∑–∞–¥–∞—á –Ω–∞ –∑–Ω–∞—á–Ω–æ –º–µ–Ω—à–∏—Ö –∫—É—Ä–∞—Ç–æ—Ä—Å—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö. –ù–∞—Ä–µ—à—Ç—ñ, –Ω–∞–≤—á–∞–Ω–Ω—è –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è (RLHF –∞–±–æ DPO) –≤—á–∏—Ç—å –º–æ–¥–µ–ª—å –±—É—Ç–∏ –∫–æ—Ä–∏—Å–Ω–æ—é, —á–µ—Å–Ω–æ—é —Ç–∞ –±–µ–∑–ø–µ—á–Ω–æ—é.",
              "–ü–æ—è–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–Ω–æ-–µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤ —è–∫ LoRA –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑—É–≤–∞–ª–∞ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ ‚Äî —Ç–µ–ø–µ—Ä –º–æ–∂–Ω–∞ –∞–¥–∞–ø—Ç—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å –∑ 70B –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –Ω–∞ –æ–¥–Ω–æ–º—É —Å–ø–æ–∂–∏–≤—á–æ–º—É GPU. –¶–µ —Å—Ç–≤–æ—Ä–∏–ª–æ –∂–∏–≤—É –µ–∫–æ—Å–∏—Å—Ç–µ–º—É —Å–ø—ñ–ª—å–Ω–æ—Ç–Ω–∏—Ö —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–≤ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –¥–æ–º–µ–Ω—ñ–≤, –º–æ–≤ —Ç–∞ –≤–∏–ø–∞–¥–∫—ñ–≤ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è. –†–æ–∑—É–º—ñ–Ω–Ω—è –∫–æ–ª–∏ —Ñ–∞–π–Ω-—Ç—é–Ω–∏—Ç–∏ vs –∫–æ–ª–∏ –ø—Ä–æ—Å—Ç–æ –∫—Ä–∞—â–µ –ø—Ä–æ–º–ø—Ç–∏—Ç–∏ ‚Äî –∫–ª—é—á–æ–≤–∞ –Ω–∞–≤–∏—á–∫–∞ –¥–ª—è –ø—Ä–∞–∫—Ç–∏–∫—ñ–≤ –®–Ü."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Pre-training",
                "desc": "Massive compute (thousands of GPUs for months) on trillions of tokens. The model learns language structure, world knowledge, and reasoning by predicting the next token. Costs $10M-$100M+ for frontier models.",
                "links": [
                  {
                    "title": "Data to Model",
                    "href": "data-to-model.html"
                  },
                  {
                    "title": "Hardware",
                    "href": "../level-4/hardware.html"
                  }
                ]
              },
              {
                "text": "Supervised Fine-Tuning (SFT)",
                "desc": "Training on curated instruction-response pairs to teach the model to follow instructions and produce useful outputs. Thousands to millions of examples, typically taking hours to days on a few GPUs.",
                "links": [
                  {
                    "title": "Prompt",
                    "href": "../level-2/prompt.html"
                  }
                ]
              },
              {
                "text": "RLHF",
                "desc": "Reinforcement Learning from Human Feedback ‚Äî humans compare model outputs, a reward model learns their preferences, then the LLM is optimized to maximize that reward. The technique that made ChatGPT work.",
                "links": [
                  {
                    "title": "Alignment",
                    "href": "../level-5/alignment.html"
                  }
                ]
              },
              {
                "text": "DPO",
                "desc": "Direct Preference Optimization ‚Äî a simpler alternative to RLHF that eliminates the separate reward model. Directly optimizes the LLM on preference pairs. Increasingly popular for its stability and simplicity.",
                "links": [
                  {
                    "title": "Alignment",
                    "href": "../level-5/alignment.html"
                  }
                ]
              },
              {
                "text": "LoRA",
                "desc": "Low-Rank Adaptation ‚Äî fine-tuning only small additional matrices (1-5% of parameters) while keeping the original model frozen. Produces tiny adapter files (10-100MB) that can be swapped in and out.",
                "links": [
                  {
                    "title": "Model Optimization",
                    "href": "model-optimization.html"
                  }
                ]
              },
              {
                "text": "QLoRA",
                "desc": "Quantized LoRA ‚Äî combines model quantization (4-bit) with LoRA adapters, enabling fine-tuning of 70B models on a single 24GB GPU. A breakthrough for accessible AI customization.",
                "links": [
                  {
                    "title": "Model Optimization",
                    "href": "model-optimization.html"
                  }
                ]
              },
              {
                "text": "Full Fine-tuning vs PEFT",
                "desc": "Full fine-tuning updates all parameters ‚Äî maximum quality but requires multi-GPU setups and risks catastrophic forgetting. Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA are cheaper and preserve base model knowledge.",
                "links": [
                  {
                    "title": "Neural Networks",
                    "href": "neural-networks.html"
                  }
                ]
              },
              {
                "text": "When to Fine-tune vs Prompt",
                "desc": "Fine-tune for: consistent style/format, domain-specific knowledge, specific output structures. Prompt for: flexible tasks, rapid iteration, no training data available. Fine-tuning is a commitment; prompting is an experiment.",
                "links": [
                  {
                    "title": "Prompting Techniques",
                    "href": "../level-4/prompting-techniques.html"
                  }
                ]
              },
              {
                "text": "Training Cost Spectrum",
                "desc": "Pre-training ($10M+) vs fine-tuning ($100-10K) vs prompting (free/cheap). API-based fine-tuning (OpenAI, Anthropic) costs pennies per example. Self-hosted fine-tuning requires GPU rental ($1-5/hr for A100).",
                "links": [
                  {
                    "title": "API Providers",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              },
              {
                "text": "Evaluation and Iteration",
                "desc": "Measuring fine-tuned model quality with held-out test sets, automated metrics (perplexity, BLEU, ROUGE), and human evaluation. Always compare against the base model to quantify improvement.",
                "links": [
                  {
                    "title": "SOTA",
                    "href": "../level-1/sota.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è",
                "desc": "–ú–∞—Å–∏–≤–Ω—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è (—Ç–∏—Å—è—á—ñ GPU –ø—Ä–æ—Ç—è–≥–æ–º –º—ñ—Å—è—Ü—ñ–≤) –Ω–∞ —Ç—Ä–∏–ª—å–π–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω—ñ–≤. –ú–æ–¥–µ–ª—å –≤–∏–≤—á–∞—î –º–æ–≤–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –∑–Ω–∞–Ω–Ω—è –ø—Ä–æ —Å–≤—ñ—Ç —Ç–∞ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. –ö–æ—à—Ç—É—î $10M-$100M+ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–∏—Ä–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π.",
                "links": [
                  {
                    "title": "–í—ñ–¥ –¥–∞–Ω–∏—Ö –¥–æ –º–æ–¥–µ–ª—ñ",
                    "href": "data-to-model.html"
                  },
                  {
                    "title": "–û–±–ª–∞–¥–Ω–∞–Ω–Ω—è",
                    "href": "../level-4/hardware.html"
                  }
                ]
              },
              {
                "text": "–ö–µ—Ä–æ–≤–∞–Ω–µ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ (SFT)",
                "desc": "–ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –∫—É—Ä–∞—Ç–æ—Ä—Å—å–∫–∏—Ö –ø–∞—Ä–∞—Ö —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è-–≤—ñ–¥–ø–æ–≤—ñ–¥—å –¥–ª—è –≤—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Å–ª—ñ–¥—É–≤–∞—Ç–∏ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è–º. –¢–∏—Å—è—á—ñ –¥–æ –º—ñ–ª—å–π–æ–Ω—ñ–≤ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤, –∑–∞–∑–≤–∏—á–∞–π –≤—ñ–¥ –≥–æ–¥–∏–Ω –¥–æ –¥–Ω—ñ–≤ –Ω–∞ –∫—ñ–ª—å–∫–æ—Ö GPU.",
                "links": [
                  {
                    "title": "–ü—Ä–æ–º–ø—Ç",
                    "href": "../level-2/prompt.html"
                  }
                ]
              },
              {
                "text": "RLHF",
                "desc": "–ù–∞–≤—á–∞–Ω–Ω—è –∑ –ø—ñ–¥–∫—Ä—ñ–ø–ª–µ–Ω–Ω—è–º –∑–∞ –∑–≤–æ—Ä–æ—Ç–Ω–∏–º –∑–≤'—è–∑–∫–æ–º –≤—ñ–¥ –ª—é–¥–µ–π ‚Äî –ª—é–¥–∏ –ø–æ—Ä—ñ–≤–Ω—é—é—Ç—å –≤–∏—Ö–æ–¥–∏ –º–æ–¥–µ–ª—ñ, –º–æ–¥–µ–ª—å –Ω–∞–≥–æ—Ä–æ–¥–∏ –≤–∏–≤—á–∞—î —ó—Ö –ø–µ—Ä–µ–≤–∞–≥–∏, –ø–æ—Ç—ñ–º LLM –æ–ø—Ç–∏–º—ñ–∑—É—î—Ç—å—Å—è –¥–ª—è –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó —Ü—ñ—î—ó –Ω–∞–≥–æ—Ä–æ–¥–∏. –¢–µ—Ö–Ω—ñ–∫–∞, —â–æ –∑—Ä–æ–±–∏–ª–∞ ChatGPT —Ä–æ–±–æ—á–∏–º.",
                "links": [
                  {
                    "title": "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è",
                    "href": "../level-5/alignment.html"
                  }
                ]
              },
              {
                "text": "DPO",
                "desc": "–ü—Ä—è–º–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø–µ—Ä–µ–≤–∞–≥ ‚Äî –ø—Ä–æ—Å—Ç—ñ—à–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ RLHF –±–µ–∑ –æ–∫—Ä–µ–º–æ—ó –º–æ–¥–µ–ª—ñ –Ω–∞–≥–æ—Ä–æ–¥–∏. –ù–∞–ø—Ä—è–º—É –æ–ø—Ç–∏–º—ñ–∑—É—î LLM –Ω–∞ –ø–∞—Ä–∞—Ö –ø–µ—Ä–µ–≤–∞–≥. –î–µ–¥–∞–ª—ñ –ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∞ –∑–∞ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å —Ç–∞ –ø—Ä–æ—Å—Ç–æ—Ç—É.",
                "links": [
                  {
                    "title": "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è",
                    "href": "../level-5/alignment.html"
                  }
                ]
              },
              {
                "text": "LoRA",
                "desc": "Low-Rank Adaptation ‚Äî —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –ª–∏—à–µ –º–∞–ª–∏—Ö –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –º–∞—Ç—Ä–∏—Ü—å (1-5% –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤) –ø—Ä–∏ –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ñ–π –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ–π –º–æ–¥–µ–ª—ñ. –°—Ç–≤–æ—Ä—é—î –∫—Ä–∏—Ö—ñ—Ç–Ω—ñ —Ñ–∞–π–ª–∏ –∞–¥–∞–ø—Ç–µ—Ä—ñ–≤ (10-100–ú–ë), —è–∫—ñ –º–æ–∂–Ω–∞ –ø—ñ–¥–º—ñ–Ω—è—Ç–∏.",
                "links": [
                  {
                    "title": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π",
                    "href": "model-optimization.html"
                  }
                ]
              },
              {
                "text": "QLoRA",
                "desc": "–ö–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–∏–π LoRA ‚Äî –ø–æ—î–¥–Ω—É—î –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—é –º–æ–¥–µ–ª—ñ (4-–±—ñ—Ç) –∑ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏, –¥–æ–∑–≤–æ–ª—è—é—á–∏ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ 70B –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º—É 24–ì–ë GPU. –ü—Ä–æ—Ä–∏–≤ –¥–ª—è –¥–æ—Å—Ç—É–ø–Ω–æ—ó –∫–∞—Å—Ç–æ–º—ñ–∑–∞—Ü—ñ—ó –®–Ü.",
                "links": [
                  {
                    "title": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π",
                    "href": "model-optimization.html"
                  }
                ]
              },
              {
                "text": "–ü–æ–≤–Ω–∏–π —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ vs PEFT",
                "desc": "–ü–æ–≤–Ω–∏–π —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –æ–Ω–æ–≤–ª—é—î –≤—Å—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å, –∞–ª–µ –≤–∏–º–∞–≥–∞—î –º—É–ª—å—Ç–∏-GPU —Ç–∞ —Ä–∏–∑–∏–∫—É—î –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ñ—á–Ω–∏–º –∑–∞–±—É–≤–∞–Ω–Ω—è–º. PEFT –º–µ—Ç–æ–¥–∏ —è–∫ LoRA –¥–µ—à–µ–≤—à—ñ —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å –∑–Ω–∞–Ω–Ω—è –±–∞–∑–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂",
                    "href": "neural-networks.html"
                  }
                ]
              },
              {
                "text": "–ö–æ–ª–∏ —Ñ–∞–π–Ω-—Ç—é–Ω–∏—Ç–∏ vs –ø—Ä–æ–º–ø—Ç–∏—Ç–∏",
                "desc": "–§–∞–π–Ω-—Ç—é–Ω–∏—Ç–∏ –¥–ª—è: —Å—Ç–∞–±—ñ–ª—å–Ω–æ–≥–æ —Å—Ç–∏–ª—é/—Ñ–æ—Ä–º–∞—Ç—É, –¥–æ–º–µ–Ω–Ω–∏—Ö –∑–Ω–∞–Ω—å, —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –≤–∏—Ö–æ–¥—É. –ü—Ä–æ–º–ø—Ç–∏—Ç–∏ –¥–ª—è: –≥–Ω—É—á–∫–∏—Ö –∑–∞–¥–∞—á, —à–≤–∏–¥–∫–æ—ó —ñ—Ç–µ—Ä–∞—Ü—ñ—ó, –≤—ñ–¥—Å—É—Ç–Ω–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö. –§–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ ‚Äî –∑–æ–±–æ–≤'—è–∑–∞–Ω–Ω—è; –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ ‚Äî –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç.",
                "links": [
                  {
                    "title": "–¢–µ—Ö–Ω—ñ–∫–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É",
                    "href": "../level-4/prompting-techniques.html"
                  }
                ]
              },
              {
                "text": "–°–ø–µ–∫—Ç—Ä –≤–∏—Ç—Ä–∞—Ç –Ω–∞–≤—á–∞–Ω–Ω—è",
                "desc": "–ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è ($10M+) vs —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ ($100-10K) vs –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ (–±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–æ). API —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ (OpenAI, Anthropic) –∫–æ—à—Ç—É—î –∫–æ–ø—ñ–π–∫–∏ –∑–∞ –ø—Ä–∏–∫–ª–∞–¥. –í–ª–∞—Å–Ω–∏–π —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –≤–∏–º–∞–≥–∞—î –æ—Ä–µ–Ω–¥–∏ GPU ($1-5/–≥–æ–¥ –∑–∞ A100).",
                "links": [
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              },
              {
                "text": "–û—Ü—ñ–Ω–∫–∞ —Ç–∞ —ñ—Ç–µ—Ä–∞—Ü—ñ—è",
                "desc": "–í–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è —è–∫–æ—Å—Ç—ñ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥—É —Ç–µ—Å—Ç–æ–≤–∏–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏, –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ (–ø–µ—Ä–ø–ª–µ–∫—Å—ñ—è, BLEU, ROUGE) —Ç–∞ –ª—é–¥—Å—å–∫–æ—é –æ—Ü—ñ–Ω–∫–æ—é. –ó–∞–≤–∂–¥–∏ –ø–æ—Ä—ñ–≤–Ω—é–π—Ç–µ –∑ –±–∞–∑–æ–≤–æ—é –º–æ–¥–µ–ª–ª—é –¥–ª—è –∫—ñ–ª—å–∫—ñ—Å–Ω–æ—ó –æ—Ü—ñ–Ω–∫–∏ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è.",
                "links": [
                  {
                    "title": "SOTA",
                    "href": "../level-1/sota.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "RLHF",
                "def": "Reinforcement Learning from Human Feedback ‚Äî training models to align with human preferences using a learned reward model."
              },
              {
                "term": "LoRA",
                "def": "Low-Rank Adaptation ‚Äî efficient fine-tuning that trains only small additional matrices while keeping the base model frozen."
              },
              {
                "term": "SFT",
                "def": "Supervised Fine-Tuning ‚Äî training on curated instruction-response pairs to teach the model to follow instructions."
              },
              {
                "term": "DPO",
                "def": "Direct Preference Optimization ‚Äî simpler alternative to RLHF that directly optimizes on human preference pairs without a reward model."
              }
            ],
            "uk": [
              {
                "term": "RLHF",
                "def": "–ù–∞–≤—á–∞–Ω–Ω—è –∑ –ø—ñ–¥–∫—Ä—ñ–ø–ª–µ–Ω–Ω—è–º –∑–∞ –∑–≤–æ—Ä–æ—Ç–Ω–∏–º –∑–≤'—è–∑–∫–æ–º –≤—ñ–¥ –ª—é–¥–µ–π ‚Äî –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ –ª—é–¥—Å—å–∫–∏—Ö –ø–µ—Ä–µ–≤–∞–≥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å –Ω–∞–≥–æ—Ä–æ–¥–∏."
              },
              {
                "term": "LoRA",
                "def": "Low-Rank Adaptation ‚Äî –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏–π —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –∑ –Ω–∞–≤—á–∞–Ω–Ω—è–º –ª–∏—à–µ –º–∞–ª–∏—Ö –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –º–∞—Ç—Ä–∏—Ü—å –ø—Ä–∏ –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ñ–π –±–∞–∑–æ–≤—ñ–π –º–æ–¥–µ–ª—ñ."
              },
              {
                "term": "SFT",
                "def": "–ö–µ—Ä–æ–≤–∞–Ω–µ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ ‚Äî –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –∫—É—Ä–∞—Ç–æ—Ä—Å—å–∫–∏—Ö –ø–∞—Ä–∞—Ö —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è-–≤—ñ–¥–ø–æ–≤—ñ–¥—å –¥–ª—è –≤—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Å–ª—ñ–¥—É–≤–∞—Ç–∏ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è–º."
              },
              {
                "term": "DPO",
                "def": "–ü—Ä—è–º–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø–µ—Ä–µ–≤–∞–≥ ‚Äî –ø—Ä–æ—Å—Ç—ñ—à–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ RLHF –±–µ–∑ –º–æ–¥–µ–ª—ñ –Ω–∞–≥–æ—Ä–æ–¥–∏, —â–æ –Ω–∞–ø—Ä—è–º—É –æ–ø—Ç–∏–º—ñ–∑—É—î –Ω–∞ –ø–∞—Ä–∞—Ö –ø–µ—Ä–µ–≤–∞–≥."
              }
            ]
          },
          "tips": {
            "en": [
              "Try prompting and few-shot examples first ‚Äî fine-tuning is worth it only when prompting hits a quality ceiling you cannot overcome",
              "Start with LoRA/QLoRA before attempting full fine-tuning ‚Äî you'll get 90% of the quality at 5% of the cost and compute",
              "Always hold out 10-20% of your data for evaluation ‚Äî without a test set, you cannot tell if your fine-tune actually improved the model"
            ],
            "uk": [
              "–°–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ —Ç–∞ few-shot –ø—Ä–∏–∫–ª–∞–¥–∏ —Å–ø–æ—á–∞—Ç–∫—É ‚Äî —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –≤–∞—Ä—Ç–æ –ª–∏—à–µ –∫–æ–ª–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ –¥–æ—Å—è–≥–∞—î —Å—Ç–µ–ª—ñ —è–∫–æ—Å—Ç—ñ",
              "–ü–æ—á–Ω—ñ—Ç—å –∑ LoRA/QLoRA –ø–µ—Ä–µ–¥ –ø–æ–≤–Ω–∏–º —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥–æ–º ‚Äî –æ—Ç—Ä–∏–º–∞—î—Ç–µ 90% —è–∫–æ—Å—Ç—ñ –∑–∞ 5% –≤–∏—Ç—Ä–∞—Ç —Ç–∞ –æ–±—á–∏—Å–ª–µ–Ω—å",
              "–ó–∞–≤–∂–¥–∏ –∑–∞–ª–∏—à–∞–π—Ç–µ 10-20% –¥–∞–Ω–∏—Ö –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ ‚Äî –±–µ–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä—É –≤–∏ –Ω–µ –∑–º–æ–∂–µ—Ç–µ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —á–∏ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –ø–æ–∫—Ä–∞—â–∏–≤ –º–æ–¥–µ–ª—å"
            ]
          },
          "related": [
            "Models",
            "Video Content"
          ]
        },
        {
          "slug": "model-optimization",
          "title": {
            "en": "Model Optimization",
            "uk": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π"
          },
          "desc": {
            "en": "Making models faster, smaller, and cheaper to run.",
            "uk": "–Ø–∫ –∑—Ä–æ–±–∏—Ç–∏ –º–æ–¥–µ–ª—ñ —à–≤–∏–¥—à–∏–º–∏, –º–µ–Ω—à–∏–º–∏ —Ç–∞ –¥–µ—à–µ–≤—à–∏–º–∏ —É –∑–∞–ø—É—Å–∫—É."
          },
          "overview": {
            "en": [
              "Running large AI models requires expensive hardware. Model optimization techniques reduce computational requirements while maintaining quality. Quantization (reducing numerical precision), pruning (removing unnecessary connections), and distillation (training smaller models from larger ones) make it possible to run models on consumer hardware that would otherwise require data center GPUs.",
              "Optimization is what makes AI practical. Without it, only companies with massive GPU clusters could use frontier models. Thanks to quantization and efficient inference engines like llama.cpp and vLLM, a 70B parameter model can run on a gaming PC, and inference costs have dropped 100x in two years. This democratization drives the open-source AI movement."
            ],
            "uk": [
              "–ó–∞–ø—É—Å–∫ –≤–µ–ª–∏–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –®–Ü –≤–∏–º–∞–≥–∞—î –¥–æ—Ä–æ–≥–æ–≥–æ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è. –¢–µ—Ö–Ω—ñ–∫–∏ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –º–æ–¥–µ–ª–µ–π –∑–º–µ–Ω—à—É—é—Ç—å –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω—ñ –≤–∏–º–æ–≥–∏ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ —è–∫–æ—Å—Ç—ñ. –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—è (–∑–º–µ–Ω—à–µ–Ω–Ω—è —á–∏—Å–ª–æ–≤–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ), –ø—Ä—É–Ω—ñ–Ω–≥ (–≤–∏–¥–∞–ª–µ–Ω–Ω—è –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö –∑'—î–¥–Ω–∞–Ω—å) —Ç–∞ –¥–∏—Å—Ç–∏–ª—è—Ü—ñ—è (–Ω–∞–≤—á–∞–Ω–Ω—è –º–µ–Ω—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∑ –±—ñ–ª—å—à–∏—Ö) –¥–æ–∑–≤–æ–ª—è—é—Ç—å –∑–∞–ø—É—Å–∫–∞—Ç–∏ –º–æ–¥–µ–ª—ñ –Ω–∞ —Å–ø–æ–∂–∏–≤—á–æ–º—É –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—ñ.",
              "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è ‚Äî —Ü–µ —Ç–µ, —â–æ —Ä–æ–±–∏—Ç—å –®–Ü –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–º. –ë–µ–∑ –Ω–µ—ó –ª–∏—à–µ –∫–æ–º–ø–∞–Ω—ñ—ó –∑ –º–∞—Å–∏–≤–Ω–∏–º–∏ GPU-–∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –º–æ–≥–ª–∏ –± –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ñ—Ä–æ–Ω—Ç–∏—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ. –ó–∞–≤–¥—è–∫–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—ó —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏–º —Ä—É—à—ñ—è–º —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É —è–∫ llama.cpp —Ç–∞ vLLM, –º–æ–¥–µ–ª—å –∑ 70B –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–æ–∂–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –Ω–∞ —ñ–≥—Ä–æ–≤–æ–º—É –ü–ö, –∞ –≤–∏—Ç—Ä–∞—Ç–∏ –Ω–∞ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å –≤–ø–∞–ª–∏ —É 100 —Ä–∞–∑—ñ–≤ –∑–∞ –¥–≤–∞ —Ä–æ–∫–∏. –¶—è –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü—ñ—è —Ä—É—Ö–∞—î open-source –®–Ü —Ä—É—Ö."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Quantization Basics",
                "desc": "Reducing numerical precision from FP32 (32 bits per weight) ‚Üí FP16 ‚Üí INT8 ‚Üí INT4 shrinks model size 2-8x. Each step trades a small amount of quality for major size and speed improvements.",
                "links": [
                  {
                    "title": "Model Formats",
                    "href": "../level-4/model-formats.html"
                  }
                ]
              },
              {
                "text": "GPTQ and AWQ",
                "desc": "GPU-optimized post-training quantization methods. GPTQ uses calibration data for higher quality. AWQ (Activation-aware Weight Quantization) preserves important weights based on activation patterns. Both enable fast GPU inference.",
                "links": [
                  {
                    "title": "Hardware",
                    "href": "../level-4/hardware.html"
                  }
                ]
              },
              {
                "text": "GGUF and llama.cpp",
                "desc": "GGUF is the format for llama.cpp, enabling mixed CPU+GPU inference on consumer machines. Supports quantization from Q2 (smallest) to Q8 (highest quality). Q4_K_M is the popular sweet spot.",
                "links": [
                  {
                    "title": "Model Formats",
                    "href": "../level-4/model-formats.html"
                  }
                ]
              },
              {
                "text": "Pruning",
                "desc": "Removing weights close to zero that contribute little to output quality. Structured pruning removes entire neurons or attention heads. Can reduce model size 50-90% with careful calibration.",
                "links": [
                  {
                    "title": "Neural Networks",
                    "href": "neural-networks.html"
                  }
                ]
              },
              {
                "text": "Knowledge Distillation",
                "desc": "Training a small \"student\" model from a large \"teacher\" model. The student learns from the teacher's probability distributions, not just hard labels, capturing richer information. GPT-4 distilled into GPT-4o mini is a practical example.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Flash Attention",
                "desc": "Memory-efficient attention computation by Tri Dao that fuses operations and uses tiling to avoid materializing the full attention matrix. Reduces GPU memory usage 5-20x and speeds up training by 2-4x.",
                "links": [
                  {
                    "title": "Model Types",
                    "href": "model-types.html"
                  }
                ]
              },
              {
                "text": "Speculative Decoding",
                "desc": "Using a fast small model to draft multiple tokens at once, then the large model verifies them in a single forward pass. Achieves 2-3x speedup without quality loss since rejected tokens are regenerated.",
                "links": [
                  {
                    "title": "LLM",
                    "href": "../level-2/llm.html"
                  }
                ]
              },
              {
                "text": "KV Cache Optimization",
                "desc": "During generation, each token must attend to all previous tokens. KV cache stores these attention states but grows with sequence length. PagedAttention (vLLM) manages this memory like virtual memory in operating systems.",
                "links": [
                  {
                    "title": "Context",
                    "href": "../level-2/context.html"
                  }
                ]
              },
              {
                "text": "Model Merging",
                "desc": "Combining weights from multiple fine-tuned models without additional training. Methods like TIES, DARE, and SLERP interpolate between model checkpoints. Community merges on HuggingFace often outperform individual fine-tunes.",
                "links": [
                  {
                    "title": "Training & Fine-tuning",
                    "href": "training-finetuning.html"
                  }
                ]
              },
              {
                "text": "Practical Impact",
                "desc": "A 70B model quantized to Q4 fits in 48GB VRAM (2x consumer GPUs). vLLM serves models 24x faster than naive inference. These optimizations mean a $2000 PC can run models that cost $100K+ to train.",
                "links": [
                  {
                    "title": "API Providers",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–û—Å–Ω–æ–≤–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—ó",
                "desc": "–ó–º–µ–Ω—à–µ–Ω–Ω—è —á–∏—Å–ª–æ–≤–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ –≤—ñ–¥ FP32 (32 –±—ñ—Ç–∏ –Ω–∞ –≤–∞–≥—É) ‚Üí FP16 ‚Üí INT8 ‚Üí INT4 –∑–º–µ–Ω—à—É—î –º–æ–¥–µ–ª—å —É 2-8 —Ä–∞–∑—ñ–≤. –ö–æ–∂–µ–Ω –∫—Ä–æ–∫ –æ–±–º—ñ–Ω—é—î –º–∞–ª—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å —è–∫–æ—Å—Ç—ñ –Ω–∞ –∑–Ω–∞—á–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É —Ç–∞ —à–≤–∏–¥–∫–æ—Å—Ç—ñ.",
                "links": [
                  {
                    "title": "–§–æ—Ä–º–∞—Ç–∏ –º–æ–¥–µ–ª–µ–π",
                    "href": "../level-4/model-formats.html"
                  }
                ]
              },
              {
                "text": "GPTQ —Ç–∞ AWQ",
                "desc": "–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ –¥–ª—è GPU –º–µ—Ç–æ–¥–∏ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ—ó –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—ó. GPTQ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∫–∞–ª—ñ–±—Ä–∞—Ü—ñ–π–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è –≤–∏—â–æ—ó —è–∫–æ—Å—Ç—ñ. AWQ –∑–±–µ—Ä—ñ–≥–∞—î –≤–∞–∂–ª–∏–≤—ñ –≤–∞–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–∞—Ç–µ—Ä–Ω—ñ–≤ –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó. –û–±–∏–¥–≤–∞ –∑–∞–±–µ–∑–ø–µ—á—É—é—Ç—å —à–≤–∏–¥–∫–∏–π GPU —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å.",
                "links": [
                  {
                    "title": "–û–±–ª–∞–¥–Ω–∞–Ω–Ω—è",
                    "href": "../level-4/hardware.html"
                  }
                ]
              },
              {
                "text": "GGUF —Ç–∞ llama.cpp",
                "desc": "GGUF ‚Äî —Ñ–æ—Ä–º–∞—Ç –¥–ª—è llama.cpp, —â–æ –¥–æ–∑–≤–æ–ª—è—î –∑–º—ñ—à–∞–Ω–∏–π CPU+GPU —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ —Å–ø–æ–∂–∏–≤—á–∏—Ö –º–∞—à–∏–Ω–∞—Ö. –ü—ñ–¥—Ç—Ä–∏–º—É—î –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—é –≤—ñ–¥ Q2 (–Ω–∞–π–º–µ–Ω—à–∏–π) –¥–æ Q8 (–Ω–∞–π–≤–∏—â–∞ —è–∫—ñ—Å—Ç—å). Q4_K_M ‚Äî –ø–æ–ø—É–ª—è—Ä–Ω–∏–π –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –±–∞–ª–∞–Ω—Å.",
                "links": [
                  {
                    "title": "–§–æ—Ä–º–∞—Ç–∏ –º–æ–¥–µ–ª–µ–π",
                    "href": "../level-4/model-formats.html"
                  }
                ]
              },
              {
                "text": "–ü—Ä—É–Ω—ñ–Ω–≥",
                "desc": "–í–∏–¥–∞–ª–µ–Ω–Ω—è –≤–∞–≥ –±–ª–∏–∑—å–∫–∏—Ö –¥–æ –Ω—É–ª—è, —â–æ –º–∞–ª–æ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ —è–∫—ñ—Å—Ç—å. –°—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –ø—Ä—É–Ω—ñ–Ω–≥ –≤–∏–¥–∞–ª—è—î —Ü—ñ–ª—ñ –Ω–µ–π—Ä–æ–Ω–∏ –∞–±–æ –≥–æ–ª–æ–≤–∏ —É–≤–∞–≥–∏. –ú–æ–∂–µ –∑–º–µ–Ω—à–∏—Ç–∏ –º–æ–¥–µ–ª—å –Ω–∞ 50-90% –ø—Ä–∏ —Ä–µ—Ç–µ–ª—å–Ω—ñ–π –∫–∞–ª—ñ–±—Ä–∞—Ü—ñ—ó.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂",
                    "href": "neural-networks.html"
                  }
                ]
              },
              {
                "text": "–î–∏—Å—Ç–∏–ª—è—Ü—ñ—è –∑–Ω–∞–Ω—å",
                "desc": "–ù–∞–≤—á–∞–Ω–Ω—è –º–∞–ª–æ–≥–æ \"—É—á–Ω—è\" –∑ –≤–µ–ª–∏–∫–æ–≥–æ \"–≤—á–∏—Ç–µ–ª—è\". –£—á–µ–Ω—å –≤—á–∏—Ç—å—Å—è –∑ —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π –≤—á–∏—Ç–µ–ª—è, –∞ –Ω–µ –ª–∏—à–µ –∑ –∂–æ—Ä—Å—Ç–∫–∏—Ö –º—ñ—Ç–æ–∫, –∑–∞—Ö–æ–ø–ª—é—é—á–∏ –±–∞–≥–∞—Ç—à—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é. GPT-4 –¥–∏—Å—Ç–∏–ª—å–æ–≤–∞–Ω–∏–π —É GPT-4o mini ‚Äî –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Flash Attention",
                "desc": "–ü–∞–º'—è—Ç—å-–µ—Ñ–µ–∫—Ç–∏–≤–Ω–µ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —É–≤–∞–≥–∏ –≤—ñ–¥ Tri Dao, —â–æ –æ–±'—î–¥–Ω—É—î –æ–ø–µ—Ä–∞—Ü—ñ—ó —Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ç–∞–π–ª—ñ–Ω–≥. –ó–º–µ–Ω—à—É—î –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è GPU –ø–∞–º'—è—Ç—ñ –≤ 5-20 —Ä–∞–∑—ñ–≤ —Ç–∞ –ø—Ä–∏—Å–∫–æ—Ä—é—î –Ω–∞–≤—á–∞–Ω–Ω—è –≤ 2-4 —Ä–∞–∑–∏.",
                "links": [
                  {
                    "title": "–¢–∏–ø–∏ –º–æ–¥–µ–ª–µ–π",
                    "href": "model-types.html"
                  }
                ]
              },
              {
                "text": "–°–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–µ –¥–µ–∫–æ–¥—É–≤–∞–Ω–Ω—è",
                "desc": "–®–≤–∏–¥–∫–∞ –º–∞–ª–∞ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä—É—î –∫—ñ–ª—å–∫–∞ —Ç–æ–∫–µ–Ω—ñ–≤-—á–µ—Ä–Ω–µ—Ç–æ–∫, –ø–æ—Ç—ñ–º –≤–µ–ª–∏–∫–∞ –º–æ–¥–µ–ª—å –≤–µ—Ä–∏—Ñ—ñ–∫—É—î —ó—Ö –æ–¥–Ω–∏–º –ø—Ä—è–º–∏–º –ø—Ä–æ—Ö–æ–¥–æ–º. –î–æ—Å—è–≥–∞—î 2-3x –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –±–µ–∑ –≤—Ç—Ä–∞—Ç–∏ —è–∫–æ—Å—Ç—ñ, –±–æ –≤—ñ–¥—Ö–∏–ª–µ–Ω—ñ —Ç–æ–∫–µ–Ω–∏ –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–æ–≤—É—é—Ç—å—Å—è.",
                "links": [
                  {
                    "title": "LLM",
                    "href": "../level-2/llm.html"
                  }
                ]
              },
              {
                "text": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è KV –∫–µ—à—É",
                "desc": "–ü—ñ–¥ —á–∞—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–∂–µ–Ω —Ç–æ–∫–µ–Ω –ø–æ–≤–∏–Ω–µ–Ω –∑–≤–µ—Ä—Ç–∞—Ç–∏ —É–≤–∞–≥—É –Ω–∞ –≤—Å—ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ. KV –∫–µ—à –∑–±–µ—Ä—ñ–≥–∞—î —Ü—ñ —Å—Ç–∞–Ω–∏, –∞–ª–µ —Ä–æ—Å—Ç–µ –∑ –¥–æ–≤–∂–∏–Ω–æ—é. PagedAttention (vLLM) –∫–µ—Ä—É—î —Ü—ñ—î—é –ø–∞–º'—è—Ç—Ç—é —è–∫ –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–∞ –ø–∞–º'—è—Ç—å –≤ –û–°.",
                "links": [
                  {
                    "title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç",
                    "href": "../level-2/context.html"
                  }
                ]
              },
              {
                "text": "–ó–ª–∏—Ç—Ç—è –º–æ–¥–µ–ª–µ–π",
                "desc": "–ö–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è –≤–∞–≥ –∑ –∫—ñ–ª—å–∫–æ—Ö —Ñ–∞–π–Ω-—Ç—é–Ω–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è. –ú–µ—Ç–æ–¥–∏ TIES, DARE —Ç–∞ SLERP —ñ–Ω—Ç–µ—Ä–ø–æ–ª—é—é—Ç—å –º—ñ–∂ —á–µ–∫–ø–æ—ñ–Ω—Ç–∞–º–∏. –°–ø—ñ–ª—å–Ω–æ—Ç–Ω—ñ –∑–ª–∏—Ç—Ç—è –Ω–∞ HuggingFace —á–∞—Å—Ç–æ –ø–µ—Ä–µ–≤–µ—Ä—à—É—é—Ç—å —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω—ñ —Ñ–∞–π–Ω-—Ç—é–Ω–∏.",
                "links": [
                  {
                    "title": "–ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥",
                    "href": "training-finetuning.html"
                  }
                ]
              },
              {
                "text": "–ü—Ä–∞–∫—Ç–∏—á–Ω–∏–π –≤–ø–ª–∏–≤",
                "desc": "70B –º–æ–¥–µ–ª—å –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–∞ –¥–æ Q4 –≤–º—ñ—â—É—î—Ç—å—Å—è –≤ 48–ì–ë VRAM (2x —Å–ø–æ–∂–∏–≤—á—ñ GPU). vLLM –æ–±—Å–ª—É–≥–æ–≤—É—î –º–æ–¥–µ–ª—ñ –≤ 24 —Ä–∞–∑–∏ —à–≤–∏–¥—à–µ –∑–∞ –Ω–∞—ó–≤–Ω–∏–π —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å. –¶—ñ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –æ–∑–Ω–∞—á–∞—é—Ç—å, —â–æ –ü–ö –∑–∞ $2000 –º–æ–∂–µ –∑–∞–ø—É—Å–∫–∞—Ç–∏ –º–æ–¥–µ–ª—ñ, –Ω–∞–≤—á–∞–Ω–Ω—è —è–∫–∏—Ö –∫–æ—à—Ç—É–≤–∞–ª–æ $100K+.",
                "links": [
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Quantization",
                "def": "Reducing numerical precision of model weights (FP32‚ÜíINT4) to decrease size 2-8x and speed up inference."
              },
              {
                "term": "Distillation",
                "def": "Training a smaller student model to mimic a larger teacher model's probability distributions and capabilities."
              },
              {
                "term": "Flash Attention",
                "def": "Memory-efficient attention implementation that avoids materializing the full attention matrix, reducing GPU memory 5-20x."
              },
              {
                "term": "Speculative Decoding",
                "def": "Using a fast draft model to generate candidate tokens, verified in batch by the larger model for 2-3x speedup."
              }
            ],
            "uk": [
              {
                "term": "–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—è",
                "def": "–ó–º–µ–Ω—à–µ–Ω–Ω—è —á–∏—Å–ª–æ–≤–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ –≤–∞–≥ –º–æ–¥–µ–ª—ñ (FP32‚ÜíINT4) –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É —É 2-8 —Ä–∞–∑—ñ–≤ —Ç–∞ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É."
              },
              {
                "term": "–î–∏—Å—Ç–∏–ª—è—Ü—ñ—è",
                "def": "–ù–∞–≤—á–∞–Ω–Ω—è –º–µ–Ω—à–æ–≥–æ —É—á–Ω—è —ñ–º—ñ—Ç—É–≤–∞—Ç–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª–∏ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π —Ç–∞ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –±—ñ–ª—å—à–æ–≥–æ –≤—á–∏—Ç–µ–ª—è."
              },
              {
                "term": "Flash Attention",
                "def": "–ü–∞–º'—è—Ç—å-–µ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è —É–≤–∞–≥–∏ –±–µ–∑ –º–∞—Ç–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –ø–æ–≤–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ, —â–æ –∑–º–µ–Ω—à—É—î GPU –ø–∞–º'—è—Ç—å —É 5-20 —Ä–∞–∑—ñ–≤."
              },
              {
                "term": "–°–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–µ –¥–µ–∫–æ–¥—É–≤–∞–Ω–Ω—è",
                "def": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —à–≤–∏–¥–∫–æ—ó —á–µ—Ä–Ω–µ—Ç–∫–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–∞–Ω–¥–∏–¥–∞—Ç-—Ç–æ–∫–µ–Ω—ñ–≤, –≤–µ—Ä–∏—Ñ—ñ–∫–æ–≤–∞–Ω–∏—Ö –ø–∞–∫–µ—Ç–æ–º –≤–µ–ª–∏–∫–æ—é –º–æ–¥–µ–ª–ª—é –¥–ª—è 2-3x –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è."
              }
            ]
          },
          "tips": {
            "en": [
              "For local model inference, start with GGUF Q4_K_M quantization ‚Äî it offers the best quality-to-size ratio for most use cases",
              "Use vLLM or TGI for serving models in production ‚Äî they implement PagedAttention and continuous batching for 10-24x throughput improvement",
              "Before optimizing, profile your bottleneck: is it memory (try quantization), compute (try smaller model or distillation), or latency (try speculative decoding)?"
            ],
            "uk": [
              "–î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –ø–æ—á–Ω—ñ—Ç—å –∑ GGUF Q4_K_M –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—ó ‚Äî –≤–æ–Ω–∞ –ø—Ä–æ–ø–æ–Ω—É—î –Ω–∞–π–∫—Ä–∞—â–∏–π –±–∞–ª–∞–Ω—Å —è–∫–æ—Å—Ç—ñ –¥–æ —Ä–æ–∑–º—ñ—Ä—É –¥–ª—è –±—ñ–ª—å—à–æ—Å—Ç—ñ –≤–∏–ø–∞–¥–∫—ñ–≤",
              "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ vLLM –∞–±–æ TGI –¥–ª—è –æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π —É –ø—Ä–æ–¥–∞–∫—à–Ω—ñ ‚Äî –≤–æ–Ω–∏ —Ä–µ–∞–ª—ñ–∑—É—é—Ç—å PagedAttention —Ç–∞ –±–µ–∑–ø–µ—Ä–µ—Ä–≤–Ω–∏–π –±–∞—Ç—á–∏–Ω–≥ –¥–ª—è 10-24x –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –ø—Ä–æ–ø—É—Å–∫–Ω–æ—ó –∑–¥–∞—Ç–Ω–æ—Å—Ç—ñ",
              "–ü–µ—Ä–µ–¥ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—î—é –ø—Ä–æ—Ñ—ñ–ª—é–π—Ç–µ –≤—É–∑—å–∫–µ –º—ñ—Å—Ü–µ: –ø–∞–º'—è—Ç—å (—Å–ø—Ä–æ–±—É–π—Ç–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—é), –æ–±—á–∏—Å–ª–µ–Ω–Ω—è (–º–µ–Ω—à–∞ –º–æ–¥–µ–ª—å –∞–±–æ –¥–∏—Å—Ç–∏–ª—è—Ü—ñ—è) —á–∏ –∑–∞—Ç—Ä–∏–º–∫–∞ (—Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–µ –¥–µ–∫–æ–¥—É–≤–∞–Ω–Ω—è)?"
            ]
          },
          "related": [
            "Models",
            "Video Content"
          ]
        },
        {
          "slug": "model-types",
          "title": {
            "en": "Model Types & Structures",
            "uk": "–¢–∏–ø–∏ —Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª–µ–π"
          },
          "desc": {
            "en": "Different model architectures and their trade-offs.",
            "uk": "–†—ñ–∑–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª–µ–π —Ç–∞ —ó—Ö –∫–æ–º–ø—Ä–æ–º—ñ—Å–∏."
          },
          "overview": {
            "en": [
              "Not all neural networks are structured the same way. Different architectures have different strengths. Decoder-only transformers (GPT, Llama) excel at text generation. Encoder-decoder models (T5) are great for translation and summarization. MoE (Mixture of Experts) architectures enable much larger models by only activating a subset of parameters per input, and newer state-space models (Mamba) offer alternatives to the quadratic cost of attention.",
              "Architecture choice has massive practical implications. MoE models like Mixtral offer frontier-level quality at a fraction of the inference cost because only a small portion of parameters activate per token. State-space models promise efficient processing of extremely long sequences. Understanding these trade-offs helps you choose the right model for your specific performance, cost, and latency requirements."
            ],
            "uk": [
              "–ù–µ –≤—Å—ñ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂—ñ –º–∞—é—Ç—å –æ–¥–Ω–∞–∫–æ–≤—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –†—ñ–∑–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –º–∞—é—Ç—å —Ä—ñ–∑–Ω—ñ –ø–µ—Ä–µ–≤–∞–≥–∏. –î–µ–∫–æ–¥–µ—Ä-only —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ (GPT, Llama) –≤—ñ–¥–º—ñ–Ω–Ω—ñ —É –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É. –ú–æ–¥–µ–ª—ñ –µ–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä (T5) —á—É–¥–æ–≤—ñ –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É. MoE –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –∑–Ω–∞—á–Ω–æ –±—ñ–ª—å—à—ñ –º–æ–¥–µ–ª—ñ, –∞–∫—Ç–∏–≤—É—é—á–∏ –ª–∏—à–µ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω—É –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤, –∞ –Ω–æ–≤—ñ –º–æ–¥–µ–ª—ñ –ø—Ä–æ—Å—Ç–æ—Ä—É —Å—Ç–∞–Ω—ñ–≤ (Mamba) –ø—Ä–æ–ø–æ–Ω—É—é—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∏ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ñ–π –≤–∞—Ä—Ç–æ—Å—Ç—ñ —É–≤–∞–≥–∏.",
              "–í–∏–±—ñ—Ä –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –º–∞—î –º–∞—Å–∏–≤–Ω—ñ –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –Ω–∞—Å–ª—ñ–¥–∫–∏. MoE –º–æ–¥–µ–ª—ñ —è–∫ Mixtral –ø—Ä–æ–ø–æ–Ω—É—é—Ç—å —Ñ—Ä–æ–Ω—Ç–∏—Ä–Ω—É —è–∫—ñ—Å—Ç—å –∑–∞ —á–∞—Å—Ç–∫—É –≤–∏—Ç—Ä–∞—Ç —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É, –±–æ –ª–∏—à–µ –º–∞–ª–∞ —á–∞—Å—Ç–∏–Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –∞–∫—Ç–∏–≤—É—î—Ç—å—Å—è –Ω–∞ —Ç–æ–∫–µ–Ω. –ú–æ–¥–µ–ª—ñ –ø—Ä–æ—Å—Ç–æ—Ä—É —Å—Ç–∞–Ω—ñ–≤ –æ–±—ñ—Ü—è—é—Ç—å –µ—Ñ–µ–∫—Ç–∏–≤–Ω—É –æ–±—Ä–æ–±–∫—É –Ω–∞–¥–∑–≤–∏—á–∞–π–Ω–æ –¥–æ–≤–≥–∏—Ö –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π. –†–æ–∑—É–º—ñ–Ω–Ω—è —Ü–∏—Ö –∫–æ–º–ø—Ä–æ–º—ñ—Å—ñ–≤ –¥–æ–ø–æ–º–∞–≥–∞—î –æ–±—Ä–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—É –º–æ–¥–µ–ª—å –¥–ª—è –≤–∞—à–∏—Ö –≤–∏–º–æ–≥ –¥–æ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ, –≤–∞—Ä—Ç–æ—Å—Ç—ñ —Ç–∞ –∑–∞—Ç—Ä–∏–º–∫–∏."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Transformer Variants",
                "desc": "Encoder-only (BERT ‚Äî great for classification, embeddings), decoder-only (GPT, Llama ‚Äî text generation), encoder-decoder (T5, BART ‚Äî translation, summarization). Each variant processes text differently based on its attention mask pattern.",
                "links": [
                  {
                    "title": "Neural Networks",
                    "href": "neural-networks.html"
                  }
                ]
              },
              {
                "text": "Decoder-Only Dominance",
                "desc": "GPT, Claude, Gemini, Llama all use decoder-only architecture. It won out because it scales efficiently, can be pre-trained with simple next-token prediction, and handles both understanding and generation in one model.",
                "links": [
                  {
                    "title": "LLM",
                    "href": "../level-2/llm.html"
                  }
                ]
              },
              {
                "text": "Mixture of Experts (MoE)",
                "desc": "Each token is routed to only 2 of 8+ specialized expert sub-networks by a learned router. This means a model with 46B total parameters may only use 12B per forward pass, making it dramatically faster than a dense model of equal quality.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "MoE in Practice",
                "desc": "Mixtral 8x7B (46B total, 12B active), DeepSeek-V3, and likely GPT-4. MoE models need more memory (all experts loaded) but compute less per token. Ideal for deployment where memory is cheap but compute/latency matter.",
                "links": [
                  {
                    "title": "Model Optimization",
                    "href": "model-optimization.html"
                  }
                ]
              },
              {
                "text": "Dense vs Sparse Models",
                "desc": "Dense models (Llama, Claude) activate all parameters for every token ‚Äî predictable, easier to optimize. Sparse models (MoE) activate only a fraction ‚Äî more efficient but harder to train and balance across experts.",
                "links": []
              },
              {
                "text": "Multi-Head Attention",
                "desc": "Multiple parallel attention heads capture different types of relationships ‚Äî syntax, semantics, long-range dependencies. Modern models use 32-128 heads. Grouped Query Attention (GQA) reduces memory by sharing key/value heads.",
                "links": [
                  {
                    "title": "Context",
                    "href": "../level-2/context.html"
                  }
                ]
              },
              {
                "text": "State-Space Models",
                "desc": "Mamba and S4 process sequences in O(n) time vs O(n^2) for attention. They maintain a compressed state that evolves as they read each token. Promising for extremely long sequences (100K+ tokens) where attention becomes prohibitively expensive.",
                "links": []
              },
              {
                "text": "Hybrid Architectures",
                "desc": "Combining attention layers with state-space or linear attention layers. Jamba (AI21) mixes Mamba and Transformer layers. Hybrids aim to get the best of both: global attention for reasoning, efficient processing for long contexts.",
                "links": []
              },
              {
                "text": "Model Depth vs Width",
                "desc": "More layers (deeper) vs wider layers (more parameters per layer). Deeper models tend to reason better but are slower. Wider models process faster but may reason less deeply. Scaling laws help labs find optimal proportions.",
                "links": [
                  {
                    "title": "Data to Model",
                    "href": "data-to-model.html"
                  }
                ]
              },
              {
                "text": "Vision and Multimodal Architectures",
                "desc": "Vision Transformers (ViT) apply attention to image patches. Multimodal models combine text transformers with vision encoders (CLIP, SigLIP). Diffusion models use a completely different architecture for image generation.",
                "links": [
                  {
                    "title": "Multimodal AI",
                    "href": "../level-1/multimodal-ai.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–í–∞—Ä—ñ–∞–Ω—Ç–∏ Transformer",
                "desc": "Encoder-only (BERT ‚Äî –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è, –µ–º–±–µ–¥—ñ–Ω–≥–∏), decoder-only (GPT, Llama ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É), encoder-decoder (T5, BART ‚Äî –ø–µ—Ä–µ–∫–ª–∞–¥, —Å—É–º–∞—Ä–∏–∑–∞—Ü—ñ—è). –ö–æ–∂–µ–Ω –≤–∞—Ä—ñ–∞–Ω—Ç –æ–±—Ä–æ–±–ª—è—î —Ç–µ–∫—Å—Ç –ø–æ-—Ä—ñ–∑–Ω–æ–º—É –∑–∞–≤–¥—è–∫–∏ –ø–∞—Ç–µ—Ä–Ω—É –º–∞—Å–∫–∏ —É–≤–∞–≥–∏.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂",
                    "href": "neural-networks.html"
                  }
                ]
              },
              {
                "text": "–î–æ–º—ñ–Ω—É–≤–∞–Ω–Ω—è Decoder-Only",
                "desc": "GPT, Claude, Gemini, Llama ‚Äî –≤—Å—ñ decoder-only. –ü–µ—Ä–µ–º–æ–≥–ª–∞, –±–æ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –º–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è, –Ω–∞–≤—á–∞—î—Ç—å—Å—è –ø—Ä–æ—Å—Ç–∏–º –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è–º –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ —ñ –æ–±—Ä–æ–±–ª—è—î —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é –≤ –æ–¥–Ω—ñ–π –º–æ–¥–µ–ª—ñ.",
                "links": [
                  {
                    "title": "LLM",
                    "href": "../level-2/llm.html"
                  }
                ]
              },
              {
                "text": "Mixture of Experts (MoE)",
                "desc": "–ö–æ–∂–µ–Ω —Ç–æ–∫–µ–Ω –º–∞—Ä—à—Ä—É—Ç–∏–∑—É—î—Ç—å—Å—è –ª–∏—à–µ –¥–æ 2 –∑ 8+ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –µ–∫—Å–ø–µ—Ä—Ç–Ω–∏—Ö –ø—ñ–¥–º–µ—Ä–µ–∂. –ú–æ–¥–µ–ª—å –∑ 46B –∑–∞–≥–∞–ª—å–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–æ–∂–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ª–∏—à–µ 12B –Ω–∞ –ø—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥, —â–æ —Ä–æ–±–∏—Ç—å —ó—ó –¥—Ä–∞–º–∞—Ç–∏—á–Ω–æ —à–≤–∏–¥—à–æ—é –∑–∞ —â—ñ–ª—å–Ω—É –º–æ–¥–µ–ª—å —Ä—ñ–≤–Ω–æ—ó —è–∫–æ—Å—Ç—ñ.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "MoE –Ω–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ",
                "desc": "Mixtral 8x7B (46B –∑–∞–≥–∞–ª—å–Ω–∏—Ö, 12B –∞–∫—Ç–∏–≤–Ω–∏—Ö), DeepSeek-V3 —Ç–∞ –π–º–æ–≤—ñ—Ä–Ω–æ GPT-4. MoE –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –±—ñ–ª—å—à–µ –ø–∞–º'—è—Ç—ñ (–≤—Å—ñ –µ–∫—Å–ø–µ—Ä—Ç–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ), –∞–ª–µ –æ–±—á–∏—Å–ª—é—é—Ç—å –º–µ–Ω—à–µ –Ω–∞ —Ç–æ–∫–µ–Ω. –Ü–¥–µ–∞–ª—å–Ω—ñ –¥–µ –ø–∞–º'—è—Ç—å –¥–µ—à–µ–≤–∞, –∞ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è/–∑–∞—Ç—Ä–∏–º–∫–∞ –≤–∞–∂–ª–∏–≤—ñ.",
                "links": [
                  {
                    "title": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π",
                    "href": "model-optimization.html"
                  }
                ]
              },
              {
                "text": "–©—ñ–ª—å–Ω—ñ vs —Ä–æ–∑—Ä—ñ–¥–∂–µ–Ω—ñ –º–æ–¥–µ–ª—ñ",
                "desc": "–©—ñ–ª—å–Ω—ñ (Llama, Claude) –∞–∫—Ç–∏–≤—É—é—Ç—å —É—Å—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ ‚Äî –ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω—ñ, –ª–µ–≥—à—ñ –≤ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó. –†–æ–∑—Ä—ñ–¥–∂–µ–Ω—ñ (MoE) –∞–∫—Ç–∏–≤—É—é—Ç—å —á–∞—Å—Ç–∫—É ‚Äî –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—à—ñ, –∞–ª–µ —Å–∫–ª–∞–¥–Ω—ñ—à—ñ –≤ –Ω–∞–≤—á–∞–Ω–Ω—ñ —Ç–∞ –±–∞–ª–∞–Ω—Å—É–≤–∞–Ω–Ω—ñ –º—ñ–∂ –µ–∫—Å–ø–µ—Ä—Ç–∞–º–∏.",
                "links": []
              },
              {
                "text": "–ë–∞–≥–∞—Ç–æ–≥–æ–ª–æ–≤–∞ —É–≤–∞–≥–∞",
                "desc": "–ö—ñ–ª—å–∫–∞ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö –≥–æ–ª—ñ–≤ —É–≤–∞–≥–∏ –∑–∞—Ö–æ–ø–ª—é—é—Ç—å —Ä—ñ–∑–Ω—ñ –∑–≤'—è–∑–∫–∏ ‚Äî —Å–∏–Ω—Ç–∞–∫—Å–∏—Å, —Å–µ–º–∞–Ω—Ç–∏–∫—É, –¥–∞–ª–µ–∫–æ—Å—è–∂–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ. –°—É—á–∞—Å–Ω—ñ –º–æ–¥–µ–ª—ñ –º–∞—é—Ç—å 32-128 –≥–æ–ª—ñ–≤. Grouped Query Attention (GQA) –∑–º–µ–Ω—à—É—î –ø–∞–º'—è—Ç—å —Å–ø—ñ–ª—å–Ω–∏–º –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –≥–æ–ª—ñ–≤ –∫–ª—é—á/–∑–Ω–∞—á–µ–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç",
                    "href": "../level-2/context.html"
                  }
                ]
              },
              {
                "text": "–ú–æ–¥–µ–ª—ñ –ø—Ä–æ—Å—Ç–æ—Ä—É —Å—Ç–∞–Ω—ñ–≤",
                "desc": "Mamba —Ç–∞ S4 –æ–±—Ä–æ–±–ª—è—é—Ç—å –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –∑–∞ O(n) —á–∞—Å vs O(n^2) –¥–ª—è —É–≤–∞–≥–∏. –í–æ–Ω–∏ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å —Å—Ç–∏—Å–Ω—É—Ç–∏–π —Å—Ç–∞–Ω, —â–æ –µ–≤–æ–ª—é—Ü—ñ–æ–Ω—É—î –∑ –∫–æ–∂–Ω–∏–º —Ç–æ–∫–µ–Ω–æ–º. –ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ñ –¥–ª—è –Ω–∞–¥–∑–≤–∏—á–∞–π–Ω–æ –¥–æ–≤–≥–∏—Ö –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π (100K+ —Ç–æ–∫–µ–Ω—ñ–≤).",
                "links": []
              },
              {
                "text": "–ì—ñ–±—Ä–∏–¥–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏",
                "desc": "–ö–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è —à–∞—Ä—ñ–≤ —É–≤–∞–≥–∏ –∑ —à–∞—Ä–∞–º–∏ –ø—Ä–æ—Å—Ç–æ—Ä—É —Å—Ç–∞–Ω—ñ–≤. Jamba (AI21) –∑–º—ñ—à—É—î —à–∞—Ä–∏ Mamba —Ç–∞ Transformer. –ì—ñ–±—Ä–∏–¥–∏ –ø—Ä–∞–≥–Ω—É—Ç—å –ø–æ—î–¥–Ω–∞—Ç–∏ –ø–µ—Ä–µ–≤–∞–≥–∏: –≥–ª–æ–±–∞–ª—å–Ω—É —É–≤–∞–≥—É –¥–ª—è –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è, –µ—Ñ–µ–∫—Ç–∏–≤–Ω—É –æ–±—Ä–æ–±–∫—É –¥–ª—è –¥–æ–≤–≥–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ–≤.",
                "links": []
              },
              {
                "text": "–ì–ª–∏–±–∏–Ω–∞ vs —à–∏—Ä–∏–Ω–∞ –º–æ–¥–µ–ª—ñ",
                "desc": "–ë—ñ–ª—å—à–µ —à–∞—Ä—ñ–≤ (–≥–ª–∏–±—à–∞) vs —à–∏—Ä—à—ñ —à–∞—Ä–∏ (–±—ñ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –Ω–∞ —à–∞—Ä). –ì–ª–∏–±—à—ñ –º–æ–¥–µ–ª—ñ –∫—Ä–∞—â–µ –º—ñ—Ä–∫—É—é—Ç—å, –∞–ª–µ –ø–æ–≤—ñ–ª—å–Ω—ñ—à—ñ. –®–∏—Ä—à—ñ –æ–±—Ä–æ–±–ª—è—é—Ç—å —à–≤–∏–¥—à–µ, –∞–ª–µ –º—ñ—Ä–∫—É—é—Ç—å –º–µ–Ω—à –≥–ª–∏–±–æ–∫–æ. –ó–∞–∫–æ–Ω–∏ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–æ–ø–æ–º–∞–≥–∞—é—Ç—å –∑–Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º—É–º.",
                "links": [
                  {
                    "title": "–í—ñ–¥ –¥–∞–Ω–∏—Ö –¥–æ –º–æ–¥–µ–ª—ñ",
                    "href": "data-to-model.html"
                  }
                ]
              },
              {
                "text": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –¥–ª—è –∑–æ—Ä—É —Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ",
                "desc": "Vision Transformers (ViT) –∑–∞—Å—Ç–æ—Å–æ–≤—É—é—Ç—å —É–≤–∞–≥—É –¥–æ –ø–∞—Ç—á—ñ–≤ –∑–æ–±—Ä–∞–∂–µ–Ω—å. –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ –ø–æ—î–¥–Ω—É—é—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –∑ –≤—ñ–∑—É–∞–ª—å–Ω–∏–º–∏ –µ–Ω–∫–æ–¥–µ—Ä–∞–º–∏ (CLIP, SigLIP). –î–∏—Ñ—É–∑—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –∑–æ–≤—Å—ñ–º —ñ–Ω—à—É –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑–æ–±—Ä–∞–∂–µ–Ω—å.",
                "links": [
                  {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∏–π –®–Ü",
                    "href": "../level-1/multimodal-ai.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "MoE",
                "def": "Mixture of Experts ‚Äî architecture that routes each token to specialized sub-networks, using only a fraction of total parameters per forward pass."
              },
              {
                "term": "Decoder-Only",
                "def": "Transformer variant that generates text autoregressively by predicting one token at a time ‚Äî the dominant architecture for modern LLMs."
              },
              {
                "term": "State-Space Model",
                "def": "Alternative to attention (Mamba, S4) that processes sequences in linear O(n) time rather than quadratic O(n^2)."
              },
              {
                "term": "Grouped Query Attention",
                "def": "Memory optimization where multiple attention heads share key/value projections, reducing KV cache memory 4-8x."
              }
            ],
            "uk": [
              {
                "term": "MoE",
                "def": "Mixture of Experts ‚Äî –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞, —â–æ –º–∞—Ä—à—Ä—É—Ç–∏–∑—É—î –∫–æ–∂–µ–Ω —Ç–æ–∫–µ–Ω –¥–æ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –ø—ñ–¥–º–µ—Ä–µ–∂, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –ª–∏—à–µ —á–∞—Å—Ç–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –Ω–∞ –ø—Ä–æ—Ö—ñ–¥."
              },
              {
                "term": "Decoder-Only",
                "def": "–í–∞—Ä—ñ–∞–Ω—Ç Transformer –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å–∏–≤–Ω–æ—ó –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É –ø–æ –æ–¥–Ω–æ–º—É —Ç–æ–∫–µ–Ω—É ‚Äî –¥–æ–º—ñ–Ω—É—é—á–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ —Å—É—á–∞—Å–Ω–∏—Ö LLM."
              },
              {
                "term": "–ú–æ–¥–µ–ª—å –ø—Ä–æ—Å—Ç–æ—Ä—É —Å—Ç–∞–Ω—ñ–≤",
                "def": "–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ —É–≤–∞–∑—ñ (Mamba, S4), —â–æ –æ–±—Ä–æ–±–ª—è—î –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –∑–∞ –ª—ñ–Ω—ñ–π–Ω–∏–π O(n) —á–∞—Å –∑–∞–º—ñ—Å—Ç—å –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ O(n^2)."
              },
              {
                "term": "Grouped Query Attention",
                "def": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø–∞–º'—è—Ç—ñ –¥–µ –∫—ñ–ª—å–∫–∞ –≥–æ–ª—ñ–≤ —É–≤–∞–≥–∏ —Å–ø—ñ–ª—å–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –ø—Ä–æ—î–∫—Ü—ñ—ó –∫–ª—é—á/–∑–Ω–∞—á–µ–Ω–Ω—è, –∑–º–µ–Ω—à—É—é—á–∏ KV –∫–µ—à —É 4-8 —Ä–∞–∑—ñ–≤."
              }
            ]
          },
          "tips": {
            "en": [
              "When choosing a model, check if it's dense or MoE ‚Äî MoE models need more RAM but run faster per token, which matters for real-time applications",
              "For embedding and classification tasks, encoder models (BERT variants) are still better and far cheaper than using LLMs via their API",
              "Watch the state-space model space (Mamba, RWKV) ‚Äî if they close the quality gap with transformers, they will enable much cheaper inference on long documents"
            ],
            "uk": [
              "–ü—Ä–∏ –≤–∏–±–æ—Ä—ñ –º–æ–¥–µ–ª—ñ –ø–µ—Ä–µ–≤—ñ—Ä—Ç–µ —á–∏ –≤–æ–Ω–∞ —â—ñ–ª—å–Ω–∞ —á–∏ MoE ‚Äî MoE –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –±—ñ–ª—å—à–µ RAM, –∞–ª–µ –ø—Ä–∞—Ü—é—é—Ç—å —à–≤–∏–¥—à–µ –Ω–∞ —Ç–æ–∫–µ–Ω, —â–æ –≤–∞–∂–ª–∏–≤–æ –¥–ª—è —Ä–µ–∞–ª-—Ç–∞–π–º –∑–∞—Å—Ç–æ—Å—É–Ω–∫—ñ–≤",
              "–î–ª—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ —Ç–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó encoder –º–æ–¥–µ–ª—ñ (–≤–∞—Ä—ñ–∞–Ω—Ç–∏ BERT) –≤—Å–µ —â–µ –∫—Ä–∞—â—ñ —Ç–∞ –∑–Ω–∞—á–Ω–æ –¥–µ—à–µ–≤—à—ñ –Ω—ñ–∂ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è LLM —á–µ—Ä–µ–∑ API",
              "–°–ª—ñ–¥–∫—É–π—Ç–µ –∑–∞ –º–æ–¥–µ–ª—è–º–∏ –ø—Ä–æ—Å—Ç–æ—Ä—É —Å—Ç–∞–Ω—ñ–≤ (Mamba, RWKV) ‚Äî —è–∫—â–æ –≤–æ–Ω–∏ –∑–∞–∫—Ä–∏—é—Ç—å —Ä–æ–∑—Ä–∏–≤ —è–∫–æ—Å—Ç—ñ –∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º–∏, –≤–æ–Ω–∏ –¥–æ–∑–≤–æ–ª—è—Ç—å –∑–Ω–∞—á–Ω–æ –¥–µ—à–µ–≤—à–∏–π —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –¥–æ–≤–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"
            ]
          },
          "related": [
            "Models",
            "Video Content"
          ]
        }
      ]
    },
    {
      "num": 4,
      "emoji": "üöÄ",
      "title": {
        "en": "Master",
        "uk": "–ú–∞–π—Å—Ç–µ—Ä"
      },
      "desc": {
        "en": "Advanced techniques: prompting strategies, agents, RAG, tool use, and practical AI development.",
        "uk": "–ü—Ä–æ—Å—É–Ω—É—Ç—ñ —Ç–µ—Ö–Ω—ñ–∫–∏: —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É, –∞–≥–µ–Ω—Ç–∏, RAG, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ —Ç–∞ –ø—Ä–∞–∫—Ç–∏—á–Ω–∞ —Ä–æ–∑—Ä–æ–±–∫–∞ –®–Ü."
      },
      "topics": [
        {
          "slug": "prompting-techniques",
          "title": {
            "en": "Prompting Techniques",
            "uk": "–¢–µ—Ö–Ω—ñ–∫–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É"
          },
          "desc": {
            "en": "Advanced prompting strategies for getting the best results from AI models.",
            "uk": "–ü—Ä–æ—Å—É–Ω—É—Ç—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –≤—ñ–¥ –º–æ–¥–µ–ª–µ–π –®–Ü."
          },
          "overview": {
            "en": [
              "Beyond basic prompt writing lies a rich landscape of techniques that can dramatically improve AI output quality. These strategies ‚Äî from Chain-of-Thought reasoning to Tree-of-Thought exploration ‚Äî exploit the way LLMs process and generate text to unlock capabilities that simple prompts cannot.",
              "Mastering prompting techniques is arguably the highest-leverage skill in AI today. The same model can produce mediocre or exceptional results depending entirely on how you prompt it. These techniques work because they shape the model's reasoning process, not just its output format."
            ],
            "uk": [
              "–ó–∞ –º–µ–∂–∞–º–∏ –±–∞–∑–æ–≤–æ–≥–æ –Ω–∞–ø–∏—Å–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤ –ª–µ–∂–∏—Ç—å –±–∞–≥–∞—Ç–∏–π –ª–∞–Ω–¥—à–∞—Ñ—Ç —Ç–µ—Ö–Ω—ñ–∫, —â–æ –º–æ–∂—É—Ç—å –¥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ —è–∫—ñ—Å—Ç—å –≤–∏—Ö–æ–¥—É –®–Ü. –¶—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó ‚Äî –≤—ñ–¥ –ª–∞–Ω—Ü—é–≥–∞ –¥—É–º–æ–∫ –¥–æ –¥–µ—Ä–µ–≤–∞ –¥—É–º–æ–∫ ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —Å–ø–æ—Å—ñ–± –æ–±—Ä–æ–±–∫–∏ —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É LLM –¥–ª—è —Ä–æ–∑–∫—Ä–∏—Ç—Ç—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π, –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏—Ö –ø—Ä–æ—Å—Ç–∏–º –ø—Ä–æ–º–ø—Ç–∞–º.",
              "–û–ø–∞–Ω—É–≤–∞–Ω–Ω—è —Ç–µ—Ö–Ω—ñ–∫ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É ‚Äî —Ü–µ, –º–∞–±—É—Ç—å, –Ω–∞–π–≤–ø–ª–∏–≤–æ–≤—ñ—à–∞ –Ω–∞–≤–∏—á–∫–∞ –≤ –®–Ü —Å—å–æ–≥–æ–¥–Ω—ñ. –û–¥–Ω–∞ –π —Ç–∞ –∂ –º–æ–¥–µ–ª—å –º–æ–∂–µ –≤–∏–¥–∞–≤–∞—Ç–∏ –ø–æ—Å–µ—Ä–µ–¥–Ω—ñ –∞–±–æ –≤–∏–Ω—è—Ç–∫–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–∞–ª–µ–∂–Ω–æ –≤–∏–∫–ª—é—á–Ω–æ –≤—ñ–¥ —Ç–æ–≥–æ, —è–∫ –≤–∏ —ó—ó –ø—Ä–æ–º–ø—Ç–∏—Ç–µ. –¶—ñ —Ç–µ—Ö–Ω—ñ–∫–∏ –ø—Ä–∞—Ü—é—é—Ç—å, –±–æ —Ñ–æ—Ä–º—É—é—Ç—å –ø—Ä–æ—Ü–µ—Å –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ, –∞ –Ω–µ –ª–∏—à–µ —Ñ–æ—Ä–º–∞—Ç –≤–∏—Ö–æ–¥—É."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Zero-Shot Prompting",
                "desc": "Asking the model to perform a task without any examples. Works well for simple, well-defined tasks where the model already has strong capabilities from training.",
                "links": [
                  {
                    "title": "Prompt",
                    "href": "../level-2/prompt.html"
                  }
                ]
              },
              {
                "text": "Few-Shot Prompting",
                "desc": "Providing 2-5 input/output examples before your actual request. Shows the model exactly what format, style, and quality you expect ‚Äî dramatically improves consistency.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Chain-of-Thought (CoT)",
                "desc": "Adding \"think step by step\" or showing reasoning examples forces the model to break problems into steps. Dramatically improves math, logic, and multi-step reasoning accuracy.",
                "links": [
                  {
                    "title": "Reasoning",
                    "href": "../level-1/reasoning.html"
                  }
                ]
              },
              {
                "text": "Tree-of-Thought",
                "desc": "Exploring multiple reasoning paths in parallel, evaluating each, and selecting the best. Like CoT but branching ‚Äî the model considers several approaches before committing to an answer.",
                "links": []
              },
              {
                "text": "ReAct Pattern",
                "desc": "Reasoning + Acting ‚Äî the model alternates between thinking about what to do and taking actions (tool calls). Powers most AI agents: observe ‚Üí think ‚Üí act ‚Üí observe results ‚Üí think again.",
                "links": [
                  {
                    "title": "Agents",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "Role Prompting",
                "desc": "\"You are an expert in...\" activates domain-specific knowledge and communication style. Combining roles with constraints creates powerful persona engineering for consistent outputs.",
                "links": []
              },
              {
                "text": "Constitutional Prompting",
                "desc": "Defining principles and rules the model must follow, then having it self-evaluate against those rules. Used by Anthropic for Claude's safety ‚Äî the model critiques and revises its own outputs.",
                "links": [
                  {
                    "title": "Alignment",
                    "href": "../level-5/alignment.html"
                  }
                ]
              },
              {
                "text": "Prompt Chaining",
                "desc": "Breaking a complex task into a sequence of simpler prompts where each output feeds into the next. Enables complex workflows that no single prompt could handle reliably.",
                "links": []
              },
              {
                "text": "Meta-Prompting",
                "desc": "Using AI to generate and optimize prompts. Ask the model to write a better version of your prompt, then use that improved prompt. Iterative meta-prompting converges on high-quality prompts.",
                "links": []
              },
              {
                "text": "Structured Output Forcing",
                "desc": "Using JSON schemas, XML tags, or markdown templates to constrain output format. Eliminates parsing issues and ensures programmatic usability. Most APIs now support native structured output.",
                "links": [
                  {
                    "title": "Tool Use",
                    "href": "tool-use.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "Zero-Shot –ø—Ä–æ–º–ø—Ç–∏–Ω–≥",
                "desc": "–ó–∞–ø–∏—Ç –¥–æ –º–æ–¥–µ–ª—ñ –≤–∏–∫–æ–Ω–∞—Ç–∏ –∑–∞–¥–∞—á—É –±–µ–∑ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤. –î–æ–±—Ä–µ –ø—Ä–∞—Ü—é—î –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö, —á—ñ—Ç–∫–æ –≤–∏–∑–Ω–∞—á–µ–Ω–∏—Ö –∑–∞–¥–∞—á, –¥–µ –º–æ–¥–µ–ª—å –≤–∂–µ –º–∞—î —Å–∏–ª—å–Ω—ñ –∑–¥—ñ–±–Ω–æ—Å—Ç—ñ –∑ –Ω–∞–≤—á–∞–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–ü—Ä–æ–º–ø—Ç",
                    "href": "../level-2/prompt.html"
                  }
                ]
              },
              {
                "text": "Few-Shot –ø—Ä–æ–º–ø—Ç–∏–Ω–≥",
                "desc": "–ù–∞–¥–∞–Ω–Ω—è 2-5 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –≤—Ö—ñ–¥/–≤–∏—Ö—ñ–¥ –ø–µ—Ä–µ–¥ –≤–∞—à–∏–º –∑–∞–ø–∏—Ç–æ–º. –ü–æ–∫–∞–∑—É—î –º–æ–¥–µ–ª—ñ —Ç–æ—á–Ω–æ —è–∫–∏–π —Ñ–æ—Ä–º–∞—Ç, —Å—Ç–∏–ª—å —Ç–∞ —è–∫—ñ—Å—Ç—å –≤–∏ –æ—á—ñ–∫—É—î—Ç–µ ‚Äî –¥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø–æ–∫—Ä–∞—â—É—î —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "–õ–∞–Ω—Ü—é–≥ –¥—É–º–æ–∫ (CoT)",
                "desc": "–î–æ–¥–∞–≤–∞–Ω–Ω—è \"–¥—É–º–∞–π –∫—Ä–æ–∫ –∑–∞ –∫—Ä–æ–∫–æ–º\" –∞–±–æ –ø–æ–∫–∞–∑ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –º—ñ—Ä–∫—É–≤–∞–Ω—å –∑–º—É—à—É—î –º–æ–¥–µ–ª—å —Ä–æ–∑–±–∏–≤–∞—Ç–∏ –ø—Ä–æ–±–ª–µ–º–∏ –Ω–∞ –∫—Ä–æ–∫–∏. –î—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø–æ–∫—Ä–∞—â—É—î —Ç–æ—á–Ω—ñ—Å—Ç—å —É –º–∞—Ç–µ–º–∞—Ç–∏—Ü—ñ, –ª–æ–≥—ñ—Ü—ñ —Ç–∞ –±–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤–∏—Ö –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è—Ö.",
                "links": [
                  {
                    "title": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                    "href": "../level-1/reasoning.html"
                  }
                ]
              },
              {
                "text": "–î–µ—Ä–µ–≤–æ –¥—É–º–æ–∫",
                "desc": "–ü–∞—Ä–∞–ª–µ–ª—å–Ω–µ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Ö —à–ª—è—Ö—ñ–≤ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è, –æ—Ü—ñ–Ω–∫–∞ –∫–æ–∂–Ω–æ–≥–æ —Ç–∞ –≤–∏–±—ñ—Ä –Ω–∞–π–∫—Ä–∞—â–æ–≥–æ. –Ø–∫ CoT, –∞–ª–µ –∑ —Ä–æ–∑–≥–∞–ª—É–∂–µ–Ω–Ω—è–º ‚Äî –º–æ–¥–µ–ª—å —Ä–æ–∑–≥–ª—è–¥–∞—î –∫—ñ–ª—å–∫–∞ –ø—ñ–¥—Ö–æ–¥—ñ–≤ –ø–µ—Ä–µ–¥ —Ñ—ñ–Ω–∞–ª—å–Ω–æ—é –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é.",
                "links": []
              },
              {
                "text": "–ü–∞—Ç–µ—Ä–Ω ReAct",
                "desc": "Reasoning + Acting ‚Äî –º–æ–¥–µ–ª—å —á–µ—Ä–≥—É—î –º—ñ–∂ –æ–±–º—ñ—Ä–∫–æ–≤—É–≤–∞–Ω–Ω—è–º —Ç–∞ –¥—ñ—è–º–∏ (–≤–∏–∫–ª–∏–∫–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤). –û—Å–Ω–æ–≤–∞ –±—ñ–ª—å—à–æ—Å—Ç—ñ –®–Ü-–∞–≥–µ–Ω—Ç—ñ–≤: —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è ‚Üí –¥—É–º–∫–∞ ‚Üí –¥—ñ—è ‚Üí —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Üí –Ω–æ–≤–∞ –¥—É–º–∫–∞.",
                "links": [
                  {
                    "title": "–ê–≥–µ–Ω—Ç–∏",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "–†–æ–ª—å–æ–≤–∏–π –ø—Ä–æ–º–ø—Ç–∏–Ω–≥",
                "desc": "\"–í–∏ –µ–∫—Å–ø–µ—Ä—Ç —É...\" –∞–∫—Ç–∏–≤—É—î –¥–æ–º–µ–Ω–Ω–æ-—Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –∑–Ω–∞–Ω–Ω—è —Ç–∞ —Å—Ç–∏–ª—å –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—ó. –ö–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è —Ä–æ–ª–µ–π –∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º–∏ —Å—Ç–≤–æ—Ä—é—î –ø–æ—Ç—É–∂–Ω—É —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—é –ø–µ—Ä—Å–æ–Ω –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤.",
                "links": []
              },
              {
                "text": "–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ–π–Ω–∏–π –ø—Ä–æ–º–ø—Ç–∏–Ω–≥",
                "desc": "–í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø—Ä–∏–Ω—Ü–∏–ø—ñ–≤ —Ç–∞ –ø—Ä–∞–≤–∏–ª, —è–∫–∏–º –º–æ–¥–µ–ª—å –ø–æ–≤–∏–Ω–Ω–∞ —Å–ª—ñ–¥—É–≤–∞—Ç–∏, –∞ –ø–æ—Ç—ñ–º —Å–∞–º–æ–æ—Ü—ñ–Ω–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è Anthropic –¥–ª—è –±–µ–∑–ø–µ–∫–∏ Claude ‚Äî –º–æ–¥–µ–ª—å –∫—Ä–∏—Ç–∏–∫—É—î —Ç–∞ –ø–µ—Ä–µ–≥–ª—è–¥–∞—î –≤–ª–∞—Å–Ω—ñ –≤–∏—Ö–æ–¥–∏.",
                "links": [
                  {
                    "title": "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è",
                    "href": "../level-5/alignment.html"
                  }
                ]
              },
              {
                "text": "–õ–∞–Ω—Ü—é–∂–æ–∫ –ø—Ä–æ–º–ø—Ç—ñ–≤",
                "desc": "–†–æ–∑–±–∏—Ç—Ç—è —Å–∫–ª–∞–¥–Ω–æ—ó –∑–∞–¥–∞—á—ñ –Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å –ø—Ä–æ—Å—Ç—ñ—à–∏—Ö –ø—Ä–æ–º–ø—Ç—ñ–≤, –¥–µ –∫–æ–∂–µ–Ω –≤–∏—Ö—ñ–¥ —Å—Ç–∞—î –≤—Ö–æ–¥–æ–º –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ. –î–æ–∑–≤–æ–ª—è—î —Å–∫–ª–∞–¥–Ω—ñ –≤–æ—Ä–∫—Ñ–ª–æ—É, –Ω–µ–º–æ–∂–ª–∏–≤—ñ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç—É.",
                "links": []
              },
              {
                "text": "–ú–µ—Ç–∞-–ø—Ä–æ–º–ø—Ç–∏–Ω–≥",
                "desc": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –®–Ü –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –ø—Ä–æ–º–ø—Ç—ñ–≤. –ü–æ–ø—Ä–æ—Å—ñ—Ç—å –º–æ–¥–µ–ª—å –Ω–∞–ø–∏—Å–∞—Ç–∏ –∫—Ä–∞—â—É –≤–µ—Ä—Å—ñ—é –≤–∞—à–æ–≥–æ –ø—Ä–æ–º–ø—Ç—É, –∞ –ø–æ—Ç—ñ–º –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ —Ü–µ–π –ø–æ–∫—Ä–∞—â–µ–Ω–∏–π –ø—Ä–æ–º–ø—Ç.",
                "links": []
              },
              {
                "text": "–ü—Ä–∏–º—É—Å–æ–≤–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –≤–∏—Ö—ñ–¥",
                "desc": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è JSON-—Å—Ö–µ–º, XML-—Ç–µ–≥—ñ–≤ –∞–±–æ markdown-—à–∞–±–ª–æ–Ω—ñ–≤ –¥–ª—è –æ–±–º–µ–∂–µ–Ω–Ω—è —Ñ–æ—Ä–º–∞—Ç—É –≤–∏—Ö–æ–¥—É. –£—Å—É–≤–∞—î –ø—Ä–æ–±–ª–µ–º–∏ –ø–∞—Ä—Å–∏–Ω–≥—É. –ë—ñ–ª—å—à—ñ—Å—Ç—å API —Ç–µ–ø–µ—Ä –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å –Ω–∞—Ç–∏–≤–Ω–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –≤–∏—Ö—ñ–¥.",
                "links": [
                  {
                    "title": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤",
                    "href": "tool-use.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Chain-of-Thought",
                "def": "Prompting technique that elicits step-by-step reasoning, dramatically improving accuracy on complex tasks."
              },
              {
                "term": "ReAct",
                "def": "Reasoning + Acting pattern where models alternate between thinking and taking actions with tools."
              },
              {
                "term": "Few-Shot",
                "def": "Providing examples in the prompt to demonstrate desired output format and quality."
              },
              {
                "term": "Prompt Chaining",
                "def": "Breaking complex tasks into sequences of simpler prompts, each building on the previous output."
              }
            ],
            "uk": [
              {
                "term": "–õ–∞–Ω—Ü—é–≥ –¥—É–º–æ–∫",
                "def": "–¢–µ—Ö–Ω—ñ–∫–∞ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É, —â–æ –≤–∏–∫–ª–∏–∫–∞—î –ø–æ–∫—Ä–æ–∫–æ–≤–µ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è, –¥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø–æ–∫—Ä–∞—â—É—é—á–∏ —Ç–æ—á–Ω—ñ—Å—Ç—å —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–¥–∞—á."
              },
              {
                "term": "ReAct",
                "def": "–ü–∞—Ç–µ—Ä–Ω –ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è + –î—ñ—è, –¥–µ –º–æ–¥–µ–ª—ñ —á–µ—Ä–≥—É—é—Ç—å –º—ñ–∂ –æ–±–º—ñ—Ä–∫–æ–≤—É–≤–∞–Ω–Ω—è–º —Ç–∞ –¥—ñ—è–º–∏ –∑ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏."
              },
              {
                "term": "Few-Shot",
                "def": "–ù–∞–¥–∞–Ω–Ω—è –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —É –ø—Ä–æ–º–ø—Ç—ñ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó –±–∞–∂–∞–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—É —Ç–∞ —è–∫–æ—Å—Ç—ñ –≤–∏—Ö–æ–¥—É."
              },
              {
                "term": "–õ–∞–Ω—Ü—é–∂–æ–∫ –ø—Ä–æ–º–ø—Ç—ñ–≤",
                "def": "–†–æ–∑–±–∏—Ç—Ç—è —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–¥–∞—á –Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –ø—Ä–æ—Å—Ç—ñ—à–∏—Ö –ø—Ä–æ–º–ø—Ç—ñ–≤, –∫–æ–∂–µ–Ω –∑ —è–∫–∏—Ö –±—É–¥—É—î –Ω–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–º—É –≤–∏—Ö–æ–¥—ñ."
              }
            ]
          },
          "tips": {
            "en": [
              "Start with zero-shot, add CoT if accuracy is low, then add few-shot examples if format is wrong",
              "For complex tasks, prompt chaining (multiple focused prompts) almost always beats a single long prompt",
              "Test your prompts with edge cases ‚Äî the prompt that works for typical inputs often fails on unusual ones"
            ],
            "uk": [
              "–ü–æ—á–Ω—ñ—Ç—å –∑ zero-shot, –¥–æ–¥–∞–π—Ç–µ CoT —è–∫—â–æ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∏–∑—å–∫–∞, –ø–æ—Ç—ñ–º –¥–æ–¥–∞–π—Ç–µ few-shot –ø—Ä–∏–∫–ª–∞–¥–∏ —è–∫—â–æ —Ñ–æ—Ä–º–∞—Ç –Ω–µ–≤—ñ—Ä–Ω–∏–π",
              "–î–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–¥–∞—á –ª–∞–Ω—Ü—é–∂–æ–∫ –ø—Ä–æ–º–ø—Ç—ñ–≤ (–∫—ñ–ª—å–∫–∞ —Ñ–æ–∫—É—Å–æ–≤–∞–Ω–∏—Ö) –º–∞–π–∂–µ –∑–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–º–∞–≥–∞—î –æ–¥–∏–Ω –¥–æ–≤–≥–∏–π –ø—Ä–æ–º–ø—Ç",
              "–¢–µ—Å—Ç—É–π—Ç–µ –ø—Ä–æ–º–ø—Ç–∏ –Ω–∞ –≥—Ä–∞–Ω–∏—á–Ω–∏—Ö –≤–∏–ø–∞–¥–∫–∞—Ö ‚Äî –ø—Ä–æ–º–ø—Ç —â–æ –ø—Ä–∞—Ü—é—î –¥–ª—è —Ç–∏–ø–æ–≤–∏—Ö –≤—Ö–æ–¥—ñ–≤ —á–∞—Å—Ç–æ –∑–±–æ—ó—Ç—å –Ω–∞ –Ω–µ–∑–≤–∏—á–∞–π–Ω–∏—Ö"
            ]
          },
          "related": [
            "Feed",
            "Agents & Tools"
          ]
        },
        {
          "slug": "base-tools",
          "title": {
            "en": "Tools & Libraries",
            "uk": "–Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —Ç–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏"
          },
          "desc": {
            "en": "Key frameworks and libraries for building AI-powered applications.",
            "uk": "–ö–ª—é—á–æ–≤—ñ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ —Ç–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –®–Ü-–¥–æ–¥–∞—Ç–∫—ñ–≤."
          },
          "overview": {
            "en": [
              "The AI development ecosystem has matured rapidly, with frameworks and libraries for every layer of the stack. From orchestration frameworks like LangChain that manage complex LLM workflows, to provider SDKs that handle API communication, to the Hugging Face ecosystem that democratizes access to thousands of models.",
              "Choosing the right tools depends on your use case. Quick prototypes might use LangChain for its batteries-included approach. Production systems often prefer direct SDK usage for control and reliability. Understanding the landscape helps you pick the right tool for each job."
            ],
            "uk": [
              "–ï–∫–æ—Å–∏—Å—Ç–µ–º–∞ —Ä–æ–∑—Ä–æ–±–∫–∏ –®–Ü —à–≤–∏–¥–∫–æ –¥–æ–∑—Ä—ñ–ª–∞, –∑ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º–∏ —Ç–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞–º–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è —Å—Ç–µ–∫—É. –í—ñ–¥ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∏—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫—ñ–≤ —è–∫ LangChain –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö LLM-–≤–æ—Ä–∫—Ñ–ª–æ—É, –¥–æ SDK –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤ –¥–ª—è API-–∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—ó, –¥–æ –µ–∫–æ—Å–∏—Å—Ç–µ–º–∏ Hugging Face, —â–æ –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑—É—î –¥–æ—Å—Ç—É–ø –¥–æ —Ç–∏—Å—è—á –º–æ–¥–µ–ª–µ–π.",
              "–í–∏–±—ñ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–∏—Ö —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –≤–∞—à–æ–≥–æ –≤–∏–ø–∞–¥–∫—É. –®–≤–∏–¥–∫—ñ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏ –º–æ–∂—É—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ LangChain –∑ –π–æ–≥–æ –ø–æ–≤–Ω–∏–º –Ω–∞–±–æ—Ä–æ–º. –ü—Ä–æ–¥–∞–∫—à–Ω-—Å–∏—Å—Ç–µ–º–∏ —á–∞—Å—Ç–æ –≤—ñ–¥–¥–∞—é—Ç—å –ø–µ—Ä–µ–≤–∞–≥—É –ø—Ä—è–º–æ–º—É SDK –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—é —Ç–∞ –Ω–∞–¥—ñ–π–Ω–æ—Å—Ç—ñ. –†–æ–∑—É–º—ñ–Ω–Ω—è –ª–∞–Ω–¥—à–∞—Ñ—Ç—É –¥–æ–ø–æ–º–∞–≥–∞—î –æ–±—Ä–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç."
            ]
          },
          "details": {
            "en": [
              {
                "text": "LangChain",
                "desc": "The most popular LLM orchestration framework. Provides chains (sequential LLM calls), agents (autonomous tool-using LLMs), memory (conversation persistence), and 700+ integrations.",
                "links": [
                  {
                    "title": "Agents",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "LlamaIndex",
                "desc": "Specialized for data retrieval and RAG. Connects to 160+ data sources, handles document loading, chunking, embedding, and querying. Ideal for building knowledge-base applications.",
                "links": [
                  {
                    "title": "RAG",
                    "href": "rag.html"
                  }
                ]
              },
              {
                "text": "Haystack",
                "desc": "End-to-end NLP/RAG framework from deepset. Pipeline-based architecture for search, question answering, and document processing. Strong focus on production-readiness.",
                "links": [
                  {
                    "title": "RAG",
                    "href": "rag.html"
                  }
                ]
              },
              {
                "text": "OpenAI SDK",
                "desc": "Official Python and Node.js SDKs for GPT models. Clean API for chat completions, function calling, embeddings, and assistants. The de facto standard that others follow.",
                "links": [
                  {
                    "title": "API Providers",
                    "href": "api-providers.html"
                  }
                ]
              },
              {
                "text": "Anthropic SDK",
                "desc": "Official SDK for Claude models. Supports messages API, tool use, streaming, vision, and prompt caching. Known for its clean, well-documented design.",
                "links": [
                  {
                    "title": "API Providers",
                    "href": "api-providers.html"
                  }
                ]
              },
              {
                "text": "Hugging Face Transformers",
                "desc": "The largest open-source ML library. 400K+ models, tokenizers, training pipelines, and inference tools. Bridges the gap between research and production for open models.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Vercel AI SDK",
                "desc": "Frontend-first AI SDK for React/Next.js. Handles streaming responses, tool calls, and multi-step interactions in the browser. Ideal for building AI-powered web apps.",
                "links": [
                  {
                    "title": "Applied Frameworks",
                    "href": "frameworks.html"
                  }
                ]
              },
              {
                "text": "Semantic Kernel",
                "desc": "Microsoft's AI orchestration SDK for .NET, Python, and Java. Integrates with Azure OpenAI and other providers. Enterprise-focused with plugin architecture.",
                "links": []
              },
              {
                "text": "Instructor & Outlines",
                "desc": "Libraries for structured output extraction. Instructor wraps LLM calls to return validated Pydantic models. Outlines enforces output schemas at the token generation level.",
                "links": [
                  {
                    "title": "Prompting Techniques",
                    "href": "prompting-techniques.html"
                  }
                ]
              },
              {
                "text": "Choosing Your Stack",
                "desc": "Prototype: LangChain for speed. Production: direct SDKs for control. RAG: LlamaIndex. Web apps: Vercel AI SDK. Open models: Hugging Face. Pick based on your deployment target and team skills.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "LangChain",
                "desc": "–ù–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü—ñ—ó LLM. –ù–∞–¥–∞—î –ª–∞–Ω—Ü—é–≥–∏ (–ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ –≤–∏–∫–ª–∏–∫–∏ LLM), –∞–≥–µ–Ω—Ç–∏ (–∞–≤—Ç–æ–Ω–æ–º–Ω—ñ LLM –∑ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏), –ø–∞–º'—è—Ç—å —Ç–∞ 700+ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ–π.",
                "links": [
                  {
                    "title": "–ê–≥–µ–Ω—Ç–∏",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "LlamaIndex",
                "desc": "–°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –¥–ª—è –ø–æ—à—É–∫—É –¥–∞–Ω–∏—Ö —Ç–∞ RAG. –ü—ñ–¥–∫–ª—é—á–∞—î—Ç—å—Å—è –¥–æ 160+ –¥–∂–µ—Ä–µ–ª –¥–∞–Ω–∏—Ö, –æ–±—Ä–æ–±–ª—è—î –¥–æ–∫—É–º–µ–Ω—Ç–∏, —á–∞–Ω–∫—ñ–Ω–≥, –µ–º–±–µ–¥—ñ–Ω–≥–∏ —Ç–∞ –∑–∞–ø–∏—Ç–∏. –Ü–¥–µ–∞–ª—å–Ω–∏–π –¥–ª—è –±–∞–∑ –∑–Ω–∞–Ω—å.",
                "links": [
                  {
                    "title": "RAG",
                    "href": "rag.html"
                  }
                ]
              },
              {
                "text": "Haystack",
                "desc": "–ù–∞—Å–∫—Ä—ñ–∑–Ω–∏–π NLP/RAG —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –≤—ñ–¥ deepset. –ü–∞–π–ø–ª–∞–π–Ω-–∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –ø–æ—à—É–∫—É, –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è —Ç–∞ –æ–±—Ä–æ–±–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤. –§–æ–∫—É—Å –Ω–∞ –ø—Ä–æ–¥–∞–∫—à–Ω-–≥–æ—Ç–æ–≤–Ω—ñ—Å—Ç—å.",
                "links": [
                  {
                    "title": "RAG",
                    "href": "rag.html"
                  }
                ]
              },
              {
                "text": "OpenAI SDK",
                "desc": "–û—Ñ—ñ—Ü—ñ–π–Ω—ñ Python —Ç–∞ Node.js SDK –¥–ª—è –º–æ–¥–µ–ª–µ–π GPT. –ß–∏—Å—Ç–∏–π API –¥–ª—è —á–∞—Ç-–∫–æ–º–ø–ª—ñ—Ü—ñ–π, –≤–∏–∫–ª–∏–∫—É —Ñ—É–Ω–∫—Ü—ñ–π, –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ —Ç–∞ –∞—Å–∏—Å—Ç–µ–Ω—Ç—ñ–≤. –î–µ-—Ñ–∞–∫—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≥–∞–ª—É–∑—ñ.",
                "links": [
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "api-providers.html"
                  }
                ]
              },
              {
                "text": "Anthropic SDK",
                "desc": "–û—Ñ—ñ—Ü—ñ–π–Ω–∏–π SDK –¥–ª—è –º–æ–¥–µ–ª–µ–π Claude. –ü—ñ–¥—Ç—Ä–∏–º—É—î messages API, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤, —Å—Ç—Ä—ñ–º—ñ–Ω–≥, –≤—ñ–∑—ñ—é —Ç–∞ –∫–µ—à—É–≤–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤. –í—ñ–¥–æ–º–∏–π —á–∏—Å—Ç–∏–º, –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤–∞–Ω–∏–º –¥–∏–∑–∞–π–Ω–æ–º.",
                "links": [
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "api-providers.html"
                  }
                ]
              },
              {
                "text": "Hugging Face Transformers",
                "desc": "–ù–∞–π–±—ñ–ª—å—à–∞ open-source ML –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞. 400K+ –º–æ–¥–µ–ª–µ–π, —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–∏, –ø–∞–π–ø–ª–∞–π–Ω–∏ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É. –ú–æ—Å—Ç –º—ñ–∂ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è–º–∏ —Ç–∞ –ø—Ä–æ–¥–∞–∫—à–Ω–æ–º.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Vercel AI SDK",
                "desc": "–§—Ä–æ–Ω—Ç–µ–Ω–¥-first –®–Ü SDK –¥–ª—è React/Next.js. –û–±—Ä–æ–±–ª—è—î —Å—Ç—Ä—ñ–º—ñ–Ω–≥ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π, –≤–∏–∫–ª–∏–∫–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ —Ç–∞ –±–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤—ñ –≤–∑–∞—î–º–æ–¥—ñ—ó –≤ –±—Ä–∞—É–∑–µ—Ä—ñ. –Ü–¥–µ–∞–ª—å–Ω–∏–π –¥–ª—è –®–Ü –≤–µ–±-–¥–æ–¥–∞—Ç–∫—ñ–≤.",
                "links": [
                  {
                    "title": "–ü—Ä–∏–∫–ª–∞–¥–Ω—ñ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏",
                    "href": "frameworks.html"
                  }
                ]
              },
              {
                "text": "Semantic Kernel",
                "desc": "SDK –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü—ñ—ó –®–Ü –≤—ñ–¥ Microsoft –¥–ª—è .NET, Python —Ç–∞ Java. –Ü–Ω—Ç–µ–≥—Ä—É—î—Ç—å—Å—è –∑ Azure OpenAI —Ç–∞ —ñ–Ω—à–∏–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏. –ï–Ω—Ç–µ—Ä–ø—Ä–∞–π–∑-—Ñ–æ–∫—É—Å –∑ –ø–ª–∞–≥—ñ–Ω-–∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–æ—é.",
                "links": []
              },
              {
                "text": "Instructor —Ç–∞ Outlines",
                "desc": "–ë—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–æ–≥–æ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è. Instructor –æ–±–≥–æ—Ä—Ç–∞—î –≤–∏–∫–ª–∏–∫–∏ LLM –¥–ª—è –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è –≤–∞–ª—ñ–¥–æ–≤–∞–Ω–∏—Ö Pydantic –º–æ–¥–µ–ª–µ–π. Outlines –ø—Ä–∏–º—É—Å–æ–≤–æ –¥–æ—Ç—Ä–∏–º—É—î—Ç—å—Å—è —Å—Ö–µ–º –Ω–∞ —Ä—ñ–≤–Ω—ñ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–æ–∫–µ–Ω—ñ–≤.",
                "links": [
                  {
                    "title": "–¢–µ—Ö–Ω—ñ–∫–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É",
                    "href": "prompting-techniques.html"
                  }
                ]
              },
              {
                "text": "–í–∏–±—ñ—Ä –≤–∞—à–æ–≥–æ —Å—Ç–µ–∫—É",
                "desc": "–ü—Ä–æ—Ç–æ—Ç–∏–ø: LangChain –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ. –ü—Ä–æ–¥–∞–∫—à–Ω: –ø—Ä—è–º—ñ SDK –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—é. RAG: LlamaIndex. –í–µ–±-–¥–æ–¥–∞—Ç–∫–∏: Vercel AI SDK. –í—ñ–¥–∫—Ä–∏—Ç—ñ –º–æ–¥–µ–ª—ñ: Hugging Face. –û–±–∏—Ä–∞–π—Ç–µ –∑–∞ —Ü—ñ–ª–ª—é —Ç–∞ –Ω–∞–≤–∏—á–∫–∞–º–∏.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "LangChain",
                "def": "Most popular LLM orchestration framework with chains, agents, memory, and 700+ integrations."
              },
              {
                "term": "LlamaIndex",
                "def": "Data framework for LLM applications specializing in ingestion, indexing, and retrieval."
              },
              {
                "term": "Hugging Face",
                "def": "Open-source AI platform hosting 400K+ models and the Transformers library for ML development."
              },
              {
                "term": "SDK",
                "def": "Software Development Kit ‚Äî library providing programmatic access to an AI provider's models and APIs."
              }
            ],
            "uk": [
              {
                "term": "LangChain",
                "def": "–ù–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü—ñ—ó LLM –∑ –ª–∞–Ω—Ü—é–≥–∞–º–∏, –∞–≥–µ–Ω—Ç–∞–º–∏, –ø–∞–º'—è—Ç—Ç—é —Ç–∞ 700+ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è–º–∏."
              },
              {
                "term": "LlamaIndex",
                "def": "–§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–∞–Ω–∏—Ö –¥–ª—è LLM-–¥–æ–¥–∞—Ç–∫—ñ–≤, —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –Ω–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ, —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó —Ç–∞ –ø–æ—à—É–∫—É."
              },
              {
                "term": "Hugging Face",
                "def": "Open-source –®–Ü-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∑ 400K+ –º–æ–¥–µ–ª—è–º–∏ —Ç–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–æ—é Transformers –¥–ª—è ML-—Ä–æ–∑—Ä–æ–±–∫–∏."
              },
              {
                "term": "SDK",
                "def": "Software Development Kit ‚Äî –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø—É –¥–æ –º–æ–¥–µ–ª–µ–π —Ç–∞ API –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –®–Ü."
              }
            ]
          },
          "tips": {
            "en": [
              "Start with a provider SDK directly (OpenAI or Anthropic) before adding LangChain ‚Äî understand what the abstraction hides",
              "For RAG projects, LlamaIndex saves weeks of work on document processing and chunking pipelines",
              "Always version-pin your AI library dependencies ‚Äî these ecosystems change rapidly and breaking changes are common"
            ],
            "uk": [
              "–ü–æ—á–Ω—ñ—Ç—å –∑ SDK –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –Ω–∞–ø—Ä—è–º—É (OpenAI –∞–±–æ Anthropic) –ø–µ—Ä–µ–¥ –¥–æ–¥–∞–≤–∞–Ω–Ω—è–º LangChain ‚Äî –∑—Ä–æ–∑—É–º—ñ–π—Ç–µ —â–æ —Ö–æ–≤–∞—î –∞–±—Å—Ç—Ä–∞–∫—Ü—ñ—è",
              "–î–ª—è RAG-–ø—Ä–æ—î–∫—Ç—ñ–≤ LlamaIndex –µ–∫–æ–Ω–æ–º–∏—Ç—å —Ç–∏–∂–Ω—ñ —Ä–æ–±–æ—Ç–∏ –Ω–∞–¥ –æ–±—Ä–æ–±–∫–æ—é –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —Ç–∞ —á–∞–Ω–∫—ñ–Ω–≥-–ø–∞–π–ø–ª–∞–π–Ω–∞–º–∏",
              "–ó–∞–≤–∂–¥–∏ —Ñ—ñ–∫—Å—É–π—Ç–µ –≤–µ—Ä—Å—ñ—ó –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π –®–Ü-–±—ñ–±–ª—ñ–æ—Ç–µ–∫ ‚Äî —Ü—ñ –µ–∫–æ—Å–∏—Å—Ç–µ–º–∏ —à–≤–∏–¥–∫–æ –∑–º—ñ–Ω—é—é—Ç—å—Å—è —ñ –ª–∞–º–∞—é—á—ñ –∑–º—ñ–Ω–∏ —á–∞—Å—Ç—ñ"
            ]
          },
          "related": [
            "Agents & Tools",
            "Video Content"
          ]
        },
        {
          "slug": "agents",
          "title": {
            "en": "Agents",
            "uk": "–ê–≥–µ–Ω—Ç–∏"
          },
          "desc": {
            "en": "AI agents that can plan, reason, and take actions autonomously.",
            "uk": "–ê–≥–µ–Ω—Ç–∏ –®–Ü, —â–æ –º–æ–∂—É—Ç—å –ø–ª–∞–Ω—É–≤–∞—Ç–∏, –º—ñ—Ä–∫—É–≤–∞—Ç–∏ —Ç–∞ –¥—ñ—è—Ç–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ."
          },
          "overview": {
            "en": [
              "AI agents are systems that use LLMs as their \"brain\" to perceive their environment, plan actions, execute them using tools, and iterate based on results. Unlike simple chatbots that respond to one message at a time, agents can pursue multi-step goals autonomously ‚Äî browsing the web, writing code, managing files, and calling APIs.",
              "The agent paradigm is where AI moves from tool to collaborator. Agents like Claude Code, Devin, and OpenAI's operator can complete complex tasks that would take humans hours. The key challenge is reliability ‚Äî agents work best when given clear goals, appropriate tools, and guardrails to prevent harmful actions."
            ],
            "uk": [
              "–®–Ü-–∞–≥–µ–Ω—Ç–∏ ‚Äî —Ü–µ —Å–∏—Å—Ç–µ–º–∏, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å LLM —è–∫ —Å–≤—ñ–π \"–º–æ–∑–æ–∫\" –¥–ª—è —Å–ø—Ä–∏–π–Ω—è—Ç—Ç—è —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞, –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è –¥—ñ–π, —ó—Ö –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ —Ç–∞ —ñ—Ç–µ—Ä–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤. –ù–∞ –≤—ñ–¥–º—ñ–Ω—É –≤—ñ–¥ –ø—Ä–æ—Å—Ç–∏—Ö —á–∞—Ç–±–æ—Ç—ñ–≤, –∞–≥–µ–Ω—Ç–∏ –º–æ–∂—É—Ç—å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –ø–µ—Ä–µ—Å–ª—ñ–¥—É–≤–∞—Ç–∏ –±–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤—ñ —Ü—ñ–ª—ñ ‚Äî –±—Ä–∞—É–∑–∏—Ç–∏ –≤–µ–±, –ø–∏—Å–∞—Ç–∏ –∫–æ–¥, –∫–µ—Ä—É–≤–∞—Ç–∏ —Ñ–∞–π–ª–∞–º–∏ —Ç–∞ –≤–∏–∫–ª–∏–∫–∞—Ç–∏ API.",
              "–ü–∞—Ä–∞–¥–∏–≥–º–∞ –∞–≥–µ–Ω—Ç—ñ–≤ ‚Äî —Ü–µ –¥–µ –®–Ü –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –≤—ñ–¥ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –¥–æ —Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫–∞. –ê–≥–µ–Ω—Ç–∏ —è–∫ Claude Code, Devin —Ç–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä OpenAI –º–æ–∂—É—Ç—å –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–¥–∞—á—ñ, —â–æ –∑–∞–π–Ω—è–ª–∏ –± –ª—é–¥–∏–Ω—ñ –≥–æ–¥–∏–Ω–∏. –ö–ª—é—á–æ–≤–∏–π –≤–∏–∫–ª–∏–∫ ‚Äî –Ω–∞–¥—ñ–π–Ω—ñ—Å—Ç—å: –∞–≥–µ–Ω—Ç–∏ –ø—Ä–∞—Ü—é—é—Ç—å –Ω–∞–π–∫—Ä–∞—â–µ –∑ —á—ñ—Ç–∫–∏–º–∏ —Ü—ñ–ª—è–º–∏, –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–º–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —Ç–∞ –∑–∞—Ö–∏—Å–Ω–∏–º–∏ –±–∞—Ä'—î—Ä–∞–º–∏."
            ]
          },
          "details": {
            "en": [
              {
                "text": "What Is an AI Agent",
                "desc": "A system with an LLM core that perceives, plans, acts, and iterates. The perception-planning-action loop runs until the goal is achieved or a stopping condition is met.",
                "links": [
                  {
                    "title": "Generative AI",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "Agent Architectures",
                "desc": "ReAct (reason then act), Plan-and-Execute (create plan first, then execute steps), and Reflexion (self-critique and retry). Each architecture trades off between speed and reliability.",
                "links": [
                  {
                    "title": "Prompting Techniques",
                    "href": "prompting-techniques.html"
                  }
                ]
              },
              {
                "text": "Multi-Agent Systems",
                "desc": "Multiple agents collaborating on a task. CrewAI assigns roles (researcher, writer, reviewer), AutoGen enables agent conversations, MetaGPT simulates software teams. Powerful but complex.",
                "links": []
              },
              {
                "text": "Agent Memory",
                "desc": "Short-term (current conversation), long-term (persisted knowledge), and episodic (memories of past tasks). Effective memory management is what makes agents improve over time.",
                "links": [
                  {
                    "title": "Context",
                    "href": "../level-2/context.html"
                  }
                ]
              },
              {
                "text": "Tool Use in Agents",
                "desc": "Agents extend their capabilities through function calling ‚Äî web search, code execution, file I/O, API calls. The quality and breadth of available tools directly determines what an agent can accomplish.",
                "links": [
                  {
                    "title": "Tool Use",
                    "href": "tool-use.html"
                  }
                ]
              },
              {
                "text": "Coding Agents",
                "desc": "Claude Code, Cursor, Devin, GitHub Copilot Workspace ‚Äî agents that can read codebases, write code, run tests, and iterate on bugs autonomously. The fastest-growing agent category.",
                "links": [
                  {
                    "title": "Vibecoding",
                    "href": "../level-2/vibecoding.html"
                  }
                ]
              },
              {
                "text": "Browser & Computer Agents",
                "desc": "Agents that can control a browser or desktop ‚Äî clicking, typing, navigating. Anthropic's computer use, OpenAI operator, and browser-use frameworks enable real-world task automation.",
                "links": []
              },
              {
                "text": "Agent Evaluation",
                "desc": "Measuring agent performance is hard ‚Äî tasks are open-ended and multi-step. Benchmarks like SWE-bench (code), WebArena (browser), and GAIA (general) attempt to standardize evaluation.",
                "links": []
              },
              {
                "text": "Agent Safety",
                "desc": "Agents can take irreversible actions (delete files, send emails, modify databases). Sandboxing, confirmation gates, and principle-of-least-privilege tool access are critical safety measures.",
                "links": [
                  {
                    "title": "AI Safety",
                    "href": "../level-5/ai-safety.html"
                  }
                ]
              },
              {
                "text": "The Agentic Future",
                "desc": "Agents are evolving from single-purpose to general-purpose. The trajectory: task-specific bots ‚Üí coding agents ‚Üí computer-using agents ‚Üí fully autonomous assistants. We are early in this progression.",
                "links": [
                  {
                    "title": "AGI",
                    "href": "../level-5/agi.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–©–æ —Ç–∞–∫–µ –®–Ü-–∞–≥–µ–Ω—Ç",
                "desc": "–°–∏—Å—Ç–µ–º–∞ –∑ LLM-—è–¥—Ä–æ–º, —â–æ —Å–ø—Ä–∏–π–º–∞—î, –ø–ª–∞–Ω—É—î, –¥—ñ—î —Ç–∞ —ñ—Ç–µ—Ä—É—î. –¶–∏–∫–ª —Å–ø—Ä–∏–π–Ω—è—Ç—Ç—è-–ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è-–¥—ñ—ó –ø—Ä–∞—Ü—é—î –¥–æ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è —Ü—ñ–ª—ñ –∞–±–æ —É–º–æ–≤–∏ –∑—É–ø–∏–Ω–∫–∏.",
                "links": [
                  {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –∞–≥–µ–Ω—Ç—ñ–≤",
                "desc": "ReAct (–º—ñ—Ä–∫—É–π –ø–æ—Ç—ñ–º –¥—ñ–π), Plan-and-Execute (—Å–ø–æ—á–∞—Ç–∫—É –ø–ª–∞–Ω, –ø–æ—Ç—ñ–º –≤–∏–∫–æ–Ω–∞–Ω–Ω—è), Reflexion (—Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–∞ —Ç–∞ –ø–æ–≤—Ç–æ—Ä). –ö–æ–∂–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –±–∞–ª–∞–Ω—Å—É—î —à–≤–∏–¥–∫—ñ—Å—Ç—å —Ç–∞ –Ω–∞–¥—ñ–π–Ω—ñ—Å—Ç—å.",
                "links": [
                  {
                    "title": "–¢–µ—Ö–Ω—ñ–∫–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É",
                    "href": "prompting-techniques.html"
                  }
                ]
              },
              {
                "text": "–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ñ —Å–∏—Å—Ç–µ–º–∏",
                "desc": "–ö—ñ–ª—å–∫–∞ –∞–≥–µ–Ω—Ç—ñ–≤, —â–æ —Å–ø—ñ–≤–ø—Ä–∞—Ü—é—é—Ç—å –Ω–∞–¥ –∑–∞–¥–∞—á–µ—é. CrewAI –ø—Ä–∏–∑–Ω–∞—á–∞—î —Ä–æ–ª—ñ (–¥–æ—Å–ª—ñ–¥–Ω–∏–∫, –ø–∏—Å—å–º–µ–Ω–Ω–∏–∫, —Ä–µ–≤—é–µ—Ä), AutoGen –∑–∞–±–µ–∑–ø–µ—á—É—î —Ä–æ–∑–º–æ–≤–∏ –∞–≥–µ–Ω—Ç—ñ–≤, MetaGPT —Å–∏–º—É–ª—é—î –∫–æ–º–∞–Ω–¥–∏ —Ä–æ–∑—Ä–æ–±–Ω–∏–∫—ñ–≤.",
                "links": []
              },
              {
                "text": "–ü–∞–º'—è—Ç—å –∞–≥–µ–Ω—Ç—ñ–≤",
                "desc": "–ö–æ—Ä–æ—Ç–∫–æ—á–∞—Å–Ω–∞ (–ø–æ—Ç–æ—á–Ω–∞ —Ä–æ–∑–º–æ–≤–∞), –¥–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–∞ (–∑–±–µ—Ä–µ–∂–µ–Ω—ñ –∑–Ω–∞–Ω–Ω—è) —Ç–∞ –µ–ø—ñ–∑–æ–¥–∏—á–Ω–∞ (—Å–ø–æ–≥–∞–¥–∏ –ø—Ä–æ –º–∏–Ω—É–ª—ñ –∑–∞–¥–∞—á—ñ). –ï—Ñ–µ–∫—Ç–∏–≤–Ω–µ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –ø–∞–º'—è—Ç—Ç—é —Ä–æ–±–∏—Ç—å –∞–≥–µ–Ω—Ç—ñ–≤ –∫—Ä–∞—â–∏–º–∏ –∑ —á–∞—Å–æ–º.",
                "links": [
                  {
                    "title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç",
                    "href": "../level-2/context.html"
                  }
                ]
              },
              {
                "text": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ –≤ –∞–≥–µ–Ω—Ç–∞—Ö",
                "desc": "–ê–≥–µ–Ω—Ç–∏ —Ä–æ–∑—à–∏—Ä—é—é—Ç—å –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —á–µ—Ä–µ–∑ –≤–∏–∫–ª–∏–∫ —Ñ—É–Ω–∫—Ü—ñ–π ‚Äî –ø–æ—à—É–∫, –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∫–æ–¥—É, —Ñ–∞–π–ª–æ–≤–∏–π I/O, –≤–∏–∫–ª–∏–∫–∏ API. –Ø–∫—ñ—Å—Ç—å —Ç–∞ —à–∏—Ä–æ—Ç–∞ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ –≤–∏–∑–Ω–∞—á–∞—î –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –∞–≥–µ–Ω—Ç–∞.",
                "links": [
                  {
                    "title": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤",
                    "href": "tool-use.html"
                  }
                ]
              },
              {
                "text": "–ö–æ–¥—É—é—á—ñ –∞–≥–µ–Ω—Ç–∏",
                "desc": "Claude Code, Cursor, Devin, GitHub Copilot Workspace ‚Äî –∞–≥–µ–Ω—Ç–∏ —â–æ —á–∏—Ç–∞—é—Ç—å –∫–æ–¥–æ–≤—ñ –±–∞–∑–∏, –ø–∏—à—É—Ç—å –∫–æ–¥, –∑–∞–ø—É—Å–∫–∞—é—Ç—å —Ç–µ—Å—Ç–∏ —Ç–∞ —ñ—Ç–µ—Ä—É—é—Ç—å –Ω–∞–¥ –±–∞–≥–∞–º–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ. –ö–∞—Ç–µ–≥–æ—Ä—ñ—è –∞–≥–µ–Ω—Ç—ñ–≤ —â–æ –Ω–∞–π—à–≤–∏–¥—à–µ –∑—Ä–æ—Å—Ç–∞—î.",
                "links": [
                  {
                    "title": "–í–∞–π–±–∫–æ–¥–∏–Ω–≥",
                    "href": "../level-2/vibecoding.html"
                  }
                ]
              },
              {
                "text": "–ë—Ä–∞—É–∑–µ—Ä–Ω—ñ —Ç–∞ –∫–æ–º–ø'—é—Ç–µ—Ä–Ω—ñ –∞–≥–µ–Ω—Ç–∏",
                "desc": "–ê–≥–µ–Ω—Ç–∏, —â–æ –∫–µ—Ä—É—é—Ç—å –±—Ä–∞—É–∑–µ—Ä–æ–º –∞–±–æ –¥–µ—Å–∫—Ç–æ–ø–æ–º ‚Äî –∫–ª—ñ–∫–∞—é—Ç—å, –¥—Ä—É–∫—É—é—Ç—å, –Ω–∞–≤—ñ–≥—É—é—Ç—å. Computer use –≤—ñ–¥ Anthropic, –æ–ø–µ—Ä–∞—Ç–æ—Ä OpenAI —Ç–∞ browser-use —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó —Ä–µ–∞–ª—å–Ω–∏—Ö –∑–∞–¥–∞—á.",
                "links": []
              },
              {
                "text": "–û—Ü—ñ–Ω–∫–∞ –∞–≥–µ–Ω—Ç—ñ–≤",
                "desc": "–í–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –∞–≥–µ–Ω—Ç—ñ–≤ —Å–∫–ª–∞–¥–Ω–µ ‚Äî –∑–∞–¥–∞—á—ñ –≤—ñ–¥–∫—Ä–∏—Ç—ñ —Ç–∞ –±–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤—ñ. –ë–µ–Ω—á–º–∞—Ä–∫–∏ SWE-bench (–∫–æ–¥), WebArena (–±—Ä–∞—É–∑–µ—Ä), GAIA (–∑–∞–≥–∞–ª—å–Ω–∏–π) —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑—É—é—Ç—å –æ—Ü—ñ–Ω–∫—É.",
                "links": []
              },
              {
                "text": "–ë–µ–∑–ø–µ–∫–∞ –∞–≥–µ–Ω—Ç—ñ–≤",
                "desc": "–ê–≥–µ–Ω—Ç–∏ –º–æ–∂—É—Ç—å –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –Ω–µ–∑–≤–æ—Ä–æ—Ç–Ω—ñ –¥—ñ—ó (–≤–∏–¥–∞–ª–µ–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤, –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è –ª–∏—Å—Ç—ñ–≤, –∑–º—ñ–Ω–∞ –±–∞–∑ –¥–∞–Ω–∏—Ö). –ü—ñ—Å–æ—á–Ω–∏—Ü—ñ, –≥–µ–π—Ç–∏ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è —Ç–∞ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ –ø—Ä–∏–≤—ñ–ª–µ—ó ‚Äî –∫—Ä–∏—Ç–∏—á–Ω—ñ –∑–∞—Ö–æ–¥–∏ –±–µ–∑–ø–µ–∫–∏.",
                "links": [
                  {
                    "title": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü",
                    "href": "../level-5/ai-safety.html"
                  }
                ]
              },
              {
                "text": "–ê–≥–µ–Ω—Ç–Ω–µ –º–∞–π–±—É—Ç–Ω—î",
                "desc": "–ê–≥–µ–Ω—Ç–∏ –µ–≤–æ–ª—é—Ü—ñ–æ–Ω—É—é—Ç—å –≤—ñ–¥ –æ–¥–Ω–æ—Ü—ñ–ª—å–æ–≤–∏—Ö –¥–æ –∑–∞–≥–∞–ª—å–Ω–æ—Ü—ñ–ª—å–æ–≤–∏—Ö. –¢—Ä–∞—î–∫—Ç–æ—Ä—ñ—è: –∑–∞–¥–∞—á–∞-—Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –±–æ—Ç–∏ ‚Üí –∫–æ–¥—É—é—á—ñ –∞–≥–µ–Ω—Ç–∏ ‚Üí –∫–æ–º–ø'—é—Ç–µ—Ä–Ω—ñ –∞–≥–µ–Ω—Ç–∏ ‚Üí –ø–æ–≤–Ω—ñ—Å—Ç—é –∞–≤—Ç–æ–Ω–æ–º–Ω—ñ –∞—Å–∏—Å—Ç–µ–Ω—Ç–∏.",
                "links": [
                  {
                    "title": "AGI",
                    "href": "../level-5/agi.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "AI Agent",
                "def": "An LLM-powered system that autonomously perceives, plans, acts, and iterates to achieve goals."
              },
              {
                "term": "ReAct",
                "def": "Agent architecture alternating between reasoning about what to do and taking actions with tools."
              },
              {
                "term": "Multi-Agent System",
                "def": "Multiple AI agents with different roles collaborating to solve complex tasks together."
              },
              {
                "term": "Tool Calling",
                "def": "The mechanism by which agents invoke external functions, APIs, or services to extend their capabilities."
              }
            ],
            "uk": [
              {
                "term": "–®–Ü-–∞–≥–µ–Ω—Ç",
                "def": "–°–∏—Å—Ç–µ–º–∞ –Ω–∞ –±–∞–∑—ñ LLM, —â–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ —Å–ø—Ä–∏–π–º–∞—î, –ø–ª–∞–Ω—É—î, –¥—ñ—î —Ç–∞ —ñ—Ç–µ—Ä—É—î –¥–ª—è –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è —Ü—ñ–ª–µ–π."
              },
              {
                "term": "ReAct",
                "def": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –∞–≥–µ–Ω—Ç–∞, —â–æ —á–µ—Ä–≥—É—î –º—ñ–∂ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è–º –ø—Ä–æ –¥—ñ—ó —Ç–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è–º –¥—ñ–π –∑ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏."
              },
              {
                "term": "–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞ —Å–∏—Å—Ç–µ–º–∞",
                "def": "–ö—ñ–ª—å–∫–∞ –®–Ü-–∞–≥–µ–Ω—Ç—ñ–≤ –∑ —Ä—ñ–∑–Ω–∏–º–∏ —Ä–æ–ª—è–º–∏, —â–æ —Å–ø—ñ–≤–ø—Ä–∞—Ü—é—é—Ç—å –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–¥–∞—á."
              },
              {
                "term": "–í–∏–∫–ª–∏–∫ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤",
                "def": "–ú–µ—Ö–∞–Ω—ñ–∑–º, —á–µ—Ä–µ–∑ —è–∫–∏–π –∞–≥–µ–Ω—Ç–∏ –≤–∏–∫–ª–∏–∫–∞—é—Ç—å –∑–æ–≤–Ω—ñ—à–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó, API –∞–±–æ —Å–µ—Ä–≤—ñ—Å–∏ –¥–ª—è —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π."
              }
            ]
          },
          "tips": {
            "en": [
              "Start with single-agent systems before attempting multi-agent ‚Äî complexity grows exponentially with each agent added",
              "Always implement confirmation gates for irreversible actions (file deletion, sending messages, database writes)",
              "The most reliable agents have narrow, well-defined tool sets rather than access to everything"
            ],
            "uk": [
              "–ü–æ—á–Ω—ñ—Ç—å –∑ –æ–¥–Ω–æ–∞–≥–µ–Ω—Ç–Ω–∏—Ö —Å–∏—Å—Ç–µ–º –ø–µ—Ä–µ–¥ —Å–ø—Ä–æ–±–æ—é –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∏—Ö ‚Äî —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –∑—Ä–æ—Å—Ç–∞—î –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–æ –∑ –∫–æ–∂–Ω–∏–º –∞–≥–µ–Ω—Ç–æ–º",
              "–ó–∞–≤–∂–¥–∏ –≤–ø—Ä–æ–≤–∞–¥–∂—É–π—Ç–µ –≥–µ–π—Ç–∏ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è –¥–ª—è –Ω–µ–∑–≤–æ—Ä–æ—Ç–Ω–∏—Ö –¥—ñ–π (–≤–∏–¥–∞–ª–µ–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤, –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å, –∑–∞–ø–∏—Å –≤ –ë–î)",
              "–ù–∞–π–Ω–∞–¥—ñ–π–Ω—ñ—à—ñ –∞–≥–µ–Ω—Ç–∏ –º–∞—é—Ç—å –≤—É–∑—å–∫—ñ, —á—ñ—Ç–∫–æ –≤–∏–∑–Ω–∞—á–µ–Ω—ñ –Ω–∞–±–æ—Ä–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤, –∞ –Ω–µ –¥–æ—Å—Ç—É–ø –¥–æ –≤—Å—å–æ–≥–æ"
            ]
          },
          "related": [
            "Agents & Tools",
            "Video Content"
          ]
        },
        {
          "slug": "tool-use",
          "title": {
            "en": "Tool Use",
            "uk": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤"
          },
          "desc": {
            "en": "Extending AI capabilities through function calling and external tool integration.",
            "uk": "–†–æ–∑—à–∏—Ä–µ–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –®–Ü —á–µ—Ä–µ–∑ –≤–∏–∫–ª–∏–∫ —Ñ—É–Ω–∫—Ü—ñ–π —Ç–∞ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—é –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤."
          },
          "overview": {
            "en": [
              "Tool use (function calling) is the mechanism that transforms LLMs from text generators into capable agents. Instead of just producing text, models can invoke external functions ‚Äî search the web, query databases, execute code, call APIs ‚Äî and incorporate the results into their responses.",
              "Every major provider now supports tool use: OpenAI, Anthropic, and Google each have their own function calling APIs. The pattern is universal: you define available tools with JSON schemas, the model decides when to call them, and your code executes the actual function and returns results."
            ],
            "uk": [
              "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ (–≤–∏–∫–ª–∏–∫ —Ñ—É–Ω–∫—Ü—ñ–π) ‚Äî —Ü–µ –º–µ—Ö–∞–Ω—ñ–∑–º, —â–æ –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î LLM –∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ñ–≤ —Ç–µ–∫—Å—Ç—É –Ω–∞ –∑–¥–∞—Ç–Ω–∏—Ö –∞–≥–µ–Ω—Ç—ñ–≤. –ó–∞–º—ñ—Å—Ç—å –ø—Ä–æ—Å—Ç–æ –≤–∏–¥–∞—á—ñ —Ç–µ–∫—Å—Ç—É, –º–æ–¥–µ–ª—ñ –º–æ–∂—É—Ç—å –≤–∏–∫–ª–∏–∫–∞—Ç–∏ –∑–æ–≤–Ω—ñ—à–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó ‚Äî –ø–æ—à—É–∫, –∑–∞–ø–∏—Ç–∏ –¥–æ –±–∞–∑ –¥–∞–Ω–∏—Ö, –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∫–æ–¥—É, –≤–∏–∫–ª–∏–∫–∏ API ‚Äî —Ç–∞ –≤–∫–ª—é—á–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.",
              "–ö–æ–∂–µ–Ω –≤–µ–ª–∏–∫–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Ç–µ–ø–µ—Ä –ø—ñ–¥—Ç—Ä–∏–º—É—î –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤: OpenAI, Anthropic —Ç–∞ Google –º–∞—é—Ç—å –≤–ª–∞—Å–Ω—ñ API –≤–∏–∫–ª–∏–∫—É —Ñ—É–Ω–∫—Ü—ñ–π. –ü–∞—Ç–µ—Ä–Ω —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π: –≤–∏ –≤–∏–∑–Ω–∞—á–∞—î—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ JSON-—Å—Ö–µ–º–∞–º–∏, –º–æ–¥–µ–ª—å –≤–∏—Ä—ñ—à—É—î –∫–æ–ª–∏ —ó—Ö –≤–∏–∫–ª–∏–∫–∞—Ç–∏, –∞ –≤–∞—à –∫–æ–¥ –≤–∏–∫–æ–Ω—É—î —Ñ—É–Ω–∫—Ü—ñ—é —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Function Calling Basics",
                "desc": "Define tools with name, description, and JSON Schema parameters. The model generates a structured tool call instead of text. Your code executes it and returns results for the model to use.",
                "links": [
                  {
                    "title": "API Providers",
                    "href": "api-providers.html"
                  }
                ]
              },
              {
                "text": "OpenAI Function Calling",
                "desc": "tools array with function definitions. Supports parallel_tool_calls, tool_choice forcing, and strict mode for guaranteed schema adherence. The original and most widely adopted approach.",
                "links": []
              },
              {
                "text": "Anthropic Tool Use",
                "desc": "Claude's tool_use blocks in the messages API. Supports nested tools, tool_choice (auto/any/tool), and streaming tool calls. Known for high accuracy in complex multi-tool scenarios.",
                "links": []
              },
              {
                "text": "Parallel Tool Calls",
                "desc": "Models can invoke multiple tools simultaneously when tasks are independent. Searching multiple databases, calling several APIs at once. Dramatically speeds up agent workflows.",
                "links": [
                  {
                    "title": "Agents",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "Multi-Step Tool Use",
                "desc": "The model calls a tool, gets results, reasons about them, then calls another tool. This loop enables complex workflows: search ‚Üí read ‚Üí analyze ‚Üí write. Most real tasks require multiple steps.",
                "links": []
              },
              {
                "text": "Building Custom Tools",
                "desc": "Define tools for your specific domain: inventory lookup, CRM queries, internal API calls, data transformations. Good tool descriptions are critical ‚Äî the model decides when to use tools based on descriptions.",
                "links": []
              },
              {
                "text": "MCP (Model Context Protocol)",
                "desc": "Anthropic's open standard for connecting AI models to tools and data sources. Servers expose tools via a standard protocol, enabling plug-and-play tool integration across different AI applications.",
                "links": [
                  {
                    "title": "AI Protocols",
                    "href": "ai-protocols.html"
                  }
                ]
              },
              {
                "text": "Computer Use",
                "desc": "Using the screen as a tool ‚Äî the model sees screenshots and generates mouse/keyboard actions. Anthropic's computer use enables AI to operate any software, not just API-enabled ones.",
                "links": []
              },
              {
                "text": "Error Handling",
                "desc": "Tools fail ‚Äî APIs timeout, queries return errors, permissions are denied. Robust tool implementations return clear error messages so the model can retry, use alternatives, or explain the failure.",
                "links": [
                  {
                    "title": "Hallucinations",
                    "href": "../level-2/hallucination.html"
                  }
                ]
              },
              {
                "text": "Security Considerations",
                "desc": "Tool calls can have real-world side effects. Validate parameters, sanitize inputs, implement rate limits, and use principle of least privilege. Never give models unrestricted database write access.",
                "links": [
                  {
                    "title": "AI Safety",
                    "href": "../level-5/ai-safety.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–û—Å–Ω–æ–≤–∏ –≤–∏–∫–ª–∏–∫—É —Ñ—É–Ω–∫—Ü—ñ–π",
                "desc": "–í–∏–∑–Ω–∞—á—Ç–µ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –∑ —ñ–º'—è–º, –æ–ø–∏—Å–æ–º —Ç–∞ JSON Schema –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏. –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä—É—î —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –≤–∏–∫–ª–∏–∫ –∑–∞–º—ñ—Å—Ç—å —Ç–µ–∫—Å—Ç—É. –í–∞—à –∫–æ–¥ –≤–∏–∫–æ–Ω—É—î —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏.",
                "links": [
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "api-providers.html"
                  }
                ]
              },
              {
                "text": "–í–∏–∫–ª–∏–∫ —Ñ—É–Ω–∫—Ü—ñ–π OpenAI",
                "desc": "–ú–∞—Å–∏–≤ tools –∑ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ —Ñ—É–Ω–∫—Ü—ñ–π. –ü—ñ–¥—Ç—Ä–∏–º—É—î parallel_tool_calls, –ø—Ä–∏–º—É—Å–æ–≤–∏–π tool_choice —Ç–∞ strict mode –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ–≥–æ –¥–æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ö–µ–º–∏.",
                "links": []
              },
              {
                "text": "Tool Use –≤—ñ–¥ Anthropic",
                "desc": "–ë–ª–æ–∫–∏ tool_use –≤ messages API Claude. –ü—ñ–¥—Ç—Ä–∏–º—É—î –≤–∫–ª–∞–¥–µ–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏, tool_choice (auto/any/tool) —Ç–∞ —Å—Ç—Ä—ñ–º—ñ–Ω–≥ –≤–∏–∫–ª–∏–∫—ñ–≤. –í—ñ–¥–æ–º–∏–π –≤–∏—Å–æ–∫–æ—é —Ç–æ—á–Ω—ñ—Å—Ç—é —É —Å–∫–ª–∞–¥–Ω–∏—Ö –º—É–ª—å—Ç–∏—ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—è—Ö.",
                "links": []
              },
              {
                "text": "–ü–∞—Ä–∞–ª–µ–ª—å–Ω—ñ –≤–∏–∫–ª–∏–∫–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤",
                "desc": "–ú–æ–¥–µ–ª—ñ –º–æ–∂—É—Ç—å –≤–∏–∫–ª–∏–∫–∞—Ç–∏ –∫—ñ–ª—å–∫–∞ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ –æ–¥–Ω–æ—á–∞—Å–Ω–æ –¥–ª—è –Ω–µ–∑–∞–ª–µ–∂–Ω–∏—Ö –∑–∞–¥–∞—á. –ü–æ—à—É–∫ —É –∫—ñ–ª—å–∫–æ—Ö –±–∞–∑–∞—Ö, –≤–∏–∫–ª–∏–∫ –∫—ñ–ª—å–∫–æ—Ö API. –î—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∏—Å–∫–æ—Ä—é—î –≤–æ—Ä–∫—Ñ–ª–æ—É –∞–≥–µ–Ω—Ç—ñ–≤.",
                "links": [
                  {
                    "title": "–ê–≥–µ–Ω—Ç–∏",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "–ë–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤",
                "desc": "–ú–æ–¥–µ–ª—å –≤–∏–∫–ª–∏–∫–∞—î —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –æ—Ç—Ä–∏–º—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏, –º—ñ—Ä–∫—É—î, –ø–æ—Ç—ñ–º –≤–∏–∫–ª–∏–∫–∞—î —ñ–Ω—à–∏–π. –¶–µ–π —Ü–∏–∫–ª –∑–∞–±–µ–∑–ø–µ—á—É—î —Å–∫–ª–∞–¥–Ω—ñ –≤–æ—Ä–∫—Ñ–ª–æ—É: –ø–æ—à—É–∫ ‚Üí —á–∏—Ç–∞–Ω–Ω—è ‚Üí –∞–Ω–∞–ª—ñ–∑ ‚Üí –∑–∞–ø–∏—Å.",
                "links": []
              },
              {
                "text": "–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–ª–∞—Å–Ω–∏—Ö —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤",
                "desc": "–í–∏–∑–Ω–∞—á—Ç–µ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –¥–ª—è –≤–∞—à–æ–≥–æ –¥–æ–º–µ–Ω—É: –ø–æ—à—É–∫ —ñ–Ω–≤–µ–Ω—Ç–∞—Ä—é, –∑–∞–ø–∏—Ç–∏ CRM, –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ API. –•–æ—Ä–æ—à—ñ –æ–ø–∏—Å–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ –∫—Ä–∏—Ç–∏—á–Ω—ñ ‚Äî –º–æ–¥–µ–ª—å –≤–∏—Ä—ñ—à—É—î –∫–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ–ø–∏—Å—ñ–≤.",
                "links": []
              },
              {
                "text": "MCP (Model Context Protocol)",
                "desc": "–í—ñ–¥–∫—Ä–∏—Ç–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç Anthropic –¥–ª—è –∑'—î–¥–Ω–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –®–Ü –∑ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —Ç–∞ –¥–∂–µ—Ä–µ–ª–∞–º–∏ –¥–∞–Ω–∏—Ö. –°–µ—Ä–≤–µ—Ä–∏ –Ω–∞–¥–∞—é—Ç—å —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –ø—Ä–æ—Ç–æ–∫–æ–ª –¥–ª—è plug-and-play —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó.",
                "links": [
                  {
                    "title": "–®–Ü-–ø—Ä–æ—Ç–æ–∫–æ–ª–∏",
                    "href": "ai-protocols.html"
                  }
                ]
              },
              {
                "text": "–ö–µ—Ä—É–≤–∞–Ω–Ω—è –∫–æ–º–ø'—é—Ç–µ—Ä–æ–º",
                "desc": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –µ–∫—Ä–∞–Ω—É —è–∫ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—É ‚Äî –º–æ–¥–µ–ª—å –±–∞—á–∏—Ç—å —Å–∫—Ä—ñ–Ω—à–æ—Ç–∏ —Ç–∞ –≥–µ–Ω–µ—Ä—É—î –¥—ñ—ó –º–∏—à—ñ/–∫–ª–∞–≤—ñ–∞—Ç—É—Ä–∏. Computer use –≤—ñ–¥ Anthropic –¥–æ–∑–≤–æ–ª—è—î –®–Ü –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ –±—É–¥—å-—è–∫–∏–º –ü–ó.",
                "links": []
              },
              {
                "text": "–û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫",
                "desc": "–Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –∑–±–æ—è—Ç—å ‚Äî API —Ç–∞–π–º–∞—É—Ç—è—Ç—å, –∑–∞–ø–∏—Ç–∏ –ø–æ–≤–µ—Ä—Ç–∞—é—Ç—å –ø–æ–º–∏–ª–∫–∏. –†–æ–±–∞—Å—Ç–Ω—ñ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –ø–æ–≤–µ—Ä—Ç–∞—é—Ç—å —á—ñ—Ç–∫—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø—Ä–æ –ø–æ–º–∏–ª–∫–∏, —â–æ–± –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –ø–æ–≤—Ç–æ—Ä–∏—Ç–∏ –∞–±–æ –ø–æ—è—Å–Ω–∏—Ç–∏ –∑–±—ñ–π.",
                "links": [
                  {
                    "title": "–ì–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó",
                    "href": "../level-2/hallucination.html"
                  }
                ]
              },
              {
                "text": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è –±–µ–∑–ø–µ–∫–∏",
                "desc": "–í–∏–∫–ª–∏–∫–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ –º–∞—é—Ç—å —Ä–µ–∞–ª—å–Ω—ñ –Ω–∞—Å–ª—ñ–¥–∫–∏. –í–∞–ª—ñ–¥—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏, —Å–∞–Ω—ñ—Ç–∏–∑—É–π—Ç–µ –≤—Ö–æ–¥–∏, –æ–±–º–µ–∂—É–π—Ç–µ —á–∞—Å—Ç–æ—Ç—É —Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ –ø—Ä–∏–≤—ñ–ª–µ—ó. –ù—ñ–∫–æ–ª–∏ –Ω–µ –¥–∞–≤–∞–π—Ç–µ –º–æ–¥–µ–ª—è–º –Ω–µ–æ–±–º–µ–∂–µ–Ω–∏–π –∑–∞–ø–∏—Å –≤ –ë–î.",
                "links": [
                  {
                    "title": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü",
                    "href": "../level-5/ai-safety.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Function Calling",
                "def": "API feature allowing LLMs to invoke external functions with structured parameters instead of generating text."
              },
              {
                "term": "JSON Schema",
                "def": "The standard format for defining tool parameters ‚Äî types, descriptions, required fields, enums."
              },
              {
                "term": "MCP",
                "def": "Model Context Protocol ‚Äî Anthropic's open standard for connecting AI to tools and data sources."
              },
              {
                "term": "Computer Use",
                "def": "AI capability to control a computer by viewing screenshots and generating mouse/keyboard actions."
              }
            ],
            "uk": [
              {
                "term": "–í–∏–∫–ª–∏–∫ —Ñ—É–Ω–∫—Ü—ñ–π",
                "def": "–§—É–Ω–∫—Ü—ñ—è API, —â–æ –¥–æ–∑–≤–æ–ª—è—î LLM –≤–∏–∫–ª–∏–∫–∞—Ç–∏ –∑–æ–≤–Ω—ñ—à–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –∑—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∑–∞–º—ñ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É."
              },
              {
                "term": "JSON Schema",
                "def": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ ‚Äî —Ç–∏–ø–∏, –æ–ø–∏—Å–∏, –æ–±–æ–≤'—è–∑–∫–æ–≤—ñ –ø–æ–ª—è, –ø–µ—Ä–µ—Ä–∞—Ö—É–≤–∞–Ω–Ω—è."
              },
              {
                "term": "MCP",
                "def": "Model Context Protocol ‚Äî –≤—ñ–¥–∫—Ä–∏—Ç–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç Anthropic –¥–ª—è –∑'—î–¥–Ω–∞–Ω–Ω—è –®–Ü –∑ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —Ç–∞ –¥–∂–µ—Ä–µ–ª–∞–º–∏ –¥–∞–Ω–∏—Ö."
              },
              {
                "term": "–ö–µ—Ä—É–≤–∞–Ω–Ω—è –∫–æ–º–ø'—é—Ç–µ—Ä–æ–º",
                "def": "–ú–æ–∂–ª–∏–≤—ñ—Å—Ç—å –®–Ü –∫–µ—Ä—É–≤–∞—Ç–∏ –∫–æ–º–ø'—é—Ç–µ—Ä–æ–º —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–≥–ª—è–¥ —Å–∫—Ä—ñ–Ω—à–æ—Ç—ñ–≤ —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é –¥—ñ–π –º–∏—à—ñ/–∫–ª–∞–≤—ñ–∞—Ç—É—Ä–∏."
              }
            ]
          },
          "tips": {
            "en": [
              "Write detailed tool descriptions ‚Äî the model decides when to use each tool based solely on the description text",
              "Return structured error messages from tools so the model can reason about failures and retry intelligently",
              "Start with 3-5 focused tools rather than 50 ‚Äî models choose better with fewer, well-defined options"
            ],
            "uk": [
              "–ü–∏—à—ñ—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ñ –æ–ø–∏—Å–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ ‚Äî –º–æ–¥–µ–ª—å –≤–∏—Ä—ñ—à—É—î –∫–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∫–æ–∂–µ–Ω —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ª–∏—à–µ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ–ø–∏—Å—É",
              "–ü–æ–≤–µ—Ä—Ç–∞–π—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø—Ä–æ –ø–æ–º–∏–ª–∫–∏ –∑ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤, —â–æ–± –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –º—ñ—Ä–∫—É–≤–∞—Ç–∏ —Ç–∞ –ø–æ–≤—Ç–æ—Ä–∏—Ç–∏ —Ä–æ–∑—É–º–Ω–æ",
              "–ü–æ—á–Ω—ñ—Ç—å –∑ 3-5 —Ñ–æ–∫—É—Å–æ–≤–∞–Ω–∏—Ö —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ –∑–∞–º—ñ—Å—Ç—å 50 ‚Äî –º–æ–¥–µ–ª—ñ –æ–±–∏—Ä–∞—é—Ç—å –∫—Ä–∞—â–µ –∑ –º–µ–Ω—à–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é —á—ñ—Ç–∫–æ –≤–∏–∑–Ω–∞—á–µ–Ω–∏—Ö –æ–ø—Ü—ñ–π"
            ]
          },
          "related": [
            "Agents & Tools"
          ]
        },
        {
          "slug": "rag",
          "title": {
            "en": "RAG (Retrieval-Augmented Generation)",
            "uk": "RAG (–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ –ø–æ—à—É–∫–æ–≤–∏–º –¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è–º)"
          },
          "desc": {
            "en": "Grounding AI responses in your own data using retrieval techniques.",
            "uk": "–ó–∞–∑–µ–º–ª–µ–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –®–Ü –Ω–∞ –≤–∞—à–∏—Ö –≤–ª–∞—Å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –ø–æ—à—É–∫–æ–≤–∏—Ö —Ç–µ—Ö–Ω—ñ–∫."
          },
          "overview": {
            "en": [
              "RAG is the most practical technique for making AI work with your own data. Instead of fine-tuning a model on your documents (expensive and inflexible), RAG retrieves relevant information at query time and includes it in the prompt. The model then generates answers grounded in your actual data rather than its training knowledge.",
              "A typical RAG pipeline: embed your documents into vectors, store in a vector database, and at query time retrieve the most relevant chunks to include in the context. This pattern powers knowledge bases, customer support bots, code assistants, and enterprise search. Getting the retrieval right is 80% of the challenge."
            ],
            "uk": [
              "RAG ‚Äî —Ü–µ –Ω–∞–π–ø—Ä–∞–∫—Ç–∏—á–Ω—ñ—à–∞ —Ç–µ—Ö–Ω—ñ–∫–∞ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –®–Ü –∑ –≤–∞—à–∏–º–∏ –≤–ª–∞—Å–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏. –ó–∞–º—ñ—Å—Ç—å —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥—É –º–æ–¥–µ–ª—ñ –Ω–∞ –≤–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö (–¥–æ—Ä–æ–≥–æ —Ç–∞ –Ω–µ–≥–Ω—É—á–∫–æ), RAG –≤–∏—Ç—è–≥—É—î —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—ñ–¥ —á–∞—Å –∑–∞–ø–∏—Ç—É —Ç–∞ –≤–∫–ª—é—á–∞—î –≤ –ø—Ä–æ–º–ø—Ç. –ú–æ–¥–µ–ª—å –ø–æ—Ç—ñ–º –≥–µ–Ω–µ—Ä—É—î –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ, –∑–∞–∑–µ–º–ª–µ–Ω—ñ –Ω–∞ –≤–∞—à–∏—Ö —Ä–µ–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö.",
              "–¢–∏–ø–æ–≤–∏–π RAG-–ø–∞–π–ø–ª–∞–π–Ω: –µ–º–±–µ–¥—ñ–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –≤–µ–∫—Ç–æ—Ä–∏, –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —É –≤–µ–∫—Ç–æ—Ä–Ω—ñ–π –±–∞–∑—ñ, —Ç–∞ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –Ω–∞–π—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—à–∏—Ö —á–∞–Ω–∫—ñ–≤ –ø—ñ–¥ —á–∞—Å –∑–∞–ø–∏—Ç—É –¥–ª—è –≤–∫–ª—é—á–µ–Ω–Ω—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç. –¶–µ–π –ø–∞—Ç–µ—Ä–Ω –∂–∏–≤–∏—Ç—å –±–∞–∑–∏ –∑–Ω–∞–Ω—å, –±–æ—Ç –ø—ñ–¥—Ç—Ä–∏–º–∫–∏, –∞—Å–∏—Å—Ç–µ–Ω—Ç–∏ –∫–æ–¥—É —Ç–∞ –µ–Ω—Ç–µ—Ä–ø—Ä–∞–π–∑-–ø–æ—à—É–∫. –ü—Ä–∞–≤–∏–ª—å–Ω–∏–π –ø–æ—à—É–∫ ‚Äî —Ü–µ 80% –≤–∏–∫–ª–∏–∫—É."
            ]
          },
          "details": {
            "en": [
              {
                "text": "RAG Architecture",
                "desc": "Three phases: Retrieve (find relevant documents), Augment (add them to the prompt), Generate (LLM produces grounded answer). Simple in concept, nuanced in execution.",
                "links": [
                  {
                    "title": "Context",
                    "href": "../level-2/context.html"
                  }
                ]
              },
              {
                "text": "Embeddings",
                "desc": "Dense vector representations of text that capture semantic meaning. Similar texts have similar vectors. Models: OpenAI text-embedding-3, Cohere embed-v3, open-source BGE and E5.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Vector Databases",
                "desc": "Specialized databases for storing and querying embeddings. Pinecone (managed), Qdrant (open-source), Weaviate, ChromaDB (lightweight). Each optimizes for different scale and feature needs.",
                "links": []
              },
              {
                "text": "Chunking Strategies",
                "desc": "How you split documents into chunks dramatically affects retrieval quality. Fixed-size, sentence-based, semantic, recursive, and document-structure-aware chunking each suit different content types.",
                "links": []
              },
              {
                "text": "Hybrid Search",
                "desc": "Combining semantic search (embeddings) with keyword search (BM25). Hybrid catches both conceptually similar and keyword-exact matches. Most production RAG systems use hybrid search.",
                "links": []
              },
              {
                "text": "Reranking",
                "desc": "After initial retrieval, a cross-encoder reranker scores each chunk against the query more accurately. Cohere Rerank, BGE reranker. Dramatically improves retrieval precision.",
                "links": []
              },
              {
                "text": "Advanced RAG Patterns",
                "desc": "CRAG (Corrective RAG): verify retrieval quality before generating. Self-RAG: model decides when retrieval is needed. Graph RAG: combine vector search with knowledge graphs for richer context.",
                "links": []
              },
              {
                "text": "Multi-Modal RAG",
                "desc": "RAG beyond text ‚Äî retrieving images, tables, and code snippets. Vision models can process retrieved images. Table extraction and code understanding require specialized chunking.",
                "links": [
                  {
                    "title": "Multimodality",
                    "href": "../level-1/multimodality.html"
                  }
                ]
              },
              {
                "text": "Evaluation",
                "desc": "Measuring RAG quality: retrieval metrics (precision, recall, MRR) and generation metrics (faithfulness, relevance, completeness). RAGAS framework automates RAG evaluation.",
                "links": []
              },
              {
                "text": "Common Pitfalls",
                "desc": "Too-small chunks lose context, too-large waste tokens. Poor embeddings retrieve irrelevant content. No reranking means noise in the top results. Always evaluate retrieval quality independently of generation.",
                "links": [
                  {
                    "title": "Hallucinations",
                    "href": "../level-2/hallucination.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ RAG",
                "desc": "–¢—Ä–∏ —Ñ–∞–∑–∏: Retrieve (–∑–Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏), Augment (–¥–æ–¥–∞—Ç–∏ –≤ –ø—Ä–æ–º–ø—Ç), Generate (LLM –≤–∏–¥–∞—î –∑–∞–∑–µ–º–ª–µ–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å). –ü—Ä–æ—Å—Ç–æ –≤ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó, –Ω—é–∞–Ω—Å–æ–≤–∞–Ω–æ —É –≤–∏–∫–æ–Ω–∞–Ω–Ω—ñ.",
                "links": [
                  {
                    "title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç",
                    "href": "../level-2/context.html"
                  }
                ]
              },
              {
                "text": "–ï–º–±–µ–¥—ñ–Ω–≥–∏",
                "desc": "–©—ñ–ª—å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–Ω—ñ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É, —â–æ –∑–∞—Ö–æ–ø–ª—é—é—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è. –ü–æ–¥—ñ–±–Ω—ñ —Ç–µ–∫—Å—Ç–∏ –º–∞—é—Ç—å –ø–æ–¥—ñ–±–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏. –ú–æ–¥–µ–ª—ñ: OpenAI text-embedding-3, Cohere embed-v3, open-source BGE —Ç–∞ E5.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "–í–µ–∫—Ç–æ—Ä–Ω—ñ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö",
                "desc": "–°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –±–∞–∑–∏ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ç–∞ –∑–∞–ø–∏—Ç—ñ–≤ –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤. Pinecone (–∫–µ—Ä–æ–≤–∞–Ω–∏–π), Qdrant (open-source), Weaviate, ChromaDB (–ª–µ–≥–∫–∏–π). –ö–æ–∂–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –ø—ñ–¥ —Ä—ñ–∑–Ω–∏–π –º–∞—Å—à—Ç–∞–±.",
                "links": []
              },
              {
                "text": "–°—Ç—Ä–∞—Ç–µ–≥—ñ—ó —á–∞–Ω–∫—ñ–Ω–≥—É",
                "desc": "–°–ø–æ—Å—ñ–± —Ä–æ–∑–±–∏—Ç—Ç—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –Ω–∞ —á–∞–Ω–∫–∏ –¥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –≤–ø–ª–∏–≤–∞—î –Ω–∞ —è–∫—ñ—Å—Ç—å –ø–æ—à—É–∫—É. –§—ñ–∫—Å–æ–≤–∞–Ω–∏–π, –ø–æ —Ä–µ—á–µ–Ω–Ω—è—Ö, —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π, —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–∏–π —Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ-–æ–±—ñ–∑–Ω–∞–Ω–∏–π —á–∞–Ω–∫—ñ–Ω–≥ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç—É.",
                "links": []
              },
              {
                "text": "–ì—ñ–±—Ä–∏–¥–Ω–∏–π –ø–æ—à—É–∫",
                "desc": "–ü–æ—î–¥–Ω–∞–Ω–Ω—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ –ø–æ—à—É–∫—É (–µ–º–±–µ–¥—ñ–Ω–≥–∏) –∑ –∫–ª—é—á–æ–≤–∏–º (BM25). –ì—ñ–±—Ä–∏–¥ –ª–æ–≤–∏—Ç—å —ñ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ —Å—Ö–æ–∂—ñ, —ñ —Ç–æ—á–Ω—ñ –∫–ª—é—á–æ–≤—ñ –∑–±—ñ–≥–∏. –ë—ñ–ª—å—à—ñ—Å—Ç—å –ø—Ä–æ–¥–∞–∫—à–Ω RAG –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –≥—ñ–±—Ä–∏–¥–Ω–∏–π –ø–æ—à—É–∫.",
                "links": []
              },
              {
                "text": "–ü–µ—Ä–µ—Ä–µ–π—Ç–∏–Ω–≥",
                "desc": "–ü—ñ—Å–ª—è –ø–µ—Ä–≤–∏–Ω–Ω–æ–≥–æ –ø–æ—à—É–∫—É –∫—Ä–æ—Å-–µ–Ω–∫–æ–¥–µ—Ä –ø–µ—Ä–µ—Ä–µ–π—Ç–µ—Ä –æ—Ü—ñ–Ω—é—î –∫–æ–∂–µ–Ω —á–∞–Ω–∫ —Ç–æ—á–Ω—ñ—à–µ. Cohere Rerank, BGE reranker. –î—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø–æ–∫—Ä–∞—â—É—î —Ç–æ—á–Ω—ñ—Å—Ç—å –ø–æ—à—É–∫—É.",
                "links": []
              },
              {
                "text": "–ü—Ä–æ—Å—É–Ω—É—Ç—ñ –ø–∞—Ç–µ—Ä–Ω–∏ RAG",
                "desc": "CRAG (Corrective RAG): –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫–æ—Å—Ç—ñ –ø–æ—à—É–∫—É –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—î—é. Self-RAG: –º–æ–¥–µ–ª—å –≤–∏—Ä—ñ—à—É—î –∫–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–µ–Ω –ø–æ—à—É–∫. Graph RAG: –≤–µ–∫—Ç–æ—Ä–Ω–∏–π –ø–æ—à—É–∫ + –≥—Ä–∞—Ñ –∑–Ω–∞–Ω—å –¥–ª—è –±–∞–≥–∞—Ç—à–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.",
                "links": []
              },
              {
                "text": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∏–π RAG",
                "desc": "RAG –∑–∞ –º–µ–∂–∞–º–∏ —Ç–µ–∫—Å—Ç—É ‚Äî –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å, —Ç–∞–±–ª–∏—Ü—å —Ç–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤ –∫–æ–¥—É. –ú–æ–¥–µ–ª—ñ –≤—ñ–∑—ñ—ó –æ–±—Ä–æ–±–ª—è—é—Ç—å –æ—Ç—Ä–∏–º–∞–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è. –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—å —Ç–∞ –∫–æ–¥—É –≤–∏–º–∞–≥–∞—î —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ–≥–æ —á–∞–Ω–∫—ñ–Ω–≥—É.",
                "links": [
                  {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å",
                    "href": "../level-1/multimodality.html"
                  }
                ]
              },
              {
                "text": "–û—Ü—ñ–Ω–∫–∞",
                "desc": "–í–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è —è–∫–æ—Å—Ç—ñ RAG: –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—à—É–∫—É (—Ç–æ—á–Ω—ñ—Å—Ç—å, –ø–æ–≤–Ω–æ—Ç–∞, MRR) —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó (–≤—ñ—Ä–Ω—ñ—Å—Ç—å, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å, –ø–æ–≤–Ω–æ—Ç–∞). –§—Ä–µ–π–º–≤–æ—Ä–∫ RAGAS –∞–≤—Ç–æ–º–∞—Ç–∏–∑—É—î –æ—Ü—ñ–Ω–∫—É RAG.",
                "links": []
              },
              {
                "text": "–¢–∏–ø–æ–≤—ñ –ø–∞—Å—Ç–∫–∏",
                "desc": "–ó–∞–Ω–∞–¥—Ç–æ –º–∞–ª—ñ —á–∞–Ω–∫–∏ –≤—Ç—Ä–∞—á–∞—é—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫—ñ –≤–∏—Ç—Ä–∞—á–∞—é—Ç—å —Ç–æ–∫–µ–Ω–∏. –ü–æ–≥–∞–Ω—ñ –µ–º–±–µ–¥—ñ–Ω–≥–∏ –¥–∞—é—Ç—å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–π –ø–æ—à—É–∫. –ë–µ–∑ –ø–µ—Ä–µ—Ä–µ–π—Ç–∏–Ω–≥—É ‚Äî —à—É–º —É —Ç–æ–ø-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö. –ó–∞–≤–∂–¥–∏ –æ—Ü—ñ–Ω—é–π—Ç–µ –ø–æ—à—É–∫ –æ–∫—Ä–µ–º–æ –≤—ñ–¥ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó.",
                "links": [
                  {
                    "title": "–ì–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó",
                    "href": "../level-2/hallucination.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Embedding",
                "def": "Dense vector representation of text that captures semantic meaning for similarity search."
              },
              {
                "term": "Vector Database",
                "def": "Database optimized for storing embeddings and performing fast similarity search (Pinecone, Qdrant, Weaviate)."
              },
              {
                "term": "Chunking",
                "def": "The process of splitting documents into smaller pieces for embedding and retrieval."
              },
              {
                "term": "Reranking",
                "def": "Second-stage scoring of retrieved results using a cross-encoder model for improved precision."
              }
            ],
            "uk": [
              {
                "term": "–ï–º–±–µ–¥—ñ–Ω–≥",
                "def": "–©—ñ–ª—å–Ω–µ –≤–µ–∫—Ç–æ—Ä–Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É, —â–æ –∑–∞—Ö–æ–ø–ª—é—î —Å–µ–º–∞–Ω—Ç–∏—á–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –ø–æ—à—É–∫—É –ø–æ–¥—ñ–±–Ω–æ—Å—Ç—ñ."
              },
              {
                "term": "–í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–∏—Ö",
                "def": "–ë–∞–∑–∞ –¥–∞–Ω–∏—Ö –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ —Ç–∞ —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É –ø–æ–¥—ñ–±–Ω–æ—Å—Ç—ñ (Pinecone, Qdrant, Weaviate)."
              },
              {
                "term": "–ß–∞–Ω–∫—ñ–Ω–≥",
                "def": "–ü—Ä–æ—Ü–µ—Å —Ä–æ–∑–±–∏—Ç—Ç—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –Ω–∞ –º–µ–Ω—à—ñ —á–∞—Å—Ç–∏–Ω–∏ –¥–ª—è –µ–º–±–µ–¥—ñ–Ω–≥—É —Ç–∞ –ø–æ—à—É–∫—É."
              },
              {
                "term": "–ü–µ—Ä–µ—Ä–µ–π—Ç–∏–Ω–≥",
                "def": "–î—Ä—É–≥–∞ —Å—Ç–∞–¥—ñ—è –æ—Ü—ñ–Ω–∫–∏ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∫—Ä–æ—Å-–µ–Ω–∫–æ–¥–µ—Ä–æ–º –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—ñ."
              }
            ]
          },
          "tips": {
            "en": [
              "Start with a simple chunking strategy (500 tokens with 50 overlap), then optimize only after measuring retrieval quality",
              "Always add a reranking step ‚Äî it is the single highest-impact improvement you can make to a RAG system",
              "Evaluate retrieval and generation separately: poor retrieval cannot be fixed by a better LLM"
            ],
            "uk": [
              "–ü–æ—á–Ω—ñ—Ç—å –∑ –ø—Ä–æ—Å—Ç–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó —á–∞–Ω–∫—ñ–Ω–≥—É (500 —Ç–æ–∫–µ–Ω—ñ–≤ –∑ 50 –ø–µ—Ä–µ—Ç–∏–Ω–æ–º), –æ–ø—Ç–∏–º—ñ–∑—É–π—Ç–µ –ª–∏—à–µ –ø—ñ—Å–ª—è –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è —è–∫–æ—Å—Ç—ñ –ø–æ—à—É–∫—É",
              "–ó–∞–≤–∂–¥–∏ –¥–æ–¥–∞–≤–∞–π—Ç–µ –∫—Ä–æ–∫ –ø–µ—Ä–µ—Ä–µ–π—Ç–∏–Ω–≥—É ‚Äî —Ü–µ –Ω–∞–π–≤–ø–ª–∏–≤–æ–≤—ñ—à–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –¥–ª—è RAG-—Å–∏—Å—Ç–µ–º–∏",
              "–û—Ü—ñ–Ω—é–π—Ç–µ –ø–æ—à—É–∫ —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é –æ–∫—Ä–µ–º–æ: –ø–æ–≥–∞–Ω–∏–π –ø–æ—à—É–∫ –Ω–µ –º–æ–∂–Ω–∞ –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ –∫—Ä–∞—â–æ—é LLM"
            ]
          },
          "related": [
            "Video Content",
            "Feed"
          ]
        },
        {
          "slug": "frameworks",
          "title": {
            "en": "Applied Frameworks",
            "uk": "–ü—Ä–∏–∫–ª–∞–¥–Ω—ñ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏"
          },
          "desc": {
            "en": "Practical frameworks for building production AI applications.",
            "uk": "–ü—Ä–∞–∫—Ç–∏—á–Ω—ñ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä–æ–¥–∞–∫—à–Ω –®–Ü-–¥–æ–¥–∞—Ç–∫—ñ–≤."
          },
          "overview": {
            "en": [
              "Applied AI frameworks bridge the gap between raw model APIs and production-ready applications. Instead of building everything from scratch ‚Äî routing logic, UI, memory management, deployment ‚Äî these frameworks provide opinionated structures that handle the common patterns so you can focus on your specific use case.",
              "The landscape spans from visual builders (Dify, Flowise) where non-developers can create AI workflows by dragging nodes, to developer frameworks (Vercel AI SDK, FastAPI patterns) that give programmatic control with production-grade tooling. Choosing the right framework depends on your team's skills, deployment target, and how much customization you need."
            ],
            "uk": [
              "–ü—Ä–∏–∫–ª–∞–¥–Ω—ñ –®–Ü-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –∑–∞–ø–æ–≤–Ω—é—é—Ç—å —Ä–æ–∑—Ä–∏–≤ –º—ñ–∂ —Å–∏—Ä–∏–º–∏ API –º–æ–¥–µ–ª–µ–π —Ç–∞ –ø—Ä–æ–¥–∞–∫—à–Ω-–≥–æ—Ç–æ–≤–∏–º–∏ –¥–æ–¥–∞—Ç–∫–∞–º–∏. –ó–∞–º—ñ—Å—Ç—å –±—É–¥—É–≤–∞–Ω–Ω—è –≤—Å—å–æ–≥–æ –∑ –Ω—É–ª—è ‚Äî –ª–æ–≥—ñ–∫–∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—ó, UI, —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –ø–∞–º'—è—Ç—Ç—é, –¥–µ–ø–ª–æ—é ‚Äî —Ü—ñ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –Ω–∞–¥–∞—é—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏, —â–æ –æ–±—Ä–æ–±–ª—è—é—Ç—å —Ç–∏–ø–æ–≤—ñ –ø–∞—Ç–µ—Ä–Ω–∏, —â–æ–± –≤–∏ –º–æ–≥–ª–∏ –∑–æ—Å–µ—Ä–µ–¥–∏—Ç–∏—Å—è –Ω–∞ –≤–∞—à–æ–º—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –≤–∏–ø–∞–¥–∫—É.",
              "–õ–∞–Ω–¥—à–∞—Ñ—Ç –ø—Ä–æ—Å—Ç—è–≥–∞—î—Ç—å—Å—è –≤—ñ–¥ –≤—ñ–∑—É–∞–ª—å–Ω–∏—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ñ–≤ (Dify, Flowise), –¥–µ –Ω–µ-—Ä–æ–∑—Ä–æ–±–Ω–∏–∫–∏ –º–æ–∂—É—Ç—å —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –®–Ü-–≤–æ—Ä–∫—Ñ–ª–æ—É –ø–µ—Ä–µ—Ç—è–≥—É–≤–∞–Ω–Ω—è–º –≤—É–∑–ª—ñ–≤, –¥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫—ñ–≤ –¥–ª—è —Ä–æ–∑—Ä–æ–±–Ω–∏–∫—ñ–≤ (Vercel AI SDK, –ø–∞—Ç–µ—Ä–Ω–∏ FastAPI), —â–æ –¥–∞—é—Ç—å –ø—Ä–æ–≥—Ä–∞–º–Ω–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∑ –ø—Ä–æ–¥–∞–∫—à–Ω-—ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ä—ñ—î–º. –í–∏–±—ñ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫—É –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –Ω–∞–≤–∏—á–æ–∫ –∫–æ–º–∞–Ω–¥–∏, —Ü—ñ–ª—å–æ–≤–æ—ó –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏ —Ç–∞ –ø–æ—Ç—Ä—ñ–±–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è –∫–∞—Å—Ç–æ–º—ñ–∑–∞—Ü—ñ—ó."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Dify",
                "desc": "Open-source visual AI workflow builder. Drag-and-drop interface for building chatbots, RAG apps, and agent workflows. Supports multiple LLM providers, built-in knowledge base, and API deployment.",
                "links": [
                  {
                    "title": "RAG",
                    "href": "rag.html"
                  }
                ]
              },
              {
                "text": "n8n",
                "desc": "Workflow automation platform with powerful AI nodes. Connect LLMs to 400+ integrations (Slack, email, databases, APIs). Self-hostable, event-driven, and ideal for business process automation with AI.",
                "links": [
                  {
                    "title": "Agents",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "Flowise",
                "desc": "Visual LangChain builder ‚Äî create LLM chains and agents by connecting nodes in a browser UI. Lower barrier to entry than coding LangChain directly. Good for prototyping RAG and agent workflows.",
                "links": [
                  {
                    "title": "Tools & Libraries",
                    "href": "base-tools.html"
                  }
                ]
              },
              {
                "text": "Vercel AI SDK",
                "desc": "TypeScript-first framework for building AI-powered web applications. Handles streaming, tool calling, multi-step interactions, and generative UI in React/Next.js. The standard for AI web apps.",
                "links": []
              },
              {
                "text": "FastAPI + LLM Patterns",
                "desc": "Python backend patterns: async streaming endpoints for LLM responses, WebSocket connections for real-time chat, dependency injection for provider switching, and structured output validation with Pydantic.",
                "links": [
                  {
                    "title": "API Providers",
                    "href": "api-providers.html"
                  }
                ]
              },
              {
                "text": "Streamlit & Gradio",
                "desc": "Rapid prototyping frameworks for AI demos. Streamlit creates data apps with Python scripts. Gradio builds ML model interfaces with automatic API generation. Both deploy to the cloud in minutes.",
                "links": []
              },
              {
                "text": "LangServe & LangGraph",
                "desc": "LangChain's deployment and stateful workflow tools. LangServe deploys chains as REST APIs. LangGraph builds complex multi-step agent workflows with state management, cycles, and human-in-the-loop.",
                "links": [
                  {
                    "title": "Agents",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "CrewAI & AutoGen",
                "desc": "Multi-agent orchestration frameworks. CrewAI assigns roles and tasks to collaborating agents. AutoGen (Microsoft) enables conversational agent teams. Both simplify building agent systems.",
                "links": [
                  {
                    "title": "Agents",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "Low-Code AI Platforms",
                "desc": "Platforms like Zapier AI, Make.com, and Bubble with AI plugins. Enable non-developers to build AI-powered workflows. Trade flexibility for speed and accessibility.",
                "links": []
              },
              {
                "text": "Choosing a Framework",
                "desc": "Non-technical team: Dify/Flowise. Automation: n8n. Web app: Vercel AI SDK. Python API: FastAPI. Prototype: Streamlit/Gradio. Complex agents: LangGraph. Always prototype before committing to a framework.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "Dify",
                "desc": "Open-source –≤—ñ–∑—É–∞–ª—å–Ω–∏–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –®–Ü-–≤–æ—Ä–∫—Ñ–ª–æ—É. Drag-and-drop —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —á–∞—Ç–±–æ—Ç—ñ–≤, RAG-–¥–æ–¥–∞—Ç–∫—ñ–≤ —Ç–∞ –∞–≥–µ–Ω—Ç–Ω–∏—Ö –≤–æ—Ä–∫—Ñ–ª–æ—É. –ü—ñ–¥—Ç—Ä–∏–º—É—î –∫—ñ–ª—å–∫–æ—Ö LLM-–ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤, –≤–±—É–¥–æ–≤–∞–Ω—É –±–∞–∑—É –∑–Ω–∞–Ω—å —Ç–∞ API-–¥–µ–ø–ª–æ–π.",
                "links": [
                  {
                    "title": "RAG",
                    "href": "rag.html"
                  }
                ]
              },
              {
                "text": "n8n",
                "desc": "–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –≤–æ—Ä–∫—Ñ–ª–æ—É –∑ –ø–æ—Ç—É–∂–Ω–∏–º–∏ –®–Ü-–≤—É–∑–ª–∞–º–∏. –ó'—î–¥–Ω—É—î LLM –∑ 400+ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è–º–∏ (Slack, –ø–æ—à—Ç–∞, –±–∞–∑–∏ –¥–∞–Ω–∏—Ö, API). –°–∞–º–æ—Ö–æ—Å—Ç–∏–Ω–≥, –ø–æ–¥—ñ—î–≤–∞ –º–æ–¥–µ–ª—å, —ñ–¥–µ–∞–ª—å–Ω–∏–π –¥–ª—è –±—ñ–∑–Ω–µ—Å-–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –∑ –®–Ü.",
                "links": [
                  {
                    "title": "–ê–≥–µ–Ω—Ç–∏",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "Flowise",
                "desc": "–í—ñ–∑—É–∞–ª—å–Ω–∏–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä LangChain ‚Äî —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è LLM-–ª–∞–Ω—Ü—é–≥—ñ–≤ —Ç–∞ –∞–≥–µ–Ω—Ç—ñ–≤ –∑'—î–¥–Ω–∞–Ω–Ω—è–º –≤—É–∑–ª—ñ–≤ —É –±—Ä–∞—É–∑–µ—Ä—ñ. –ù–∏–∂—á–∏–π –±–∞—Ä'—î—Ä –≤—Ö–æ–¥—É –Ω—ñ–∂ –∫–æ–¥—É–≤–∞–Ω–Ω—è LangChain –Ω–∞–ø—Ä—è–º—É. –î–æ–±—Ä–∏–π –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø—É–≤–∞–Ω–Ω—è RAG —Ç–∞ –∞–≥–µ–Ω—Ç—ñ–≤.",
                "links": [
                  {
                    "title": "–Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —Ç–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏",
                    "href": "base-tools.html"
                  }
                ]
              },
              {
                "text": "Vercel AI SDK",
                "desc": "TypeScript-first —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –®–Ü –≤–µ–±-–¥–æ–¥–∞—Ç–∫—ñ–≤. –û–±—Ä–æ–±–ª—è—î —Å—Ç—Ä—ñ–º—ñ–Ω–≥, –≤–∏–∫–ª–∏–∫ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤, –±–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤—ñ –≤–∑–∞—î–º–æ–¥—ñ—ó —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π UI –≤ React/Next.js. –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –®–Ü –≤–µ–±-–¥–æ–¥–∞—Ç–∫—ñ–≤.",
                "links": []
              },
              {
                "text": "–ü–∞—Ç—Ç–µ—Ä–Ω–∏ FastAPI + LLM",
                "desc": "–ü–∞—Ç–µ—Ä–Ω–∏ Python-–±–µ–∫–µ–Ω–¥—É: –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ñ —Å—Ç—Ä—ñ–º—ñ–Ω–≥-–µ–Ω–¥–ø–æ—ñ–Ω—Ç–∏ –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π LLM, WebSocket –¥–ª—è —Ä–µ–∞–ª-—Ç–∞–π–º —á–∞—Ç—É, dependency injection –¥–ª—è –∑–º—ñ–Ω–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞, –≤–∞–ª—ñ–¥–∞—Ü—ñ—è —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–æ–≥–æ –≤–∏—Ö–æ–¥—É —á–µ—Ä–µ–∑ Pydantic.",
                "links": [
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "api-providers.html"
                  }
                ]
              },
              {
                "text": "Streamlit —Ç–∞ Gradio",
                "desc": "–§—Ä–µ–π–º–≤–æ—Ä–∫–∏ —à–≤–∏–¥–∫–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø—É–≤–∞–Ω–Ω—è –¥–ª—è –®–Ü-–¥–µ–º–æ. Streamlit —Å—Ç–≤–æ—Ä—é—î data-–¥–æ–¥–∞—Ç–∫–∏ –∑ Python-—Å–∫—Ä–∏–ø—Ç—ñ–≤. Gradio –±—É–¥—É—î —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏ ML-–º–æ–¥–µ–ª–µ–π –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—é –≥–µ–Ω–µ—Ä–∞—Ü—ñ—î—é API. –û–±–∏–¥–≤–∞ –¥–µ–ø–ª–æ—è—Ç—å—Å—è –∑–∞ —Ö–≤–∏–ª–∏–Ω–∏.",
                "links": []
              },
              {
                "text": "LangServe —Ç–∞ LangGraph",
                "desc": "–Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –¥–µ–ø–ª–æ—é —Ç–∞ stateful –≤–æ—Ä–∫—Ñ–ª–æ—É –≤—ñ–¥ LangChain. LangServe –¥–µ–ø–ª–æ—ó—Ç—å –ª–∞–Ω—Ü—é–≥–∏ —è–∫ REST API. LangGraph –±—É–¥—É—î —Å–∫–ª–∞–¥–Ω—ñ –±–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤—ñ –∞–≥–µ–Ω—Ç–Ω—ñ –≤–æ—Ä–∫—Ñ–ª–æ—É –∑—ñ —Å—Ç–µ–π—Ç-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–æ–º —Ç–∞ —Ü–∏–∫–ª–∞–º–∏.",
                "links": [
                  {
                    "title": "–ê–≥–µ–Ω—Ç–∏",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "CrewAI —Ç–∞ AutoGen",
                "desc": "–§—Ä–µ–π–º–≤–æ—Ä–∫–∏ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü—ñ—ó –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∏—Ö —Å–∏—Å—Ç–µ–º. CrewAI –ø—Ä–∏–∑–Ω–∞—á–∞—î —Ä–æ–ª—ñ —Ç–∞ –∑–∞–¥–∞—á—ñ –∞–≥–µ–Ω—Ç–∞–º-—Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫–∞–º. AutoGen (Microsoft) –∑–∞–±–µ–∑–ø–µ—á—É—î —Ä–æ–∑–º–æ–≤–Ω—ñ –∫–æ–º–∞–Ω–¥–∏ –∞–≥–µ–Ω—Ç—ñ–≤. –û–±–∏–¥–≤–∞ —Å–ø—Ä–æ—â—É—é—Ç—å –ø–æ–±—É–¥–æ–≤—É –∞–≥–µ–Ω—Ç–Ω–∏—Ö —Å–∏—Å—Ç–µ–º.",
                "links": [
                  {
                    "title": "–ê–≥–µ–Ω—Ç–∏",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "Low-Code –®–Ü-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∏",
                "desc": "–ü–ª–∞—Ç—Ñ–æ—Ä–º–∏ —è–∫ Zapier AI, Make.com —Ç–∞ Bubble –∑ –®–Ü-–ø–ª–∞–≥—ñ–Ω–∞–º–∏. –î–æ–∑–≤–æ–ª—è—é—Ç—å –Ω–µ-—Ä–æ–∑—Ä–æ–±–Ω–∏–∫–∞–º –±—É–¥—É–≤–∞—Ç–∏ –®–Ü-–≤–æ—Ä–∫—Ñ–ª–æ—É. –û–±–º—ñ–Ω –≥–Ω—É—á–∫–æ—Å—Ç—ñ –Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å —Ç–∞ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å.",
                "links": []
              },
              {
                "text": "–í–∏–±—ñ—Ä —Ñ—Ä–µ–π–º–≤–æ—Ä–∫—É",
                "desc": "–ù–µ—Ç–µ—Ö–Ω—ñ—á–Ω–∞ –∫–æ–º–∞–Ω–¥–∞: Dify/Flowise. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—è: n8n. –í–µ–±-–¥–æ–¥–∞—Ç–æ–∫: Vercel AI SDK. Python API: FastAPI. –ü—Ä–æ—Ç–æ—Ç–∏–ø: Streamlit/Gradio. –°–∫–ª–∞–¥–Ω—ñ –∞–≥–µ–Ω—Ç–∏: LangGraph. –ó–∞–≤–∂–¥–∏ –ø—Ä–æ—Ç–æ—Ç–∏–ø—É–π—Ç–µ –ø–µ—Ä–µ–¥ –≤–∏–±–æ—Ä–æ–º.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Visual Workflow Builder",
                "def": "Tool that lets you create AI applications by connecting nodes in a drag-and-drop interface rather than writing code."
              },
              {
                "term": "Streaming",
                "def": "Delivering LLM responses token by token in real-time rather than waiting for the complete response."
              },
              {
                "term": "LangGraph",
                "def": "LangChain's framework for building stateful, multi-step agent workflows with cycles and branching logic."
              },
              {
                "term": "Low-Code AI",
                "def": "Platforms enabling AI application development with minimal programming through visual interfaces and pre-built components."
              }
            ],
            "uk": [
              {
                "term": "–í—ñ–∑—É–∞–ª—å–Ω–∏–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä",
                "def": "–Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –®–Ü-–¥–æ–¥–∞—Ç–∫—ñ–≤ –∑'—î–¥–Ω–∞–Ω–Ω—è–º –≤—É–∑–ª—ñ–≤ —É drag-and-drop —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ñ –∑–∞–º—ñ—Å—Ç—å –Ω–∞–ø–∏—Å–∞–Ω–Ω—è –∫–æ–¥—É."
              },
              {
                "term": "–°—Ç—Ä—ñ–º—ñ–Ω–≥",
                "def": "–î–æ—Å—Ç–∞–≤–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π LLM —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º —É —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ –∑–∞–º—ñ—Å—Ç—å –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –ø–æ–≤–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ."
              },
              {
                "term": "LangGraph",
                "def": "–§—Ä–µ–π–º–≤–æ—Ä–∫ LangChain –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ stateful, –±–∞–≥–∞—Ç–æ–∫—Ä–æ–∫–æ–≤–∏—Ö –∞–≥–µ–Ω—Ç–Ω–∏—Ö –≤–æ—Ä–∫—Ñ–ª–æ—É –∑ —Ü–∏–∫–ª–∞–º–∏ —Ç–∞ —Ä–æ–∑–≥–∞–ª—É–∂–µ–Ω–Ω—è–º."
              },
              {
                "term": "Low-Code –®–Ü",
                "def": "–ü–ª–∞—Ç—Ñ–æ—Ä–º–∏ –¥–ª—è —Ä–æ–∑—Ä–æ–±–∫–∏ –®–Ü-–¥–æ–¥–∞—Ç–∫—ñ–≤ –∑ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–º –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è–º —á–µ—Ä–µ–∑ –≤—ñ–∑—É–∞–ª—å–Ω—ñ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏ —Ç–∞ –≥–æ—Ç–æ–≤—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏."
              }
            ]
          },
          "tips": {
            "en": [
              "Start with Dify or Flowise for quick prototypes, then migrate to code-based frameworks when you need more control",
              "n8n is the Swiss army knife for AI automation ‚Äî if your workflow involves connecting multiple services, start here",
              "Don't over-framework: for simple LLM API calls, direct SDK usage is simpler and more maintainable than adding a framework"
            ],
            "uk": [
              "–ü–æ—á–Ω—ñ—Ç—å –∑ Dify –∞–±–æ Flowise –¥–ª—è —à–≤–∏–¥–∫–∏—Ö –ø—Ä–æ—Ç–æ—Ç–∏–ø—ñ–≤, –ø–æ—Ç—ñ–º –º—ñ–≥—Ä—É–π—Ç–µ –Ω–∞ –∫–æ–¥–æ–≤—ñ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –∫–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–µ–Ω –±—ñ–ª—å—à–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å",
              "n8n ‚Äî —Ü–µ —à–≤–µ–π—Ü–∞—Ä—Å—å–∫–∏–π –Ω—ñ–∂ –¥–ª—è –®–Ü-–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó: —è–∫—â–æ –≤–æ—Ä–∫—Ñ–ª–æ—É –≤–∫–ª—é—á–∞—î –∑'—î–¥–Ω–∞–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Ö —Å–µ—Ä–≤—ñ—Å—ñ–≤, –ø–æ—á–Ω—ñ—Ç—å —Ç—É—Ç",
              "–ù–µ –ø–µ—Ä–µ—Ñ—Ä–µ–π–º–≤–æ—Ä–∫—É–π—Ç–µ: –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö –≤–∏–∫–ª–∏–∫—ñ–≤ LLM API –ø—Ä—è–º–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è SDK –ø—Ä–æ—Å—Ç—ñ—à–µ —Ç–∞ –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω—ñ—à–µ –Ω—ñ–∂ –¥–æ–¥–∞–≤–∞–Ω–Ω—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫—É"
            ]
          },
          "related": [
            "Video Content",
            "Agents & Tools"
          ]
        },
        {
          "slug": "model-formats",
          "title": {
            "en": "Model Formats",
            "uk": "–§–æ—Ä–º–∞—Ç–∏ –º–æ–¥–µ–ª–µ–π"
          },
          "desc": {
            "en": "Understanding different model distribution and execution formats.",
            "uk": "–†–æ–∑—É–º—ñ–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤ —Ä–æ–∑–ø–æ–≤—Å—é–¥–∂–µ–Ω–Ω—è —Ç–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π."
          },
          "overview": {
            "en": [
              "AI models need to be serialized into files for distribution and loading. Different formats optimize for different goals: GGUF prioritizes running on consumer hardware with CPU+GPU splitting, GPTQ and AWQ are GPU-optimized for maximum throughput, SafeTensors ensures safe loading without code execution risks, and ONNX provides cross-platform compatibility.",
              "Understanding model formats is essential for local AI deployment. The format you choose determines which inference engine you can use (llama.cpp, vLLM, TensorRT), what hardware it runs on, how much memory it needs, and how fast it generates tokens. Most models on Hugging Face are available in multiple formats."
            ],
            "uk": [
              "–ú–æ–¥–µ–ª—ñ –®–Ü –ø–æ—Ç—Ä—ñ–±–Ω–æ —Å–µ—Ä—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —É —Ñ–∞–π–ª–∏ –¥–ª—è —Ä–æ–∑–ø–æ–≤—Å—é–¥–∂–µ–Ω–Ω—è —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è. –†—ñ–∑–Ω—ñ —Ñ–æ—Ä–º–∞—Ç–∏ –æ–ø—Ç–∏–º—ñ–∑—É—é—Ç—å —Ä—ñ–∑–Ω—ñ —Ü—ñ–ª—ñ: GGUF –ø—Ä—ñ–æ—Ä–∏—Ç–∏–∑—É—î –∑–∞–ø—É—Å–∫ –Ω–∞ —Å–ø–æ–∂–∏–≤—á–æ–º—É –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—ñ –∑ —Ä–æ–∑–ø–æ–¥—ñ–ª–æ–º CPU+GPU, GPTQ —Ç–∞ AWQ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ –¥–ª—è GPU –∑ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—é –ø—Ä–æ–ø—É—Å–∫–Ω–æ—é –∑–¥–∞—Ç–Ω—ñ—Å—Ç—é, SafeTensors –∑–∞–±–µ–∑–ø–µ—á—É—î –±–µ–∑–ø–µ—á–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –±–µ–∑ —Ä–∏–∑–∏–∫—ñ–≤ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∫–æ–¥—É, ONNX –Ω–∞–¥–∞—î –∫—Ä–æ—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–Ω—É —Å—É–º—ñ—Å–Ω—ñ—Å—Ç—å.",
              "–†–æ–∑—É–º—ñ–Ω–Ω—è —Ñ–æ—Ä–º–∞—Ç—ñ–≤ –º–æ–¥–µ–ª–µ–π –Ω–µ–æ–±—Ö—ñ–¥–Ω–µ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–µ–ø–ª–æ—é –®–Ü. –§–æ—Ä–º–∞—Ç –≤–∏–∑–Ω–∞—á–∞—î —è–∫–∏–π –¥–≤–∏–∂–æ–∫ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –≤–∏ –º–æ–∂–µ—Ç–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ (llama.cpp, vLLM, TensorRT), –Ω–∞ —è–∫–æ–º—É –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—ñ –≤—ñ–Ω –ø—Ä–∞—Ü—é—î, —Å–∫—ñ–ª—å–∫–∏ –ø–∞–º'—è—Ç—ñ –ø–æ—Ç—Ä—ñ–±–Ω–æ —Ç–∞ –Ω–∞—Å–∫—ñ–ª—å–∫–∏ —à–≤–∏–¥–∫–æ –≥–µ–Ω–µ—Ä—É—é—Ç—å—Å—è —Ç–æ–∫–µ–Ω–∏. –ë—ñ–ª—å—à—ñ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ Hugging Face –¥–æ—Å—Ç—É–ø–Ω—ñ —É –∫—ñ–ª—å–∫–æ—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö."
            ]
          },
          "details": {
            "en": [
              {
                "text": "GGUF (llama.cpp)",
                "desc": "The most versatile format for local inference. Supports CPU, GPU, and mixed CPU+GPU execution. Single-file distribution with embedded metadata. The go-to format for running models on consumer hardware.",
                "links": [
                  {
                    "title": "Hardware Basics",
                    "href": "hardware.html"
                  }
                ]
              },
              {
                "text": "GPTQ",
                "desc": "GPU-optimized post-training quantization format. Models are quantized to 4-bit or 8-bit with calibration data. Faster than GGUF on pure GPU but requires the full model to fit in VRAM.",
                "links": [
                  {
                    "title": "Model Optimization",
                    "href": "../level-3/model-optimization.html"
                  }
                ]
              },
              {
                "text": "AWQ (Activation-Aware Quantization)",
                "desc": "Advanced GPU quantization that preserves important weights based on activation patterns. Generally better quality than GPTQ at the same bit width. Supported by vLLM and TensorRT-LLM.",
                "links": []
              },
              {
                "text": "SafeTensors",
                "desc": "Hugging Face safe serialization format that prevents arbitrary code execution on load, supports memory-mapping for fast loading, and is now the default format on Hugging Face Hub.",
                "links": []
              },
              {
                "text": "ONNX (Open Neural Network Exchange)",
                "desc": "Cross-platform format supported by Microsoft, Google, and others. Enables running models on different hardware (CPU, GPU, NPU) through ONNX Runtime. Used for edge deployment and mobile inference.",
                "links": []
              },
              {
                "text": "ExLlamaV2 and EXL2",
                "desc": "Highly optimized GPU inference with variable quantization ‚Äî different layers can use different bit widths. Achieves the best perplexity-per-bit among quantized formats. Popular for enthusiast setups.",
                "links": []
              },
              {
                "text": "TensorRT-LLM",
                "desc": "NVIDIA high-performance inference engine. Compiles models into optimized execution plans for NVIDIA GPUs. Maximum throughput for production serving but requires NVIDIA hardware and compilation step.",
                "links": [
                  {
                    "title": "Hardware Basics",
                    "href": "hardware.html"
                  }
                ]
              },
              {
                "text": "Quantization Levels",
                "desc": "Q2 (smallest, lowest quality) through Q8 (largest, highest quality). Q4_K_M is the sweet spot for GGUF ‚Äî good quality with reasonable size. Q5+ recommended for reasoning-heavy tasks.",
                "links": [
                  {
                    "title": "Model Optimization",
                    "href": "../level-3/model-optimization.html"
                  }
                ]
              },
              {
                "text": "Model Distribution",
                "desc": "Hugging Face is the primary hub. Models are uploaded in multiple formats by quantization specialists (TheBloke, bartowski). Ollama and LM Studio download GGUF models with one-click setup.",
                "links": []
              },
              {
                "text": "Choosing the Right Format",
                "desc": "Consumer GPU: GGUF or EXL2. Production GPU server: AWQ or TensorRT-LLM. CPU only: GGUF. Cross-platform: ONNX. Mobile/edge: ONNX or CoreML. When in doubt, start with GGUF Q4_K_M.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "GGUF (llama.cpp)",
                "desc": "–ù–∞–π—É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—à–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É. –ü—ñ–¥—Ç—Ä–∏–º—É—î CPU, GPU —Ç–∞ –∑–º—ñ—à–∞–Ω–µ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è CPU+GPU. –†–æ–∑–ø–æ–≤—Å—é–¥–∂–µ–Ω–Ω—è –æ–¥–Ω–∏–º —Ñ–∞–π–ª–æ–º –∑ –≤–±—É–¥–æ–≤–∞–Ω–∏–º–∏ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏. –û—Å–Ω–æ–≤–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Å–ø–æ–∂–∏–≤—á–æ–≥–æ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è",
                    "href": "hardware.html"
                  }
                ]
              },
              {
                "text": "GPTQ",
                "desc": "–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –¥–ª—è GPU —Ñ–æ—Ä–º–∞—Ç –ø–æ—Å—Ç-—Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ—ó –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—ó. –ú–æ–¥–µ–ª—ñ –∫–≤–∞–Ω—Ç–∏–∑—É—é—Ç—å—Å—è –¥–æ 4-–±—ñ—Ç –∞–±–æ 8-–±—ñ—Ç –∑ –∫–∞–ª—ñ–±—Ä–∞—Ü—ñ–π–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏. –®–≤–∏–¥—à–∏–π –∑–∞ GGUF –Ω–∞ —á–∏—Å—Ç–æ–º—É GPU, –∞–ª–µ –≤–∏–º–∞–≥–∞—î –ø–æ–≤–Ω–æ–≥–æ –≤–º—ñ—â–µ–Ω–Ω—è –≤ VRAM.",
                "links": [
                  {
                    "title": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π",
                    "href": "../level-3/model-optimization.html"
                  }
                ]
              },
              {
                "text": "AWQ (Activation-Aware Quantization)",
                "desc": "–ü—Ä–æ—Å—É–Ω—É—Ç–∞ GPU-–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—è, —â–æ –∑–±–µ—Ä—ñ–≥–∞—î –≤–∞–∂–ª–∏–≤—ñ –≤–∞–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–∞—Ç–µ—Ä–Ω—ñ–≤ –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó. –ó–∞–∑–≤–∏—á–∞–π –∫—Ä–∞—â–∞ —è–∫—ñ—Å—Ç—å –Ω—ñ–∂ GPTQ –ø—Ä–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ–π –±—ñ—Ç–Ω–æ—Å—Ç—ñ. –ü—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è vLLM —Ç–∞ TensorRT-LLM.",
                "links": []
              },
              {
                "text": "SafeTensors",
                "desc": "–ë–µ–∑–ø–µ—á–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç —Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –≤—ñ–¥ Hugging Face, —â–æ –∑–∞–ø–æ–±—ñ–≥–∞—î –≤–∏–∫–æ–Ω–∞–Ω–Ω—é –¥–æ–≤—ñ–ª—å–Ω–æ–≥–æ –∫–æ–¥—É –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ, –ø—ñ–¥—Ç—Ä–∏–º—É—î memory-mapping —Ç–∞ —î —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –Ω–∞ Hugging Face Hub.",
                "links": []
              },
              {
                "text": "ONNX (Open Neural Network Exchange)",
                "desc": "–ö—Ä–æ—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç –≤—ñ–¥ Microsoft, Google —Ç–∞ —ñ–Ω—à–∏—Ö. –î–æ–∑–≤–æ–ª—è—î –∑–∞–ø—É—Å–∫–∞—Ç–∏ –º–æ–¥–µ–ª—ñ –Ω–∞ —Ä—ñ–∑–Ω–æ–º—É –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—ñ (CPU, GPU, NPU) —á–µ—Ä–µ–∑ ONNX Runtime. –î–ª—è edge-–¥–µ–ø–ª–æ—é —Ç–∞ –º–æ–±—ñ–ª—å–Ω–æ–≥–æ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É.",
                "links": []
              },
              {
                "text": "ExLlamaV2 —Ç–∞ EXL2",
                "desc": "–í–∏—Å–æ–∫–æ–æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π GPU-—ñ–Ω—Ñ–µ—Ä–µ–Ω—Å –∑—ñ –∑–º—ñ–Ω–Ω–æ—é –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—î—é ‚Äî —Ä—ñ–∑–Ω—ñ —à–∞—Ä–∏ –º–æ–∂—É—Ç—å –º–∞—Ç–∏ —Ä—ñ–∑–Ω—É –±—ñ—Ç–Ω—ñ—Å—Ç—å. –ù–∞–π–∫—Ä–∞—â–∞ —è–∫—ñ—Å—Ç—å –Ω–∞ –±—ñ—Ç —Å–µ—Ä–µ–¥ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤. –ü–æ–ø—É–ª—è—Ä–Ω–∏–π —Å–µ—Ä–µ–¥ –µ–Ω—Ç—É–∑—ñ–∞—Å—Ç—ñ–≤.",
                "links": []
              },
              {
                "text": "TensorRT-LLM",
                "desc": "–í–∏—Å–æ–∫–æ–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–∏–π –¥–≤–∏–∂–æ–∫ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –≤—ñ–¥ NVIDIA. –ö–æ–º–ø—ñ–ª—é—î –º–æ–¥–µ–ª—ñ –≤ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ –ø–ª–∞–Ω–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –¥–ª—è GPU NVIDIA. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –ø—Ä–æ–ø—É—Å–∫–Ω–∞ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω-—Å–µ—Ä–≤–µ—Ä—ñ–≤, –∞–ª–µ –≤–∏–º–∞–≥–∞—î –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è NVIDIA.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è",
                    "href": "hardware.html"
                  }
                ]
              },
              {
                "text": "–†—ñ–≤–Ω—ñ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—ó",
                "desc": "–í—ñ–¥ Q2 (–Ω–∞–π–º–µ–Ω—à–∏–π, –Ω–∞–π–Ω–∏–∂—á–∞ —è–∫—ñ—Å—Ç—å) –¥–æ Q8 (–Ω–∞–π–±—ñ–ª—å—à–∏–π, –Ω–∞–π–≤–∏—â–∞ —è–∫—ñ—Å—Ç—å). Q4_K_M ‚Äî –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –±–∞–ª–∞–Ω—Å –¥–ª—è GGUF. Q5+ —Ä–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –¥–ª—è –∑–∞–¥–∞—á –∑ —ñ–Ω—Ç–µ–Ω—Å–∏–≤–Ω–∏–º –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è–º.",
                "links": [
                  {
                    "title": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π",
                    "href": "../level-3/model-optimization.html"
                  }
                ]
              },
              {
                "text": "–†–æ–∑–ø–æ–≤—Å—é–¥–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π",
                "desc": "Hugging Face ‚Äî –æ—Å–Ω–æ–≤–Ω–∏–π —Ö–∞–±. –ú–æ–¥–µ–ª—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—é—Ç—å—Å—è —É –∫—ñ–ª—å–∫–æ—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö —Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç–∞–º–∏ –∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—ó (TheBloke, bartowski). Ollama —Ç–∞ LM Studio –∑–∞–≤–∞–Ω—Ç–∞–∂—É—é—Ç—å GGUF –º–æ–¥–µ–ª—ñ –≤ –æ–¥–∏–Ω –∫–ª—ñ–∫.",
                "links": []
              },
              {
                "text": "–í–∏–±—ñ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—É",
                "desc": "–°–ø–æ–∂–∏–≤—á–∏–π GPU: GGUF –∞–±–æ EXL2. –ü—Ä–æ–¥–∞–∫—à–Ω GPU-—Å–µ—Ä–≤–µ—Ä: AWQ –∞–±–æ TensorRT-LLM. –¢—ñ–ª—å–∫–∏ CPU: GGUF. –ö—Ä–æ—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞: ONNX. –ú–æ–±—ñ–ª—å–Ω–∏–π/edge: ONNX –∞–±–æ CoreML. –Ø–∫—â–æ —Å—É–º–Ω—ñ–≤–∞—î—Ç–µ—Å—å: GGUF Q4_K_M.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "GGUF",
                "def": "llama.cpp model format supporting CPU+GPU inference ‚Äî the most popular format for running models on consumer hardware."
              },
              {
                "term": "SafeTensors",
                "def": "Secure model serialization format that prevents code execution attacks during model loading."
              },
              {
                "term": "Quantization Level",
                "def": "Bit precision of model weights (Q2-Q8) ‚Äî lower bits mean smaller files but reduced quality."
              },
              {
                "term": "VRAM",
                "def": "Video RAM on GPU ‚Äî the primary constraint determining which model sizes and formats can run on your hardware."
              }
            ],
            "uk": [
              {
                "term": "GGUF",
                "def": "–§–æ—Ä–º–∞—Ç –º–æ–¥–µ–ª–µ–π llama.cpp –¥–ª—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É CPU+GPU ‚Äî –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –∑–∞–ø—É—Å–∫—É –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–ø–æ–∂–∏–≤—á–æ–º—É –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—ñ."
              },
              {
                "term": "SafeTensors",
                "def": "–ë–µ–∑–ø–µ—á–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç —Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –º–æ–¥–µ–ª–µ–π, —â–æ –∑–∞–ø–æ–±—ñ–≥–∞—î –∞—Ç–∞–∫–∞–º –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∫–æ–¥—É –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ."
              },
              {
                "term": "–†—ñ–≤–µ–Ω—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—ó",
                "def": "–ë—ñ—Ç–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –≤–∞–≥ –º–æ–¥–µ–ª—ñ (Q2-Q8) ‚Äî –º–µ–Ω—à–µ –±—ñ—Ç –æ–∑–Ω–∞—á–∞—î –º–µ–Ω—à—ñ —Ñ–∞–π–ª–∏, –∞–ª–µ –∑–Ω–∏–∂–µ–Ω—É —è–∫—ñ—Å—Ç—å."
              },
              {
                "term": "VRAM",
                "def": "–í—ñ–¥–µ–æ–ø–∞–º'—è—Ç—å GPU ‚Äî –æ—Å–Ω–æ–≤–Ω–µ –æ–±–º–µ–∂–µ–Ω–Ω—è, —â–æ –≤–∏–∑–Ω–∞—á–∞—î —è–∫—ñ —Ä–æ–∑–º—ñ—Ä–∏ —Ç–∞ —Ñ–æ—Ä–º–∞—Ç–∏ –º–æ–¥–µ–ª–µ–π –º–æ–∂—É—Ç—å –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –Ω–∞ –≤–∞—à–æ–º—É –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—ñ."
              }
            ]
          },
          "tips": {
            "en": [
              "Start with GGUF Q4_K_M for any new model ‚Äî it is the best balance of quality and size for most consumer setups",
              "If you have a dedicated NVIDIA GPU with enough VRAM, try AWQ or EXL2 for noticeably faster inference than GGUF",
              "Always use SafeTensors when available ‚Äî never load untrusted serialized model files that could execute arbitrary code"
            ],
            "uk": [
              "–ü–æ—á–Ω—ñ—Ç—å –∑ GGUF Q4_K_M –¥–ª—è –±—É–¥—å-—è–∫–æ—ó –Ω–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ ‚Äî —Ü–µ –Ω–∞–π–∫—Ä–∞—â–∏–π –±–∞–ª–∞–Ω—Å —è–∫–æ—Å—Ç—ñ —Ç–∞ —Ä–æ–∑–º—ñ—Ä—É –¥–ª—è –±—ñ–ª—å—à–æ—Å—Ç—ñ —Å–ø–æ–∂–∏–≤—á–∏—Ö –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π",
              "–Ø–∫—â–æ –º–∞—î—Ç–µ –≤–∏–¥—ñ–ª–µ–Ω–∏–π GPU NVIDIA –∑ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ—é VRAM, —Å–ø—Ä–æ–±—É–π—Ç–µ AWQ –∞–±–æ EXL2 –¥–ª—è –ø–æ–º—ñ—Ç–Ω–æ —à–≤–∏–¥—à–æ–≥–æ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –Ω—ñ–∂ GGUF",
              "–ó–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ SafeTensors –∫–æ–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ ‚Äî –Ω—ñ–∫–æ–ª–∏ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–π—Ç–µ –Ω–µ–Ω–∞–¥—ñ–π–Ω—ñ —Å–µ—Ä—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ —Ñ–∞–π–ª–∏ –º–æ–¥–µ–ª–µ–π, —â–æ –º–æ–∂—É—Ç—å –≤–∏–∫–æ–Ω–∞—Ç–∏ –¥–æ–≤—ñ–ª—å–Ω–∏–π –∫–æ–¥"
            ]
          },
          "related": [
            "Models"
          ]
        },
        {
          "slug": "ai-protocols",
          "title": {
            "en": "AI Protocols",
            "uk": "–®–Ü-–ø—Ä–æ—Ç–æ–∫–æ–ª–∏"
          },
          "desc": {
            "en": "Communication protocols connecting AI models to tools and services.",
            "uk": "–ö–æ–º—É–Ω—ñ–∫–∞—Ü—ñ–π–Ω—ñ –ø—Ä–æ—Ç–æ–∫–æ–ª–∏, —â–æ –∑'—î–¥–Ω—É—é—Ç—å –º–æ–¥–µ–ª—ñ –®–Ü –∑ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —Ç–∞ —Å–µ—Ä–≤—ñ—Å–∞–º–∏."
          },
          "overview": {
            "en": [
              "As AI systems become more capable, they need standardized ways to communicate with tools, data sources, and other agents. AI protocols define these interfaces ‚Äî from MCP (Model Context Protocol) which connects models to external tools via a universal standard, to A2A (Agent-to-Agent) for inter-agent communication, to the streaming protocols that deliver real-time responses.",
              "These protocols are the \"plumbing\" of the AI ecosystem. Just as HTTP standardized web communication, protocols like MCP are standardizing how AI models interact with the world. Understanding them is crucial for building interoperable AI applications that can plug into the growing ecosystem of tools and services."
            ],
            "uk": [
              "–í –º—ñ—Ä—É –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π —Å–∏—Å—Ç–µ–º –®–Ü, —ó–º –ø–æ—Ç—Ä—ñ–±–Ω—ñ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω—ñ —Å–ø–æ—Å–æ–±–∏ –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—ó –∑ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏, –¥–∂–µ—Ä–µ–ª–∞–º–∏ –¥–∞–Ω–∏—Ö —Ç–∞ —ñ–Ω—à–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏. –®–Ü-–ø—Ä–æ—Ç–æ–∫–æ–ª–∏ –≤–∏–∑–Ω–∞—á–∞—é—Ç—å —Ü—ñ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏ ‚Äî –≤—ñ–¥ MCP (Model Context Protocol), —â–æ –∑'—î–¥–Ω—É—î –º–æ–¥–µ–ª—ñ —ñ–∑ –∑–æ–≤–Ω—ñ—à–Ω—ñ–º–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —á–µ—Ä–µ–∑ —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç, –¥–æ A2A (Agent-to-Agent) –¥–ª—è –º—ñ–∂–∞–≥–µ–Ω—Ç–Ω–æ—ó –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—ó, –¥–æ –ø—Ä–æ—Ç–æ–∫–æ–ª—ñ–≤ —Å—Ç—Ä–∏–º—ñ–Ω–≥—É –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π —É —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ.",
              "–¶—ñ –ø—Ä–æ—Ç–æ–∫–æ–ª–∏ ‚Äî —Ü–µ \"—Å–∞–Ω—Ç–µ—Ö–Ω—ñ–∫–∞\" –µ–∫–æ—Å–∏—Å—Ç–µ–º–∏ –®–Ü. –¢–∞–∫ —Å–∞–º–æ —è–∫ HTTP —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑—É–≤–∞–≤ –≤–µ–±-–∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—é, –ø—Ä–æ—Ç–æ–∫–æ–ª–∏ —è–∫ MCP —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑—É—é—Ç—å –≤–∑–∞—î–º–æ–¥—ñ—é –º–æ–¥–µ–ª–µ–π –®–Ü –∑—ñ —Å–≤—ñ—Ç–æ–º. –†–æ–∑—É–º—ñ–Ω–Ω—è —ó—Ö –∫—Ä–∏—Ç–∏—á–Ω–µ –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ —ñ–Ω—Ç–µ—Ä–æ–ø–µ—Ä–∞–±–µ–ª—å–Ω–∏—Ö –®–Ü-–¥–æ–¥–∞—Ç–∫—ñ–≤, —â–æ –º–æ–∂—É—Ç—å –ø—ñ–¥–∫–ª—é—á–∞—Ç–∏—Å—è –¥–æ –∑—Ä–æ—Å—Ç–∞—é—á–æ—ó –µ–∫–æ—Å–∏—Å—Ç–µ–º–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ —Ç–∞ —Å–µ—Ä–≤—ñ—Å—ñ–≤."
            ]
          },
          "details": {
            "en": [
              {
                "text": "MCP (Model Context Protocol)",
                "desc": "Anthropic's open standard for connecting AI to tools and data. Servers expose resources, tools, and prompts via a standard JSON-RPC protocol. One MCP server works with any MCP-compatible client (Claude, Cursor, etc.).",
                "links": [
                  {
                    "title": "Tool Use",
                    "href": "tool-use.html"
                  }
                ]
              },
              {
                "text": "MCP Architecture",
                "desc": "Client-server model: MCP hosts (AI apps) connect to MCP servers (tool providers). Servers declare capabilities. Transport layers: stdio (local), SSE/HTTP (remote). Stateful sessions with capability negotiation.",
                "links": []
              },
              {
                "text": "A2A (Agent-to-Agent Protocol)",
                "desc": "Google's protocol for agent interoperability. Enables agents built on different frameworks to discover each other, negotiate capabilities, and collaborate on tasks. Complementary to MCP (tools) ‚Äî A2A handles agent-agent communication.",
                "links": [
                  {
                    "title": "Agents",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "OpenAI Function Calling",
                "desc": "The first widely adopted tool-use protocol. Define functions with JSON Schema, model generates structured calls. Now an industry standard that Anthropic, Google, and others have adopted with variations.",
                "links": [
                  {
                    "title": "Tool Use",
                    "href": "tool-use.html"
                  }
                ]
              },
              {
                "text": "Server-Sent Events (SSE)",
                "desc": "One-way streaming protocol for delivering LLM tokens in real-time. HTTP-based, simple to implement, works through proxies. Used by OpenAI, Anthropic, and most API providers for streaming responses.",
                "links": []
              },
              {
                "text": "WebSocket AI Communication",
                "desc": "Bidirectional streaming for real-time AI interactions. Used for voice AI (OpenAI Realtime API, Google Gemini Live), interactive agents, and scenarios requiring low-latency two-way communication.",
                "links": []
              },
              {
                "text": "OpenAI Assistants Protocol",
                "desc": "Stateful conversation management with threads, runs, and built-in tools (code interpreter, file search). Higher-level than raw chat completions. Influenced how other providers design managed agent APIs.",
                "links": []
              },
              {
                "text": "Structured Output Standards",
                "desc": "JSON Schema-based output enforcement across providers. OpenAI strict mode, Anthropic tool results, Google controlled generation. Converging toward a common standard for structured AI responses.",
                "links": [
                  {
                    "title": "Prompting Techniques",
                    "href": "prompting-techniques.html"
                  }
                ]
              },
              {
                "text": "OAuth and Authentication",
                "desc": "Emerging standards for secure tool access. MCP supports OAuth 2.0 for authenticated server connections. Key management, token rotation, and scope-based permissions for AI-to-service authentication.",
                "links": []
              },
              {
                "text": "The Protocol Landscape",
                "desc": "MCP is for model-to-tool. A2A is for agent-to-agent. Function calling is for in-context tool use. SSE/WebSocket for streaming. The ecosystem is converging but not yet fully standardized ‚Äî expect consolidation.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "MCP (Model Context Protocol)",
                "desc": "–í—ñ–¥–∫—Ä–∏—Ç–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç Anthropic –¥–ª—è –∑'—î–¥–Ω–∞–Ω–Ω—è –®–Ü –∑ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —Ç–∞ –¥–∞–Ω–∏–º–∏. –°–µ—Ä–≤–µ—Ä–∏ –Ω–∞–¥–∞—é—Ç—å —Ä–µ—Å—É—Ä—Å–∏, —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —Ç–∞ –ø—Ä–æ–º–ø—Ç–∏ —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π JSON-RPC –ø—Ä–æ—Ç–æ–∫–æ–ª. –û–¥–∏–Ω MCP-—Å–µ—Ä–≤–µ—Ä –ø—Ä–∞—Ü—é—î –∑ –±—É–¥—å-—è–∫–∏–º MCP-–∫–ª—ñ—î–Ω—Ç–æ–º (Claude, Cursor —Ç–æ—â–æ).",
                "links": [
                  {
                    "title": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤",
                    "href": "tool-use.html"
                  }
                ]
              },
              {
                "text": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ MCP",
                "desc": "–ö–ª—ñ—î–Ω—Ç-—Å–µ—Ä–≤–µ—Ä–Ω–∞ –º–æ–¥–µ–ª—å: MCP-—Ö–æ—Å—Ç–∏ (–®–Ü-–¥–æ–¥–∞—Ç–∫–∏) –∑'—î–¥–Ω—É—é—Ç—å—Å—è –∑ MCP-—Å–µ—Ä–≤–µ—Ä–∞–º–∏ (–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤). –°–µ—Ä–≤–µ—Ä–∏ –¥–µ–∫–ª–∞—Ä—É—é—Ç—å –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ. –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç: stdio (–ª–æ–∫–∞–ª—å–Ω–∏–π), SSE/HTTP (–≤—ñ–¥–¥–∞–ª–µ–Ω–∏–π). Stateful —Å–µ—Å—ñ—ó –∑ —É–∑–≥–æ–¥–∂–µ–Ω–Ω—è–º –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π.",
                "links": []
              },
              {
                "text": "A2A (Agent-to-Agent Protocol)",
                "desc": "–ü—Ä–æ—Ç–æ–∫–æ–ª Google –¥–ª—è —ñ–Ω—Ç–µ—Ä–æ–ø–µ—Ä–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ –∞–≥–µ–Ω—Ç—ñ–≤. –î–æ–∑–≤–æ–ª—è—î –∞–≥–µ–Ω—Ç–∞–º –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞—Ö –∑–Ω–∞—Ö–æ–¥–∏—Ç–∏ –æ–¥–Ω–µ –æ–¥–Ω–æ–≥–æ, —É–∑–≥–æ–¥–∂—É–≤–∞—Ç–∏ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —Ç–∞ —Å–ø—ñ–≤–ø—Ä–∞—Ü—é–≤–∞—Ç–∏. –î–æ–ø–æ–≤–Ω—é—î MCP (—ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏) ‚Äî A2A –æ–±—Ä–æ–±–ª—è—î –∞–≥–µ–Ω—Ç-–∞–≥–µ–Ω—Ç –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—é.",
                "links": [
                  {
                    "title": "–ê–≥–µ–Ω—Ç–∏",
                    "href": "agents.html"
                  }
                ]
              },
              {
                "text": "–í–∏–∫–ª–∏–∫ —Ñ—É–Ω–∫—Ü—ñ–π OpenAI",
                "desc": "–ü–µ—Ä—à–∏–π —à–∏—Ä–æ–∫–æ –ø—Ä–∏–π–Ω—è—Ç–∏–π –ø—Ä–æ—Ç–æ–∫–æ–ª tool-use. –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ–π —á–µ—Ä–µ–∑ JSON Schema, –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä—É—î —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ –≤–∏–∫–ª–∏–∫–∏. –¢–µ–ø–µ—Ä —ñ–Ω–¥—É—Å—Ç—Ä—ñ–∞–ª—å–Ω–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç, –ø—Ä–∏–π–Ω—è—Ç–∏–π Anthropic, Google —Ç–∞ —ñ–Ω—à–∏–º–∏.",
                "links": [
                  {
                    "title": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤",
                    "href": "tool-use.html"
                  }
                ]
              },
              {
                "text": "Server-Sent Events (SSE)",
                "desc": "–û–¥–Ω–æ—Å–ø—Ä—è–º–æ–≤–∞–Ω–∏–π —Å—Ç—Ä—ñ–º—ñ–Ω–≥-–ø—Ä–æ—Ç–æ–∫–æ–ª –¥–ª—è –¥–æ—Å—Ç–∞–≤–∫–∏ —Ç–æ–∫–µ–Ω—ñ–≤ LLM —É —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ. –ù–∞ –±–∞–∑—ñ HTTP, –ø—Ä–æ—Å—Ç–∏–π —É —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó, –ø—Ä–∞—Ü—é—î —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å—ñ. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è OpenAI, Anthropic —Ç–∞ –±—ñ–ª—å—à—ñ—Å—Ç—é API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤.",
                "links": []
              },
              {
                "text": "WebSocket –®–Ü-–∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—è",
                "desc": "–î–≤–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Å—Ç—Ä—ñ–º—ñ–Ω–≥ –¥–ª—è –®–Ü-–≤–∑–∞—î–º–æ–¥—ñ–π —É —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –®–Ü (OpenAI Realtime API, Google Gemini Live), —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏—Ö –∞–≥–µ–Ω—Ç—ñ–≤ —Ç–∞ —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤ –∑ –Ω–∏–∑—å–∫–æ—é –∑–∞—Ç—Ä–∏–º–∫–æ—é.",
                "links": []
              },
              {
                "text": "–ü—Ä–æ—Ç–æ–∫–æ–ª OpenAI Assistants",
                "desc": "Stateful —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —Ä–æ–∑–º–æ–≤–∞–º–∏ –∑ threads, runs —Ç–∞ –≤–±—É–¥–æ–≤–∞–Ω–∏–º–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ (code interpreter, file search). –í–∏—â–∏–π —Ä—ñ–≤–µ–Ω—å –Ω—ñ–∂ —Å–∏—Ä—ñ chat completions. –í–ø–ª–∏–Ω—É–≤ –Ω–∞ –¥–∏–∑–∞–π–Ω –∫–µ—Ä–æ–≤–∞–Ω–∏—Ö –∞–≥–µ–Ω—Ç–Ω–∏—Ö API —ñ–Ω—à–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤.",
                "links": []
              },
              {
                "text": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–æ–≥–æ –≤–∏—Ö–æ–¥—É",
                "desc": "–ü—Ä–∏–º—É—Å–æ–≤–µ –¥–æ—Ç—Ä–∏–º–∞–Ω–Ω—è JSON Schema —É –≤—Å—ñ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤. Strict mode OpenAI, tool results Anthropic, controlled generation Google. –ö–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü—ñ—è –¥–æ —Å–ø—ñ–ª—å–Ω–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—É —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –®–Ü.",
                "links": [
                  {
                    "title": "–¢–µ—Ö–Ω—ñ–∫–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É",
                    "href": "prompting-techniques.html"
                  }
                ]
              },
              {
                "text": "OAuth —Ç–∞ –∞–≤—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è",
                "desc": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏ –±–µ–∑–ø–µ—á–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø—É –¥–æ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤, —â–æ —Ñ–æ—Ä–º—É—é—Ç—å—Å—è. MCP –ø—ñ–¥—Ç—Ä–∏–º—É—î OAuth 2.0 –¥–ª—è –∞–≤—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–æ–≤–∞–Ω–∏—Ö –∑'—î–¥–Ω–∞–Ω—å. –£–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –∫–ª—é—á–∞–º–∏, —Ä–æ—Ç–∞—Ü—ñ—è —Ç–æ–∫–µ–Ω—ñ–≤ —Ç–∞ –ø—Ä–∏–≤—ñ–ª–µ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Å–∫–æ—É–ø—ñ–≤.",
                "links": []
              },
              {
                "text": "–õ–∞–Ω–¥—à–∞—Ñ—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª—ñ–≤",
                "desc": "MCP –¥–ª—è –º–æ–¥–µ–ª—å-—ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç. A2A –¥–ª—è –∞–≥–µ–Ω—Ç-–∞–≥–µ–Ω—Ç. Function calling –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ tool use. SSE/WebSocket –¥–ª—è —Å—Ç—Ä–∏–º—ñ–Ω–≥—É. –ï–∫–æ—Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω–≤–µ—Ä–≥—É—î, –∞–ª–µ —â–µ –Ω–µ –ø–æ–≤–Ω—ñ—Å—Ç—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–∞ ‚Äî –æ—á—ñ–∫—É–π—Ç–µ –∫–æ–Ω—Å–æ–ª—ñ–¥–∞—Ü—ñ—é.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "MCP",
                "def": "Model Context Protocol ‚Äî Anthropic's open standard for universal tool and data connectivity for AI models."
              },
              {
                "term": "A2A",
                "def": "Agent-to-Agent Protocol ‚Äî Google's standard for enabling AI agents to discover and communicate with each other."
              },
              {
                "term": "SSE",
                "def": "Server-Sent Events ‚Äî HTTP-based protocol for streaming LLM responses token by token in real time."
              },
              {
                "term": "JSON-RPC",
                "def": "Remote procedure call protocol using JSON ‚Äî the transport mechanism underlying MCP communication."
              }
            ],
            "uk": [
              {
                "term": "MCP",
                "def": "Model Context Protocol ‚Äî –≤—ñ–¥–∫—Ä–∏—Ç–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç Anthropic –¥–ª—è —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–æ—ó –∑'—î–¥–Ω—É–≤–∞–Ω–æ—Å—Ç—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ —Ç–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –º–æ–¥–µ–ª–µ–π –®–Ü."
              },
              {
                "term": "A2A",
                "def": "Agent-to-Agent Protocol ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç Google –¥–ª—è –≤–∑–∞—î–º–Ω–æ–≥–æ –≤–∏—è–≤–ª–µ–Ω–Ω—è —Ç–∞ –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—ó –®–Ü-–∞–≥–µ–Ω—Ç—ñ–≤."
              },
              {
                "term": "SSE",
                "def": "Server-Sent Events ‚Äî HTTP-–ø—Ä–æ—Ç–æ–∫–æ–ª –¥–ª—è —Å—Ç—Ä–∏–º—ñ–Ω–≥—É –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π LLM —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º —É —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ."
              },
              {
                "term": "JSON-RPC",
                "def": "–ü—Ä–æ—Ç–æ–∫–æ–ª –≤—ñ–¥–¥–∞–ª–µ–Ω–æ–≥–æ –≤–∏–∫–ª–∏–∫—É –ø—Ä–æ—Ü–µ–¥—É—Ä —á–µ—Ä–µ–∑ JSON ‚Äî —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∏–π –º–µ—Ö–∞–Ω—ñ–∑–º –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—ó MCP."
              }
            ]
          },
          "tips": {
            "en": [
              "Learn MCP first ‚Äî it is becoming the universal standard and is already supported by Claude, Cursor, VS Code, and many other tools",
              "For streaming responses in web apps, SSE is simpler than WebSockets unless you need bidirectional communication (like voice)",
              "When building tools for AI, prioritize good JSON Schema descriptions ‚Äî the protocol matters less than clear tool definitions"
            ],
            "uk": [
              "–í–∏–≤—á—ñ—Ç—å MCP –ø–µ—Ä—à–∏–º ‚Äî –≤—ñ–Ω —Å—Ç–∞—î —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º —ñ –≤–∂–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è Claude, Cursor, VS Code —Ç–∞ –±–∞–≥–∞—Ç—å–º–∞ —ñ–Ω—à–∏–º–∏",
              "–î–ª—è —Å—Ç—Ä–∏–º—ñ–Ω–≥—É –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π —É –≤–µ–±-–¥–æ–¥–∞—Ç–∫–∞—Ö SSE –ø—Ä–æ—Å—Ç—ñ—à–∏–π –∑–∞ WebSockets, —è–∫—â–æ –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –¥–≤–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—è (—è–∫ –≥–æ–ª–æ—Å)",
              "–ü—Ä–∏ –ø–æ–±—É–¥–æ–≤—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –®–Ü –ø—Ä—ñ–æ—Ä–∏—Ç–∏–∑—É–π—Ç–µ —Ö–æ—Ä–æ—à—ñ JSON Schema –æ–ø–∏—Å–∏ ‚Äî –ø—Ä–æ—Ç–æ–∫–æ–ª –º–µ–Ω—à –≤–∞–∂–ª–∏–≤–∏–π –Ω—ñ–∂ —á—ñ—Ç–∫—ñ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤"
            ]
          },
          "related": [
            "Agents & Tools",
            "Video Content"
          ]
        },
        {
          "slug": "hardware",
          "title": {
            "en": "Hardware Basics",
            "uk": "–û—Å–Ω–æ–≤–∏ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è"
          },
          "desc": {
            "en": "Hardware requirements for running AI models locally.",
            "uk": "–í–∏–º–æ–≥–∏ –¥–æ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫—É –º–æ–¥–µ–ª–µ–π –®–Ü."
          },
          "overview": {
            "en": [
              "Running AI models locally requires understanding the hardware constraints. The key bottleneck is memory ‚Äî specifically GPU VRAM for fast inference. A 7B parameter model needs about 4-6GB VRAM (quantized), while a 70B model needs 35-48GB. Your hardware determines which models you can run and how fast they generate tokens.",
              "The hardware landscape has democratized significantly. Apple Silicon Macs with unified memory can run surprisingly large models. Consumer NVIDIA GPUs (RTX 4090 with 24GB VRAM) handle 13B-34B models well. For larger models, cloud GPU providers offer pay-per-hour access. Understanding these options helps you choose the right balance of cost, speed, and capability."
            ],
            "uk": [
              "–ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π –®–Ü –ª–æ–∫–∞–ª—å–Ω–æ –≤–∏–º–∞–≥–∞—î —Ä–æ–∑—É–º—ñ–Ω–Ω—è –∞–ø–∞—Ä–∞—Ç–Ω–∏—Ö –æ–±–º–µ–∂–µ–Ω—å. –ö–ª—é—á–æ–≤–µ –≤—É–∑—å–∫–µ –º—ñ—Å—Ü–µ ‚Äî –ø–∞–º'—è—Ç—å, –∑–æ–∫—Ä–µ–º–∞ VRAM GPU –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É. –ú–æ–¥–µ–ª—å –Ω–∞ 7B –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ø–æ—Ç—Ä–µ–±—É—î –±–ª–∏–∑—å–∫–æ 4-6GB VRAM (–∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–∞), –∞ 70B ‚Äî 35-48GB. –í–∞—à–µ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è –≤–∏–∑–Ω–∞—á–∞—î —è–∫—ñ –º–æ–¥–µ–ª—ñ –≤–∏ –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å–∫–∞—Ç–∏ —Ç–∞ –Ω–∞—Å–∫—ñ–ª—å–∫–∏ —à–≤–∏–¥–∫–æ –≤–æ–Ω–∏ –≥–µ–Ω–µ—Ä—É—é—Ç—å —Ç–æ–∫–µ–Ω–∏.",
              "–õ–∞–Ω–¥—à–∞—Ñ—Ç –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è –∑–Ω–∞—á–Ω–æ –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑—É–≤–∞–≤—Å—è. Apple Silicon Mac –∑ unified memory –º–æ–∂–µ –∑–∞–ø—É—Å–∫–∞—Ç–∏ –Ω–∞–ø—Ä–æ—á—É–¥ –≤–µ–ª–∏–∫—ñ –º–æ–¥–µ–ª—ñ. –°–ø–æ–∂–∏–≤—á—ñ GPU NVIDIA (RTX 4090 –∑ 24GB VRAM) –¥–æ–±—Ä–µ –ø—Ä–∞—Ü—é—é—Ç—å –∑ 13B-34B –º–æ–¥–µ–ª—è–º–∏. –î–ª—è –±—ñ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ö–º–∞—Ä–Ω—ñ GPU-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏ –ø—Ä–æ–ø–æ–Ω—É—é—Ç—å –ø–æ–≥–æ–¥–∏–Ω–Ω–∏–π –¥–æ—Å—Ç—É–ø. –†–æ–∑—É–º—ñ–Ω–Ω—è —Ü–∏—Ö –æ–ø—Ü—ñ–π –¥–æ–ø–æ–º–∞–≥–∞—î –æ–±—Ä–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –±–∞–ª–∞–Ω—Å –≤–∞—Ä—Ç–æ—Å—Ç—ñ, —à–≤–∏–¥–∫–æ—Å—Ç—ñ —Ç–∞ –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π."
            ]
          },
          "details": {
            "en": [
              {
                "text": "GPU vs CPU Inference",
                "desc": "GPUs are 10-50x faster than CPUs for AI inference due to massive parallelism. CPUs work for small models or when using GGUF with CPU offloading. For any serious local AI work, a GPU is essential.",
                "links": []
              },
              {
                "text": "VRAM Requirements",
                "desc": "7B model: ~4GB (Q4), ~8GB (FP16). 13B: ~8GB (Q4). 34B: ~20GB (Q4). 70B: ~40GB (Q4). Rule of thumb: model size in GB at Q4 is roughly half the parameter count in billions.",
                "links": [
                  {
                    "title": "Model Formats",
                    "href": "model-formats.html"
                  }
                ]
              },
              {
                "text": "NVIDIA Consumer GPUs",
                "desc": "RTX 4090 (24GB, $1600) ‚Äî king of local LLMs. RTX 4080 (16GB). RTX 3090 (24GB, used ~$800) ‚Äî best value. RTX 4060 Ti 16GB ‚Äî budget option. VRAM matters more than compute speed for LLMs.",
                "links": []
              },
              {
                "text": "Apple Silicon",
                "desc": "M1/M2/M3/M4 Macs with unified memory can run large models. M2 Ultra (192GB) can run 70B+ models. M3 Max (128GB) handles 34B well. Slower than NVIDIA but memory bandwidth is excellent.",
                "links": []
              },
              {
                "text": "Data Center GPUs",
                "desc": "A100 (80GB), H100 (80GB), H200 (141GB) ‚Äî the hardware powering AI labs. 10-20x more expensive than consumer GPUs. Available through cloud providers for hourly rental.",
                "links": []
              },
              {
                "text": "Cloud GPU Providers",
                "desc": "RunPod, Vast.ai, Lambda Labs ‚Äî rent GPUs by the hour ($0.50-$4/hr for A100). Good for occasional use or running models too large for local hardware. No upfront investment.",
                "links": [
                  {
                    "title": "API Providers",
                    "href": "api-providers.html"
                  }
                ]
              },
              {
                "text": "Multi-GPU Setups",
                "desc": "Split large models across multiple GPUs. NVLink provides fast GPU-to-GPU communication on matching NVIDIA cards. Consumer GPUs can use PCIe with slower but functional model sharding.",
                "links": []
              },
              {
                "text": "Inference Engines",
                "desc": "llama.cpp (CPU+GPU, versatile), vLLM (high-throughput GPU serving), Ollama (easy local setup), LM Studio (GUI), TGI (HuggingFace serving). Each optimizes for different use cases.",
                "links": [
                  {
                    "title": "Model Formats",
                    "href": "model-formats.html"
                  }
                ]
              },
              {
                "text": "RAM and Storage",
                "desc": "System RAM matters for CPU inference and model loading. 32GB minimum, 64GB+ recommended. NVMe SSD dramatically speeds up model loading times (30-70B models are 20-40GB files).",
                "links": []
              },
              {
                "text": "Budget Configurations",
                "desc": "Entry ($500): used RTX 3060 12GB ‚Äî runs 7B models. Mid ($1500): RTX 4090 24GB ‚Äî runs up to 34B. High ($3000+): Mac Studio M2 Ultra or dual GPU. Budget: use cloud APIs instead of local hardware.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "GPU –ø—Ä–æ—Ç–∏ CPU —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É",
                "desc": "GPU —É 10-50 —Ä–∞–∑—ñ–≤ —à–≤–∏–¥—à–µ CPU –¥–ª—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –®–Ü –∑–∞–≤–¥—è–∫–∏ –º–∞—Å–∏–≤–Ω–æ–º—É –ø–∞—Ä–∞–ª–µ–ª—ñ–∑–º—É. CPU –ø—Ä–∞—Ü—é—î –¥–ª—è –º–∞–ª–∏—Ö –º–æ–¥–µ–ª–µ–π –∞–±–æ –∑ GGUF CPU offloading. –î–ª—è —Å–µ—Ä–π–æ–∑–Ω–æ—ó –ª–æ–∫–∞–ª—å–Ω–æ—ó —Ä–æ–±–æ—Ç–∏ –∑ –®–Ü GPU –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–π.",
                "links": []
              },
              {
                "text": "–í–∏–º–æ–≥–∏ –¥–æ VRAM",
                "desc": "7B –º–æ–¥–µ–ª—å: ~4GB (Q4), ~8GB (FP16). 13B: ~8GB (Q4). 34B: ~20GB (Q4). 70B: ~40GB (Q4). –ü—Ä–∞–≤–∏–ª–æ: —Ä–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ –≤ GB –ø—Ä–∏ Q4 –ø—Ä–∏–±–ª–∏–∑–Ω–æ –ø–æ–ª–æ–≤–∏–Ω–∞ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ —É –º—ñ–ª—å—è—Ä–¥–∞—Ö.",
                "links": [
                  {
                    "title": "–§–æ—Ä–º–∞—Ç–∏ –º–æ–¥–µ–ª–µ–π",
                    "href": "model-formats.html"
                  }
                ]
              },
              {
                "text": "–°–ø–æ–∂–∏–≤—á—ñ GPU NVIDIA",
                "desc": "RTX 4090 (24GB, $1600) ‚Äî –∫–æ—Ä–æ–ª—å –ª–æ–∫–∞–ª—å–Ω–∏—Ö LLM. RTX 4080 (16GB). RTX 3090 (24GB, –±/—É ~$800) ‚Äî –Ω–∞–π–∫—Ä–∞—â–∞ —Ü—ñ–Ω–Ω—ñ—Å—Ç—å. RTX 4060 Ti 16GB ‚Äî –±—é–¥–∂–µ—Ç–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç. VRAM –≤–∞–∂–ª–∏–≤—ñ—à–∞ –∑–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –¥–ª—è LLM.",
                "links": []
              },
              {
                "text": "Apple Silicon",
                "desc": "M1/M2/M3/M4 Mac –∑ unified memory –º–æ–∂—É—Ç—å –∑–∞–ø—É—Å–∫–∞—Ç–∏ –≤–µ–ª–∏–∫—ñ –º–æ–¥–µ–ª—ñ. M2 Ultra (192GB) –º–æ–∂–µ –∑–∞–ø—É—Å–∫–∞—Ç–∏ 70B+. M3 Max (128GB) –¥–æ–±—Ä–µ –ø—Ä–∞—Ü—é—î –∑ 34B. –ü–æ–≤—ñ–ª—å–Ω—ñ—à–∏–π –∑–∞ NVIDIA, –∞–ª–µ –ø—Ä–æ–ø—É—Å–∫–Ω–∞ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –ø–∞–º'—è—Ç—ñ –≤—ñ–¥–º—ñ–Ω–Ω–∞.",
                "links": []
              },
              {
                "text": "GPU —Å–µ—Ä–≤–µ—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—É",
                "desc": "A100 (80GB), H100 (80GB), H200 (141GB) ‚Äî –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è –®–Ü-–ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ–π. –£ 10-20 —Ä–∞–∑—ñ–≤ –¥–æ—Ä–æ–∂—á—ñ –∑–∞ —Å–ø–æ–∂–∏–≤—á—ñ GPU. –î–æ—Å—Ç—É–ø–Ω—ñ —á–µ—Ä–µ–∑ —Ö–º–∞—Ä–Ω–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤ –¥–ª—è –ø–æ–≥–æ–¥–∏–Ω–Ω–æ—ó –æ—Ä–µ–Ω–¥–∏.",
                "links": []
              },
              {
                "text": "–•–º–∞—Ä–Ω—ñ GPU-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                "desc": "RunPod, Vast.ai, Lambda Labs ‚Äî –æ—Ä–µ–Ω–¥–∞ GPU –ø–æ–≥–æ–¥–∏–Ω–Ω–æ ($0.50-$4/–≥–æ–¥ –∑–∞ A100). –î–æ–±—Ä–µ –¥–ª—è –Ω–µ—á–∞—Å—Ç–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∞–±–æ –º–æ–¥–µ–ª–µ–π –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∏—Ö –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è. –ë–µ–∑ –ø–æ—á–∞—Ç–∫–æ–≤–∏—Ö —ñ–Ω–≤–µ—Å—Ç–∏—Ü—ñ–π.",
                "links": [
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "api-providers.html"
                  }
                ]
              },
              {
                "text": "–ú—É–ª—å—Ç–∏-GPU –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó",
                "desc": "–†–æ–∑–ø–æ–¥—ñ–ª –≤–µ–ª–∏–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –º—ñ–∂ –∫—ñ–ª—å–∫–æ–º–∞ GPU. NVLink –∑–∞–±–µ–∑–ø–µ—á—É—î —à–≤–∏–¥–∫—É GPU-GPU –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—é. –°–ø–æ–∂–∏–≤—á—ñ GPU –º–æ–∂—É—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ PCIe –∑ –ø–æ–≤—ñ–ª—å–Ω—ñ—à–∏–º, –∞–ª–µ —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–º —à–∞—Ä–¥–∏–Ω–≥–æ–º –º–æ–¥–µ–ª–µ–π.",
                "links": []
              },
              {
                "text": "–î–≤–∏–∂–∫–∏ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É",
                "desc": "llama.cpp (CPU+GPU, —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π), vLLM (–≤–∏—Å–æ–∫–æ–ø—Ä–æ–ø—É—Å–∫–Ω–∏–π GPU-—Å–µ—Ä–≤—ñ–Ω–≥), Ollama (–ø—Ä–æ—Å—Ç–∏–π –ª–æ–∫–∞–ª—å–Ω–∏–π –∑–∞–ø—É—Å–∫), LM Studio (GUI), TGI (—Å–µ—Ä–≤—ñ–Ω–≥ HuggingFace). –ö–æ–∂–µ–Ω –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –ø—ñ–¥ —Ä—ñ–∑–Ω—ñ –≤–∏–ø–∞–¥–∫–∏.",
                "links": [
                  {
                    "title": "–§–æ—Ä–º–∞—Ç–∏ –º–æ–¥–µ–ª–µ–π",
                    "href": "model-formats.html"
                  }
                ]
              },
              {
                "text": "RAM —Ç–∞ —Å—Ö–æ–≤–∏—â–µ",
                "desc": "–°–∏—Å—Ç–µ–º–Ω–∞ RAM –≤–∞–∂–ª–∏–≤–∞ –¥–ª—è CPU-—ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π. –ú—ñ–Ω—ñ–º—É–º 32GB, —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ 64GB+. NVMe SSD –¥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∏—Å–∫–æ—Ä—é—î –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π (30-70B –º–æ–¥–µ–ª—ñ ‚Äî —Ñ–∞–π–ª–∏ 20-40GB).",
                "links": []
              },
              {
                "text": "–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –∑–∞ –±—é–¥–∂–µ—Ç–æ–º",
                "desc": "–ë–∞–∑–æ–≤–∞ ($500): –±/—É RTX 3060 12GB ‚Äî –∑–∞–ø—É—Å–∫ 7B –º–æ–¥–µ–ª–µ–π. –°–µ—Ä–µ–¥–Ω—è ($1500): RTX 4090 24GB ‚Äî –¥–æ 34B. –í–∏—Å–æ–∫–∞ ($3000+): Mac Studio M2 Ultra –∞–±–æ dual GPU. –ë—é–¥–∂–µ—Ç: —Ö–º–∞—Ä–Ω—ñ API –∑–∞–º—ñ—Å—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "VRAM",
                "def": "Video RAM on the GPU ‚Äî the primary constraint for which AI models can run locally."
              },
              {
                "term": "Unified Memory",
                "def": "Apple Silicon architecture where CPU and GPU share the same memory pool, enabling larger models on Mac."
              },
              {
                "term": "Model Sharding",
                "def": "Splitting a model across multiple GPUs when it is too large to fit in a single GPU's VRAM."
              },
              {
                "term": "Inference Engine",
                "def": "Software that loads and runs AI models ‚Äî llama.cpp, vLLM, Ollama, TensorRT-LLM each optimize for different scenarios."
              }
            ],
            "uk": [
              {
                "term": "VRAM",
                "def": "–í—ñ–¥–µ–æ–ø–∞–º'—è—Ç—å GPU ‚Äî –æ—Å–Ω–æ–≤–Ω–µ –æ–±–º–µ–∂–µ–Ω–Ω—è –¥–ª—è —Ç–æ–≥–æ, —è–∫—ñ –º–æ–¥–µ–ª—ñ –®–Ü –º–æ–∂—É—Ç—å –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ."
              },
              {
                "term": "Unified Memory",
                "def": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ Apple Silicon, –¥–µ CPU —Ç–∞ GPU –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —Å–ø—ñ–ª—å–Ω–∏–π –ø—É–ª –ø–∞–º'—è—Ç—ñ, –¥–æ–∑–≤–æ–ª—è—é—á–∏ –±—ñ–ª—å—à—ñ –º–æ–¥–µ–ª—ñ –Ω–∞ Mac."
              },
              {
                "term": "–®–∞—Ä–¥–∏–Ω–≥ –º–æ–¥–µ–ª—ñ",
                "def": "–†–æ–∑–ø–æ–¥—ñ–ª –º–æ–¥–µ–ª—ñ –º—ñ–∂ –∫—ñ–ª—å–∫–æ–º–∞ GPU, –∫–æ–ª–∏ –≤–æ–Ω–∞ –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∞ –¥–ª—è VRAM –æ–¥–Ω–æ–≥–æ GPU."
              },
              {
                "term": "–î–≤–∏–∂–æ–∫ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É",
                "def": "–ü–ó –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –∑–∞–ø—É—Å–∫—É –º–æ–¥–µ–ª–µ–π –®–Ü ‚Äî llama.cpp, vLLM, Ollama, TensorRT-LLM –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ –ø—ñ–¥ —Ä—ñ–∑–Ω—ñ —Å—Ü–µ–Ω–∞—Ä—ñ—ó."
              }
            ]
          },
          "tips": {
            "en": [
              "VRAM is the most important spec ‚Äî an RTX 3090 with 24GB VRAM often beats an RTX 4080 with 16GB for LLM work",
              "Start with Ollama or LM Studio for the easiest local setup experience before diving into raw llama.cpp",
              "If you only need occasional access to large models, cloud GPU rental ($1-2/hour) is cheaper than buying hardware"
            ],
            "uk": [
              "VRAM ‚Äî –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞: RTX 3090 –∑ 24GB VRAM —á–∞—Å—Ç–æ –ø–µ—Ä–µ–º–∞–≥–∞—î RTX 4080 –∑ 16GB –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ LLM",
              "–ü–æ—á–Ω—ñ—Ç—å –∑ Ollama –∞–±–æ LM Studio –¥–ª—è –Ω–∞–π–ø—Ä–æ—Å—Ç—ñ—à–æ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–æ—Å–≤—ñ–¥—É –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ—Ö–æ–¥–æ–º –¥–æ —Å–∏—Ä–æ–≥–æ llama.cpp",
              "–Ø–∫—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω –ª–∏—à–µ –Ω–µ—á–∞—Å—Ç–∏–π –¥–æ—Å—Ç—É–ø –¥–æ –≤–µ–ª–∏–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, –æ—Ä–µ–Ω–¥–∞ —Ö–º–∞—Ä–Ω–æ–≥–æ GPU ($1-2/–≥–æ–¥) –¥–µ—à–µ–≤—à–∞ –∑–∞ –∫—É–ø—ñ–≤–ª—é –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è"
            ]
          },
          "related": [
            "Models"
          ]
        },
        {
          "slug": "api-providers",
          "title": {
            "en": "API Providers",
            "uk": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏"
          },
          "desc": {
            "en": "Cloud API providers for accessing AI models without local hardware.",
            "uk": "–•–º–∞—Ä–Ω—ñ API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏ –¥–ª—è –¥–æ—Å—Ç—É–ø—É –¥–æ –º–æ–¥–µ–ª–µ–π –®–Ü –±–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è."
          },
          "overview": {
            "en": [
              "API providers offer cloud-hosted AI models accessible via HTTP APIs ‚Äî no hardware, no model management, pay-per-use pricing. The big three (OpenAI, Anthropic, Google) develop their own frontier models, while inference providers (Together AI, Groq, Fireworks) host open-source models at competitive prices. Aggregators like OpenRouter provide a single API for all providers.",
              "Choosing a provider involves balancing model quality, latency, cost, and features. OpenAI offers the broadest ecosystem, Anthropic excels at complex reasoning and safety, Google provides the largest context windows. For open models, inference providers can be 5-10x cheaper than the big three. Understanding the landscape helps you optimize for your specific use case."
            ],
            "uk": [
              "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏ –ø—Ä–æ–ø–æ–Ω—É—é—Ç—å —Ö–º–∞—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ –®–Ü, –¥–æ—Å—Ç—É–ø–Ω—ñ —á–µ—Ä–µ–∑ HTTP API ‚Äî –±–µ–∑ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è, –±–µ–∑ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –º–æ–¥–µ–ª—è–º–∏, –æ–ø–ª–∞—Ç–∞ –∑–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è. –í–µ–ª–∏–∫—ñ —Ç—Ä–∏ (OpenAI, Anthropic, Google) —Ä–æ–∑—Ä–æ–±–ª—è—é—Ç—å –≤–ª–∞—Å–Ω—ñ —Ñ—Ä–æ–Ω—Ç—ñ—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ, –∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É (Together AI, Groq, Fireworks) —Ö–æ—Å—Ç—è—Ç—å open-source –º–æ–¥–µ–ª—ñ –∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏–º–∏ —Ü—ñ–Ω–∞–º–∏. –ê–≥—Ä–µ–≥–∞—Ç–æ—Ä–∏ —è–∫ OpenRouter –Ω–∞–¥–∞—é—Ç—å —î–¥–∏–Ω–∏–π API –¥–ª—è –≤—Å—ñ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤.",
              "–í–∏–±—ñ—Ä –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –≤–∫–ª—é—á–∞—î –±–∞–ª–∞–Ω—Å—É–≤–∞–Ω–Ω—è —è–∫–æ—Å—Ç—ñ –º–æ–¥–µ–ª–µ–π, –∑–∞—Ç—Ä–∏–º–∫–∏, –≤–∞—Ä—Ç–æ—Å—Ç—ñ —Ç–∞ —Ñ—É–Ω–∫—Ü—ñ–π. OpenAI –ø—Ä–æ–ø–æ–Ω—É—î –Ω–∞–π—à–∏—Ä—à—É –µ–∫–æ—Å–∏—Å—Ç–µ–º—É, Anthropic –≤—ñ–¥–º—ñ–Ω–Ω–∏–π —É —Å–∫–ª–∞–¥–Ω–æ–º—É –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—ñ —Ç–∞ –±–µ–∑–ø–µ—Ü—ñ, Google –Ω–∞–¥–∞—î –Ω–∞–π–±—ñ–ª—å—à—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ñ –≤—ñ–∫–Ω–∞. –î–ª—è –≤—ñ–¥–∫—Ä–∏—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –º–æ–∂—É—Ç—å –±—É—Ç–∏ –≤ 5-10 —Ä–∞–∑—ñ–≤ –¥–µ—à–µ–≤—à–∏–º–∏. –†–æ–∑—É–º—ñ–Ω–Ω—è –ª–∞–Ω–¥—à–∞—Ñ—Ç—É –¥–æ–ø–æ–º–∞–≥–∞—î –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ –ø—ñ–¥ –≤–∞—à –≤–∏–ø–∞–¥–æ–∫."
            ]
          },
          "details": {
            "en": [
              {
                "text": "OpenAI API",
                "desc": "GPT-4o, GPT-4-turbo, o1/o3 reasoning models. The largest ecosystem: assistants, fine-tuning, image generation (DALL-E), speech-to-text (Whisper), embeddings. The default choice for most projects.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Anthropic API",
                "desc": "Claude Opus, Sonnet, and Haiku models. Excels at complex analysis, coding, and long-context tasks (200K tokens). Features: tool use, vision, prompt caching, batch API. Known for safety and instruction-following.",
                "links": [
                  {
                    "title": "Tool Use",
                    "href": "tool-use.html"
                  }
                ]
              },
              {
                "text": "Google AI (Gemini)",
                "desc": "Gemini Pro and Ultra with 1M+ token context windows. Gemini API for developers, Vertex AI for enterprise. Multimodal native: text, images, video, audio in one model. Competitive pricing.",
                "links": [
                  {
                    "title": "Multimodality",
                    "href": "../level-1/multimodality.html"
                  }
                ]
              },
              {
                "text": "OpenRouter",
                "desc": "Unified API gateway for 100+ models across all major providers. Single API key, consistent format, automatic fallbacks. Great for comparing models or switching providers without code changes.",
                "links": []
              },
              {
                "text": "Together AI",
                "desc": "Leading open-source model hosting. Runs Llama, Mixtral, Qwen, and other open models at low cost. Fine-tuning service included. Often 5-10x cheaper than frontier model APIs for similar-quality open models.",
                "links": []
              },
              {
                "text": "Groq",
                "desc": "Specialized inference provider using custom LPU chips. Extremely fast inference (500+ tokens/second) for supported models. Best for latency-critical applications where speed matters most.",
                "links": [
                  {
                    "title": "Hardware Basics",
                    "href": "hardware.html"
                  }
                ]
              },
              {
                "text": "Fireworks AI",
                "desc": "Fast inference with function calling optimization. Strong at serving fine-tuned models and compound AI systems. Good balance of speed, cost, and features for production workloads.",
                "links": []
              },
              {
                "text": "Pricing Models",
                "desc": "Pay-per-token (most providers), pay-per-second (some inference), subscription tiers (OpenAI Plus). Input tokens are cheaper than output. Prompt caching (Anthropic, Google) reduces costs for repeated prefixes.",
                "links": [
                  {
                    "title": "Token",
                    "href": "../level-2/token.html"
                  }
                ]
              },
              {
                "text": "Cost Optimization",
                "desc": "Use smaller models for simple tasks (Haiku, GPT-4o-mini). Cache prompts for repeated contexts. Batch non-urgent requests (50% discount). Use open models via inference providers when frontier quality is not needed.",
                "links": []
              },
              {
                "text": "Provider Selection Strategy",
                "desc": "Prototype: OpenAI (best docs, widest support). Complex reasoning: Anthropic Claude. Long context: Google Gemini. Budget: Together AI or Groq. Production: start with one, add OpenRouter for fallback.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "OpenAI API",
                "desc": "GPT-4o, GPT-4-turbo, o1/o3 –º–æ–¥–µ–ª—ñ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è. –ù–∞–π–±—ñ–ª—å—à–∞ –µ–∫–æ—Å–∏—Å—Ç–µ–º–∞: –∞—Å–∏—Å—Ç–µ–Ω—Ç–∏, —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥, –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–æ–±—Ä–∞–∂–µ–Ω—å (DALL-E), —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–ª–µ–Ω–Ω—è (Whisper), –µ–º–±–µ–¥—ñ–Ω–≥–∏. –í–∏–±—ñ—Ä –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Anthropic API",
                "desc": "–ú–æ–¥–µ–ª—ñ Claude Opus, Sonnet —Ç–∞ Haiku. –í—ñ–¥–º—ñ–Ω–Ω–∏–π —É —Å–∫–ª–∞–¥–Ω–æ–º—É –∞–Ω–∞–ª—ñ–∑—ñ, –∫–æ–¥—É–≤–∞–Ω–Ω—ñ —Ç–∞ –∑–∞–¥–∞—á–∞—Ö –∑ –¥–æ–≤–≥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (200K —Ç–æ–∫–µ–Ω—ñ–≤). –§—É–Ω–∫—Ü—ñ—ó: tool use, –≤—ñ–∑—ñ—è, –∫–µ—à—É–≤–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤, batch API. –í—ñ–¥–æ–º–∏–π –±–µ–∑–ø–µ–∫–æ—é.",
                "links": [
                  {
                    "title": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤",
                    "href": "tool-use.html"
                  }
                ]
              },
              {
                "text": "Google AI (Gemini)",
                "desc": "Gemini Pro —Ç–∞ Ultra –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏–º–∏ –≤—ñ–∫–Ω–∞–º–∏ 1M+ —Ç–æ–∫–µ–Ω—ñ–≤. Gemini API –¥–ª—è —Ä–æ–∑—Ä–æ–±–Ω–∏–∫—ñ–≤, Vertex AI –¥–ª—è –µ–Ω—Ç–µ—Ä–ø—Ä–∞–π–∑—É. –ù–∞—Ç–∏–≤–Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å: —Ç–µ–∫—Å—Ç, –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –≤—ñ–¥–µ–æ, –∞—É–¥—ñ–æ –≤ –æ–¥–Ω—ñ–π –º–æ–¥–µ–ª—ñ.",
                "links": [
                  {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å",
                    "href": "../level-1/multimodality.html"
                  }
                ]
              },
              {
                "text": "OpenRouter",
                "desc": "–Ñ–¥–∏–Ω–∏–π API-—à–ª—é–∑ –¥–ª—è 100+ –º–æ–¥–µ–ª–µ–π –≤—ñ–¥ —É—Å—ñ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤. –û–¥–∏–Ω API-–∫–ª—é—á, –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç, –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω—ñ —Ñ–æ–ª–±–µ–∫–∏. –í—ñ–¥–º—ñ–Ω–Ω–∏–π –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –∞–±–æ –∑–º—ñ–Ω–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤ –±–µ–∑ –∑–º—ñ–Ω–∏ –∫–æ–¥—É.",
                "links": []
              },
              {
                "text": "Together AI",
                "desc": "–ü—Ä–æ–≤—ñ–¥–Ω–∏–π —Ö–æ—Å—Ç–∏–Ω–≥ open-source –º–æ–¥–µ–ª–µ–π. –ó–∞–ø—É—Å–∫–∞—î Llama, Mixtral, Qwen —Ç–∞ —ñ–Ω—à—ñ –º–æ–¥–µ–ª—ñ –∑–∞ –Ω–∏–∑—å–∫–æ—é —Ü—ñ–Ω–æ—é. –°–µ—Ä–≤—ñ—Å —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥—É –≤–∫–ª—é—á–µ–Ω–æ. –ß–∞—Å—Ç–æ —É 5-10 —Ä–∞–∑—ñ–≤ –¥–µ—à–µ–≤—à–∏–π –∑–∞ —Ñ—Ä–æ–Ω—Ç—ñ—Ä–Ω—ñ API.",
                "links": []
              },
              {
                "text": "Groq",
                "desc": "–°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –∑ –∫–∞—Å—Ç–æ–º–Ω–∏–º–∏ LPU-—á—ñ–ø–∞–º–∏. –ù–∞–¥–∑–≤–∏—á–∞–π–Ω–æ —à–≤–∏–¥–∫–∏–π —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å (500+ —Ç–æ–∫–µ–Ω—ñ–≤/—Å–µ–∫). –ù–∞–π–∫—Ä–∞—â–∏–π –¥–ª—è –¥–æ–¥–∞—Ç–∫—ñ–≤, –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –¥–æ –∑–∞—Ç—Ä–∏–º–∫–∏, –¥–µ —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∞.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è",
                    "href": "hardware.html"
                  }
                ]
              },
              {
                "text": "Fireworks AI",
                "desc": "–®–≤–∏–¥–∫–∏–π —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—î—é function calling. –°–∏–ª—å–Ω–∏–π —É —Å–µ—Ä–≤—ñ–Ω–≥—É —Ñ–∞–π–Ω-—Ç—é–Ω–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π —Ç–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö –®–Ü-—Å–∏—Å—Ç–µ–º. –î–æ–±—Ä–∏–π –±–∞–ª–∞–Ω—Å —à–≤–∏–¥–∫–æ—Å—Ç—ñ, –≤–∞—Ä—Ç–æ—Å—Ç—ñ —Ç–∞ —Ñ—É–Ω–∫—Ü—ñ–π –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω-–Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—å.",
                "links": []
              },
              {
                "text": "–ú–æ–¥–µ–ª—ñ —Ü—ñ–Ω–æ—É—Ç–≤–æ—Ä–µ–Ω–Ω—è",
                "desc": "–û–ø–ª–∞—Ç–∞ –∑–∞ —Ç–æ–∫–µ–Ω (–±—ñ–ª—å—à—ñ—Å—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤), –∑–∞ —Å–µ–∫—É–Ω–¥—É (–¥–µ—è–∫—ñ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å), –ø—ñ–¥–ø–∏—Å–Ω—ñ —Ä—ñ–≤–Ω—ñ (OpenAI Plus). –í—Ö—ñ–¥–Ω—ñ —Ç–æ–∫–µ–Ω–∏ –¥–µ—à–µ–≤—à—ñ –∑–∞ –≤–∏—Ö—ñ–¥–Ω—ñ. –ö–µ—à—É–≤–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤ (Anthropic, Google) –∑–º–µ–Ω—à—É—î –≤–∏—Ç—Ä–∞—Ç–∏.",
                "links": [
                  {
                    "title": "–¢–æ–∫–µ–Ω",
                    "href": "../level-2/token.html"
                  }
                ]
              },
              {
                "text": "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –≤–∏—Ç—Ä–∞—Ç",
                "desc": "–ú–µ–Ω—à—ñ –º–æ–¥–µ–ª—ñ –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö –∑–∞–¥–∞—á (Haiku, GPT-4o-mini). –ö–µ—à—É–≤–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ–≤. –ü–∞–∫–µ—Ç–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –Ω–µ—Å—Ä–æ—á–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤ (–∑–Ω–∏–∂–∫–∞ 50%). –í—ñ–¥–∫—Ä–∏—Ç—ñ –º–æ–¥–µ–ª—ñ —á–µ—Ä–µ–∑ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É.",
                "links": []
              },
              {
                "text": "–°—Ç—Ä–∞—Ç–µ–≥—ñ—è –≤–∏–±–æ—Ä—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞",
                "desc": "–ü—Ä–æ—Ç–æ—Ç–∏–ø: OpenAI (–Ω–∞–π–∫—Ä–∞—â—ñ –¥–æ–∫–∏). –°–∫–ª–∞–¥–Ω–µ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è: Anthropic Claude. –î–æ–≤–≥–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: Google Gemini. –ë—é–¥–∂–µ—Ç: Together AI –∞–±–æ Groq. –ü—Ä–æ–¥–∞–∫—à–Ω: –ø–æ—á–Ω—ñ—Ç—å –∑ –æ–¥–Ω–æ–≥–æ, –¥–æ–¥–∞–π—Ç–µ OpenRouter –¥–ª—è —Ñ–æ–ª–±–µ–∫—É.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Inference Provider",
                "def": "Service that hosts and runs AI models, offering API access without requiring your own hardware or model management."
              },
              {
                "term": "Token Pricing",
                "def": "Pay-per-use model where costs are calculated based on number of input and output tokens processed."
              },
              {
                "term": "Prompt Caching",
                "def": "Provider feature that reduces costs and latency by caching repeated prompt prefixes across API calls."
              },
              {
                "term": "API Gateway",
                "def": "Unified access point (like OpenRouter) that routes requests to multiple AI providers through a single API."
              }
            ],
            "uk": [
              {
                "term": "–ü—Ä–æ–≤–∞–π–¥–µ—Ä —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É",
                "def": "–°–µ—Ä–≤—ñ—Å —â–æ —Ö–æ—Å—Ç–∏—Ç—å —Ç–∞ –∑–∞–ø—É—Å–∫–∞—î –º–æ–¥–µ–ª—ñ –®–Ü, –Ω–∞–¥–∞—é—á–∏ API-–¥–æ—Å—Ç—É–ø –±–µ–∑ –≤–ª–∞—Å–Ω–æ–≥–æ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è —á–∏ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –º–æ–¥–µ–ª—è–º–∏."
              },
              {
                "term": "–¢–æ–∫–µ–Ω–Ω–µ —Ü—ñ–Ω–æ—É—Ç–≤–æ—Ä–µ–Ω–Ω—è",
                "def": "–ú–æ–¥–µ–ª—å –æ–ø–ª–∞—Ç–∏ –∑–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è, –¥–µ –≤–∏—Ç—Ä–∞—Ç–∏ —Ä–æ–∑—Ä–∞—Ö–æ–≤—É—é—Ç—å—Å—è –∑–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—é –æ–±—Ä–æ–±–ª–µ–Ω–∏—Ö –≤—Ö—ñ–¥–Ω–∏—Ö —Ç–∞ –≤–∏—Ö—ñ–¥–Ω–∏—Ö —Ç–æ–∫–µ–Ω—ñ–≤."
              },
              {
                "term": "–ö–µ—à—É–≤–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤",
                "def": "–§—É–Ω–∫—Ü—ñ—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞, —â–æ –∑–º–µ–Ω—à—É—î –≤–∏—Ç—Ä–∞—Ç–∏ —Ç–∞ –∑–∞—Ç—Ä–∏–º–∫—É –∫–µ—à—É–≤–∞–Ω–Ω—è–º –ø–æ–≤—Ç–æ—Ä–Ω–∏—Ö –ø—Ä–µ—Ñ—ñ–∫—Å—ñ–≤ –ø—Ä–æ–º–ø—Ç—ñ–≤ –º—ñ–∂ –≤–∏–∫–ª–∏–∫–∞–º–∏ API."
              },
              {
                "term": "API-—à–ª—é–∑",
                "def": "–Ñ–¥–∏–Ω–∞ —Ç–æ—á–∫–∞ –¥–æ—Å—Ç—É–ø—É (—è–∫ OpenRouter), —â–æ –º–∞—Ä—à—Ä—É—Ç–∏–∑—É—î –∑–∞–ø–∏—Ç–∏ –¥–æ –∫—ñ–ª—å–∫–æ—Ö –®–Ü-–ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤ —á–µ—Ä–µ–∑ –æ–¥–∏–Ω API."
              }
            ]
          },
          "tips": {
            "en": [
              "Use GPT-4o-mini or Claude Haiku for simple tasks ‚Äî they are 10-20x cheaper than frontier models and fast enough for most use cases",
              "Always implement provider fallbacks in production ‚Äî OpenRouter makes this easy with automatic routing between providers",
              "Enable prompt caching on Anthropic and Google when you have repeated system prompts ‚Äî it can cut costs by 90% for cached portions"
            ],
            "uk": [
              "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ GPT-4o-mini –∞–±–æ Claude Haiku –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö –∑–∞–¥–∞—á ‚Äî –≤–æ–Ω–∏ —É 10-20 —Ä–∞–∑—ñ–≤ –¥–µ—à–µ–≤—à—ñ –∑–∞ —Ñ—Ä–æ–Ω—Ç—ñ—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ —Ç–∞ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ —à–≤–∏–¥–∫—ñ",
              "–ó–∞–≤–∂–¥–∏ –≤–ø—Ä–æ–≤–∞–¥–∂—É–π—Ç–µ —Ñ–æ–ª–±–µ–∫–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤ —É –ø—Ä–æ–¥–∞–∫—à–Ω—ñ ‚Äî OpenRouter —Ä–æ–±–∏—Ç—å —Ü–µ –ø—Ä–æ—Å—Ç–∏–º –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—é –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—î—é",
              "–£–≤—ñ–º–∫–Ω—ñ—Ç—å –∫–µ—à—É–≤–∞–Ω–Ω—è –ø—Ä–æ–º–ø—Ç—ñ–≤ –Ω–∞ Anthropic —Ç–∞ Google –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–∏—Ö —Å–∏—Å—Ç–µ–º–Ω–∏—Ö –ø—Ä–æ–º–ø—Ç–∞—Ö ‚Äî —Ü–µ –∑–º–µ–Ω—à—É—î –≤–∏—Ç—Ä–∞—Ç–∏ –Ω–∞ 90% –¥–ª—è –∫–µ—à–æ–≤–∞–Ω–∏—Ö —á–∞—Å—Ç–∏–Ω"
            ]
          },
          "related": [
            "Agents & Tools"
          ]
        }
      ]
    },
    {
      "num": 5,
      "emoji": "üåå",
      "title": {
        "en": "Horizons",
        "uk": "–ì–æ—Ä–∏–∑–æ–Ω—Ç–∏"
      },
      "desc": {
        "en": "Future of AI: AGI, safety, alignment, and philosophical questions about artificial intelligence.",
        "uk": "–ú–∞–π–±—É—Ç–Ω—î –®–Ü: AGI, –±–µ–∑–ø–µ–∫–∞, –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è —Ç–∞ —Ñ—ñ–ª–æ—Å–æ—Ñ—Å—å–∫—ñ –ø–∏—Ç–∞–Ω–Ω—è –ø—Ä–æ —à—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç."
      },
      "topics": [
        {
          "slug": "agi",
          "title": {
            "en": "AGI (Artificial General Intelligence)",
            "uk": "AGI (–ó–∞–≥–∞–ª—å–Ω–∏–π —à—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç)"
          },
          "desc": {
            "en": "The quest for human-level AI that can perform any intellectual task.",
            "uk": "–ü—Ä–∞–≥–Ω–µ–Ω–Ω—è –¥–æ –®–Ü –ª—é–¥—Å—å–∫–æ–≥–æ —Ä—ñ–≤–Ω—è, —â–æ –º–æ–∂–µ –≤–∏–∫–æ–Ω–∞—Ç–∏ –±—É–¥—å-—è–∫–µ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–µ –∑–∞–≤–¥–∞–Ω–Ω—è."
          },
          "overview": {
            "en": [
              "AGI refers to an AI system that can match or exceed human cognitive abilities across virtually any intellectual domain ‚Äî not just narrow tasks like chess or image recognition, but general-purpose reasoning, learning, and problem-solving. Unlike today's AI which excels at specific tasks, AGI would transfer knowledge between domains and handle novel situations without specific training.",
              "The concept is simultaneously one of the most debated and consequential in AI. Industry leaders disagree on timelines (from 2-5 years to \"never\"), on definitions (what counts as \"general\"?), and on implications (utopia vs existential risk). What is clear is that current AI capabilities are advancing rapidly toward increasingly general competence, making AGI discussions increasingly practical rather than theoretical."
            ],
            "uk": [
              "AGI –æ–∑–Ω–∞—á–∞—î —Å–∏—Å—Ç–µ–º—É –®–Ü, —â–æ –º–æ–∂–µ –∑—Ä—ñ–≤–Ω—è—Ç–∏—Å—è –∞–±–æ –ø–µ—Ä–µ–≤–µ—Ä—à–∏—Ç–∏ –ª—é–¥—Å—å–∫—ñ –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω—ñ –∑–¥—ñ–±–Ω–æ—Å—Ç—ñ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ –≤ –±—É–¥—å-—è–∫—ñ–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω—ñ–π —Å—Ñ–µ—Ä—ñ ‚Äî –Ω–µ –ª–∏—à–µ —É –≤—É–∑—å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö —è–∫ —à–∞—Ö–∏ —á–∏ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å, –∞ –≤ –∑–∞–≥–∞–ª—å–Ω–æ–º—É –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—ñ, –Ω–∞–≤—á–∞–Ω–Ω—ñ —Ç–∞ —Ä–æ–∑–≤'—è–∑–∞–Ω–Ω—ñ –ø—Ä–æ–±–ª–µ–º. –ù–∞ –≤—ñ–¥–º—ñ–Ω—É –≤—ñ–¥ —Å—É—á–∞—Å–Ω–æ–≥–æ –®–Ü, AGI –ø–µ—Ä–µ–Ω–æ—Å–∏–≤ –±–∏ –∑–Ω–∞–Ω–Ω—è –º—ñ–∂ –¥–æ–º–µ–Ω–∞–º–∏ —Ç–∞ —Å–ø—Ä–∞–≤–ª—è–≤—Å—è –∑ –Ω–æ–≤–∏–º–∏ —Å–∏—Ç—É–∞—Ü—ñ—è–º–∏ –±–µ–∑ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è.",
              "–ö–æ–Ω—Ü–µ–ø—Ü—ñ—è –æ–¥–Ω–æ—á–∞—Å–Ω–æ —î –æ–¥–Ω—ñ—î—é –∑ –Ω–∞–π–±—ñ–ª—å—à –¥–∏—Å–∫—É—Ç–æ–≤–∞–Ω–∏—Ö —Ç–∞ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö –≤ –®–Ü. –õ—ñ–¥–µ—Ä–∏ —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó –Ω–µ –∑–≥–æ–¥–Ω—ñ —â–æ–¥–æ —Ç–µ—Ä–º—ñ–Ω—ñ–≤ (–≤—ñ–¥ 2-5 —Ä–æ–∫—ñ–≤ –¥–æ \"–Ω—ñ–∫–æ–ª–∏\"), –≤–∏–∑–Ω–∞—á–µ–Ω—å (—â–æ –≤–≤–∞–∂–∞—Ç–∏ \"–∑–∞–≥–∞–ª—å–Ω–∏–º\"?), —Ç–∞ –Ω–∞—Å–ª—ñ–¥–∫—ñ–≤ (—É—Ç–æ–ø—ñ—è —á–∏ –µ–∫–∑–∏—Å—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π —Ä–∏–∑–∏–∫). –Ø—Å–Ω–æ –æ–¥–Ω–µ: –ø–æ—Ç–æ—á–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –®–Ü —à–≤–∏–¥–∫–æ –ø—Ä–æ—Å—É–≤–∞—é—Ç—å—Å—è –¥–æ –≤—Å–µ –±—ñ–ª—å—à –∑–∞–≥–∞–ª—å–Ω–æ—ó –∫–æ–º–ø–µ—Ç–µ–Ω—Ü—ñ—ó, —Ä–æ–±–ª—è—á–∏ –¥–∏—Å–∫—É—Å—ñ—ó –ø—Ä–æ AGI –≤—Å–µ –±—ñ–ª—å—à –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–º–∏."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Defining AGI",
                "desc": "No consensus definition exists. Common criteria: human-level performance across cognitive tasks, ability to learn new domains independently, transfer learning, and common-sense reasoning. Some define it economically: AI that can do any remote work a human can.",
                "links": []
              },
              {
                "text": "AGI vs Narrow AI",
                "desc": "Current AI is narrow: GPT excels at text, DALL-E at images, AlphaFold at proteins ‚Äî but none can do all three. AGI implies a single system with general capabilities. The gap is narrowing as models become increasingly multimodal and general.",
                "links": [
                  {
                    "title": "Generative AI",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "Current Progress",
                "desc": "LLMs can now reason, code, analyze, create, and use tools. Claude and GPT-4 match or exceed human performance on many benchmarks. But they still struggle with novel reasoning, persistent memory, and truly autonomous action. We are arguably at \"narrow AGI\" for cognitive tasks.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Timeline Predictions",
                "desc": "Sam Altman (OpenAI): AGI by 2025-2027. Dario Amodei (Anthropic): \"powerful AI\" within 2-3 years. Yann LeCun (Meta): decades away, current approaches insufficient. Survey of AI researchers: median estimate around 2040-2060.",
                "links": []
              },
              {
                "text": "Levels of AGI",
                "desc": "Google DeepMind proposed 5 levels: Level 1 (Emerging) ‚Äî chatbots. Level 2 (Competent) ‚Äî equal to skilled adults. Level 3 (Expert). Level 4 (Virtuoso). Level 5 (Superhuman). Current frontier models are arguably Level 1-2.",
                "links": []
              },
              {
                "text": "Missing Capabilities",
                "desc": "Current AI lacks: persistent long-term memory, true world modeling, autonomous goal-setting, physical world understanding, efficient learning from few examples, and robust common-sense reasoning. These gaps define the distance to AGI.",
                "links": [
                  {
                    "title": "General World Model",
                    "href": "world-model.html"
                  }
                ]
              },
              {
                "text": "Paths to AGI",
                "desc": "Scaling current architectures (more data, compute), new architectures (state-space models, neurosymbolic), hybrid approaches (LLMs + world models + planning), or fundamentally new paradigms. Most labs are betting on scaling with architecture innovations.",
                "links": []
              },
              {
                "text": "Economic Impact",
                "desc": "AGI could automate most knowledge work: programming, analysis, writing, research, design. Estimates suggest 300M+ jobs affected. But it also creates new economic value ‚Äî the question is distribution and transition speed.",
                "links": []
              },
              {
                "text": "AGI and Safety",
                "desc": "More capable AI demands stronger safety measures. The closer we get to AGI, the more critical alignment becomes ‚Äî ensuring AGI shares human values and remains controllable. This is the central concern of AI safety research.",
                "links": [
                  {
                    "title": "AI Safety",
                    "href": "ai-safety.html"
                  }
                ]
              },
              {
                "text": "The Social Question",
                "desc": "AGI raises profound questions: What happens to work, education, creativity? How do we distribute benefits equitably? Who controls it? International competition vs cooperation? These are not just technical questions but civilizational ones.",
                "links": [
                  {
                    "title": "AI Alignment",
                    "href": "alignment.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–í–∏–∑–Ω–∞—á–µ–Ω–Ω—è AGI",
                "desc": "–ö–æ–Ω—Å–µ–Ω—Å—É—Å–Ω–æ–≥–æ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –Ω–µ —ñ—Å–Ω—É—î. –¢–∏–ø–æ–≤—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—ó: –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –ª—é–¥—Å—å–∫–æ–≥–æ —Ä—ñ–≤–Ω—è –≤ –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—á–∏—Ç–∏—Å—è –Ω–æ–≤–∏–º –¥–æ–º–µ–Ω–∞–º, —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –∑–¥–æ—Ä–æ–≤–∏–π –≥–ª—É–∑–¥. –î–µ—è–∫—ñ –≤–∏–∑–Ω–∞—á–∞—é—Ç—å –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ: –®–Ü, —â–æ –º–æ–∂–µ —Ä–æ–±–∏—Ç–∏ –±—É–¥—å-—è–∫—É –≤—ñ–¥–¥–∞–ª–µ–Ω—É —Ä–æ–±–æ—Ç—É –ª—é–¥–∏–Ω–∏.",
                "links": []
              },
              {
                "text": "AGI –ø—Ä–æ—Ç–∏ –≤—É–∑—å–∫–æ–≥–æ –®–Ü",
                "desc": "–ü–æ—Ç–æ—á–Ω–∏–π –®–Ü –≤—É–∑—å–∫–∏–π: GPT –≤—ñ–¥–º—ñ–Ω–Ω–∏–π —É —Ç–µ–∫—Å—Ç—ñ, DALL-E —É –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è—Ö, AlphaFold —É –±—ñ–ª–∫–∞—Ö ‚Äî –∞–ª–µ –∂–æ–¥–µ–Ω –Ω–µ –º–æ–∂–µ –≤—Å–µ —Ç—Ä–∏. AGI –æ–∑–Ω–∞—á–∞—î –æ–¥–Ω—É —Å–∏—Å—Ç–µ–º—É —ñ–∑ –∑–∞–≥–∞–ª—å–Ω–∏–º–∏ –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º–∏. –†–æ–∑—Ä–∏–≤ –∑–≤—É–∂—É—î—Ç—å—Å—è –∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏.",
                "links": [
                  {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "–ü–æ—Ç–æ—á–Ω–∏–π –ø—Ä–æ–≥—Ä–µ—Å",
                "desc": "LLM —Ç–µ–ø–µ—Ä –º–æ–∂—É—Ç—å –º—ñ—Ä–∫—É–≤–∞—Ç–∏, –∫–æ–¥—É–≤–∞—Ç–∏, –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏, —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ —Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏. Claude —Ç–∞ GPT-4 –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å –∞–±–æ –ø–µ—Ä–µ–≤–µ—Ä—à—É—é—Ç—å –ª—é–¥—Å—å–∫—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –Ω–∞ –±–∞–≥–∞—Ç—å–æ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –ê–ª–µ –¥–æ—Å—ñ –º–∞—é—Ç—å –ø—Ä–æ–±–ª–µ–º–∏ –∑ –Ω–æ–≤–∏–º –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è–º —Ç–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω–∏–º–∏ –¥—ñ—è–º–∏.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "–ü—Ä–æ–≥–Ω–æ–∑–∏ —Ç–µ—Ä–º—ñ–Ω—ñ–≤",
                "desc": "–°–µ–º –ê–ª—å—Ç–º–∞–Ω (OpenAI): AGI –¥–æ 2025-2027. –î–∞—Ä—ñ–æ –ê–º–æ–¥–µ—ó (Anthropic): \"–ø–æ—Ç—É–∂–Ω–∏–π –®–Ü\" –∑–∞ 2-3 —Ä–æ–∫–∏. –Ø–Ω –õ–µ–∫—É–Ω (Meta): –¥–µ—Å—è—Ç–∏–ª—ñ—Ç—Ç—è, –ø–æ—Ç–æ—á–Ω—ñ –ø—ñ–¥—Ö–æ–¥–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—ñ. –û–ø–∏—Ç—É–≤–∞–Ω–Ω—è –¥–æ—Å–ª—ñ–¥–Ω–∏–∫—ñ–≤ –®–Ü: –º–µ–¥—ñ–∞–Ω–∞ 2040-2060.",
                "links": []
              },
              {
                "text": "–†—ñ–≤–Ω—ñ AGI",
                "desc": "Google DeepMind –∑–∞–ø—Ä–æ–ø–æ–Ω—É–≤–∞–≤ 5 —Ä—ñ–≤–Ω—ñ–≤: –†—ñ–≤–µ–Ω—å 1 (–ü–æ—á–∞—Ç–∫–æ–≤–∏–π) ‚Äî —á–∞—Ç–±–æ—Ç–∏. –†—ñ–≤–µ–Ω—å 2 (–ö–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–∏–π) ‚Äî —Ä—ñ–≤–Ω–∏–π –∫–≤–∞–ª—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–º –¥–æ—Ä–æ—Å–ª–∏–º. –†—ñ–≤–µ–Ω—å 3 (–ï–∫—Å–ø–µ—Ä—Ç). –†—ñ–≤–µ–Ω—å 4 (–í—ñ—Ä—Ç—É–æ–∑). –†—ñ–≤–µ–Ω—å 5 (–ù–∞–¥–ª—é–¥—Å—å–∫–∏–π). –ü–æ—Ç–æ—á–Ω—ñ —Ñ—Ä–æ–Ω—Ç—ñ—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ ‚Äî –†—ñ–≤–µ–Ω—å 1-2.",
                "links": []
              },
              {
                "text": "–í—ñ–¥—Å—É—Ç–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ",
                "desc": "–ü–æ—Ç–æ—á–Ω–∏–π –®–Ü –Ω–µ –º–∞—î: –ø–æ—Å—Ç—ñ–π–Ω–æ—ó –¥–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–æ—ó –ø–∞–º'—è—Ç—ñ, —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è —Å–≤—ñ—Ç—É, –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ü—ñ–ª–µ–π, —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ñ—ñ–∑–∏—á–Ω–æ–≥–æ —Å–≤—ñ—Ç—É, –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –∫—ñ–ª—å–∫–æ—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —Ç–∞ —Ä–æ–±–∞—Å—Ç–Ω–æ–≥–æ –∑–¥–æ—Ä–æ–≤–æ–≥–æ –≥–ª—É–∑–¥—É.",
                "links": [
                  {
                    "title": "–ó–∞–≥–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å —Å–≤—ñ—Ç—É",
                    "href": "world-model.html"
                  }
                ]
              },
              {
                "text": "–®–ª—è—Ö–∏ –¥–æ AGI",
                "desc": "–ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –ø–æ—Ç–æ—á–Ω–∏—Ö –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä, –Ω–æ–≤—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ (state-space, –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª—ñ—á–Ω—ñ), –≥—ñ–±—Ä–∏–¥–Ω—ñ –ø—ñ–¥—Ö–æ–¥–∏ (LLM + –º–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É + –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è) –∞–±–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ –Ω–æ–≤—ñ –ø–∞—Ä–∞–¥–∏–≥–º–∏. –ë—ñ–ª—å—à—ñ—Å—Ç—å –ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ–π —Å—Ç–∞–≤–ª—è—Ç—å –Ω–∞ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –∑ —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—è–º–∏ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏.",
                "links": []
              },
              {
                "text": "–ï–∫–æ–Ω–æ–º—ñ—á–Ω–∏–π –≤–ø–ª–∏–≤",
                "desc": "AGI –º–æ–∂–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑—É–≤–∞—Ç–∏ –±—ñ–ª—å—à—ñ—Å—Ç—å —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ—ó —Ä–æ–±–æ—Ç–∏: –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è, –∞–Ω–∞–ª—ñ–∑, –Ω–∞–ø–∏—Å–∞–Ω–Ω—è, –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è, –¥–∏–∑–∞–π–Ω. –û—Ü—ñ–Ω–∫–∏: 300M+ —Ä–æ–±–æ—á–∏—Ö –º—ñ—Å—Ü—å –ø—ñ–¥ –≤–ø–ª–∏–≤–æ–º. –ê–ª–µ —Ç–∞–∫–æ–∂ —Å—Ç–≤–æ—Ä—é—î –Ω–æ–≤—É –µ–∫–æ–Ω–æ–º—ñ—á–Ω—É —Ü—ñ–Ω–Ω—ñ—Å—Ç—å ‚Äî –ø–∏—Ç–∞–Ω–Ω—è —É —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ.",
                "links": []
              },
              {
                "text": "AGI —Ç–∞ –±–µ–∑–ø–µ–∫–∞",
                "desc": "–ë—ñ–ª—å—à –∑–¥–∞—Ç–Ω–∏–π –®–Ü –≤–∏–º–∞–≥–∞—î —Å–∏–ª—å–Ω—ñ—à–∏—Ö –∑–∞—Ö–æ–¥—ñ–≤ –±–µ–∑–ø–µ–∫–∏. –ß–∏–º –±–ª–∏–∂—á–µ –¥–æ AGI, —Ç–∏–º –∫—Ä–∏—Ç–∏—á–Ω—ñ—à–µ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è ‚Äî –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è —Ç–æ–≥–æ, —â–æ AGI –ø–æ–¥—ñ–ª—è—î –ª—é–¥—Å—å–∫—ñ —Ü—ñ–Ω–Ω–æ—Å—Ç—ñ —Ç–∞ –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –∫–æ–Ω—Ç—Ä–æ–ª—å–æ–≤–∞–Ω–∏–º.",
                "links": [
                  {
                    "title": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü",
                    "href": "ai-safety.html"
                  }
                ]
              },
              {
                "text": "–°–æ—Ü—ñ–∞–ª—å–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è",
                "desc": "AGI –ø–æ—Ä—É—à—É—î –≥–ª–∏–±–æ–∫—ñ –ø–∏—Ç–∞–Ω–Ω—è: —â–æ —Å—Ç–∞–Ω–µ—Ç—å—Å—è –∑ —Ä–æ–±–æ—Ç–æ—é, –æ—Å–≤—ñ—Ç–æ—é, –∫—Ä–µ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—é? –Ø–∫ —Ä–æ–∑–ø–æ–¥—ñ–ª–∏—Ç–∏ –≤–∏–≥–æ–¥–∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ? –•—Ç–æ –∫–æ–Ω—Ç—Ä–æ–ª—é—î? –ú—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—è —á–∏ —Å–ø—ñ–≤–ø—Ä–∞—Ü—è? –¶–µ –Ω–µ –ª–∏—à–µ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ, –∞ —Ü–∏–≤—ñ–ª—ñ–∑–∞—Ü—ñ–π–Ω—ñ –ø–∏—Ç–∞–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –®–Ü",
                    "href": "alignment.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "AGI",
                "def": "Artificial General Intelligence ‚Äî AI that matches human-level cognitive ability across all intellectual domains."
              },
              {
                "term": "Narrow AI",
                "def": "AI that excels at specific tasks but cannot generalize across domains ‚Äî all current AI systems."
              },
              {
                "term": "Transfer Learning",
                "def": "Ability to apply knowledge from one domain to another ‚Äî a key missing piece for true AGI."
              },
              {
                "term": "Frontier Model",
                "def": "The most capable AI models at any given time ‚Äî currently GPT-4, Claude, Gemini."
              }
            ],
            "uk": [
              {
                "term": "AGI",
                "def": "–ó–∞–≥–∞–ª—å–Ω–∏–π —à—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç ‚Äî –®–Ü, —â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω–∏–º –∑–¥—ñ–±–Ω–æ—Å—Ç—è–º –ª—é–¥–∏–Ω–∏ —É –≤—Å—ñ—Ö —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∏—Ö —Å—Ñ–µ—Ä–∞—Ö."
              },
              {
                "term": "–í—É–∑—å–∫–∏–π –®–Ü",
                "def": "–®–Ü, —â–æ –≤—ñ–¥–º—ñ–Ω–Ω–∏–π —É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –∞–ª–µ –Ω–µ –º–æ–∂–µ —É–∑–∞–≥–∞–ª—å–Ω—é–≤–∞—Ç–∏ –º—ñ–∂ –¥–æ–º–µ–Ω–∞–º–∏ ‚Äî –≤—Å—ñ –ø–æ—Ç–æ—á–Ω—ñ —Å–∏—Å—Ç–µ–º–∏ –®–Ü."
              },
              {
                "term": "–¢—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è",
                "def": "–ó–¥–∞—Ç–Ω—ñ—Å—Ç—å –∑–∞—Å—Ç–æ—Å–æ–≤—É–≤–∞—Ç–∏ –∑–Ω–∞–Ω–Ω—è –∑ –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω—É –¥–æ —ñ–Ω—à–æ–≥–æ ‚Äî –∫–ª—é—á–æ–≤–∏–π –≤—ñ–¥—Å—É—Ç–Ω—ñ–π –µ–ª–µ–º–µ–Ω—Ç –¥–ª—è —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ AGI."
              },
              {
                "term": "–§—Ä–æ–Ω—Ç—ñ—Ä–Ω–∞ –º–æ–¥–µ–ª—å",
                "def": "–ù–∞–π–∑–¥–∞—Ç–Ω—ñ—à—ñ –º–æ–¥–µ–ª—ñ –®–Ü —É –±—É–¥—å-—è–∫–∏–π –º–æ–º–µ–Ω—Ç ‚Äî –Ω–∞—Ä–∞–∑—ñ GPT-4, Claude, Gemini."
              }
            ]
          },
          "tips": {
            "en": [
              "Be skeptical of confident AGI timeline predictions ‚Äî the honest answer is \"nobody knows\" and predictions reflect biases",
              "Focus on how current AI capabilities affect your field today rather than waiting for AGI tomorrow",
              "The economic impact of increasingly capable narrow AI may be just as transformative as AGI, and it is already here"
            ],
            "uk": [
              "–ë—É–¥—å—Ç–µ —Å–∫–µ–ø—Ç–∏—á–Ω—ñ –¥–æ –≤–ø–µ–≤–Ω–µ–Ω–∏—Ö –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ —Ç–µ—Ä–º—ñ–Ω—ñ–≤ AGI ‚Äî —á–µ—Å–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å \"–Ω—ñ—Ö—Ç–æ –Ω–µ –∑–Ω–∞—î\", –∞ –ø—Ä–æ–≥–Ω–æ–∑–∏ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—é—Ç—å —É–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è",
              "–ó–æ—Å–µ—Ä–µ–¥—å—Ç–µ—Å—å –Ω–∞ —Ç–æ–º—É, —è–∫ –ø–æ—Ç–æ—á–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –®–Ü –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ –≤–∞—à—É —Å—Ñ–µ—Ä—É —Å—å–æ–≥–æ–¥–Ω—ñ, –∑–∞–º—ñ—Å—Ç—å –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è AGI –∑–∞–≤—Ç—Ä–∞",
              "–ï–∫–æ–Ω–æ–º—ñ—á–Ω–∏–π –≤–ø–ª–∏–≤ –≤—Å–µ –±—ñ–ª—å—à –∑–¥–∞—Ç–Ω–æ–≥–æ –≤—É–∑—å–∫–æ–≥–æ –®–Ü –º–æ–∂–µ –±—É—Ç–∏ –Ω–∞—Å—Ç—ñ–ª—å–∫–∏ –∂ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∏–º —è–∫ AGI, —ñ –≤—ñ–Ω –≤–∂–µ —Ç—É—Ç"
            ]
          },
          "related": [
            "Feed"
          ]
        },
        {
          "slug": "asi",
          "title": {
            "en": "ASI (Artificial Superintelligence)",
            "uk": "ASI (–®—Ç—É—á–Ω–∏–π —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç)"
          },
          "desc": {
            "en": "Beyond human-level AI and its implications.",
            "uk": "–®–Ü, —â–æ –ø–µ—Ä–µ–≤–∏—â—É—î –ª—é–¥—Å—å–∫–∏–π —Ä—ñ–≤–µ–Ω—å, —Ç–∞ –π–æ–≥–æ –Ω–∞—Å–ª—ñ–¥–∫–∏."
          },
          "overview": {
            "en": [
              "Artificial Superintelligence (ASI) refers to AI that surpasses the best human minds in every cognitive domain ‚Äî scientific creativity, social skills, strategic planning, and general wisdom. While AGI matches human ability, ASI exceeds it, potentially by a vast margin. Nick Bostrom's \"Superintelligence\" (2014) formalized the concept and its associated risks.",
              "ASI is the most speculative topic in AI, yet also potentially the most consequential. If AI can improve itself, the gap between human and machine intelligence could grow rapidly. This raises the \"control problem\" ‚Äî how do you ensure an intelligence far greater than your own remains aligned with your values? This question drives much of current AI safety research."
            ],
            "uk": [
              "–®—Ç—É—á–Ω–∏–π —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç (ASI) –æ–∑–Ω–∞—á–∞—î –®–Ü, —â–æ –ø–µ—Ä–µ–≤–∏—â—É—î –Ω–∞–π–∫—Ä–∞—â—ñ –ª—é–¥—Å—å–∫—ñ —É–º–∏ –≤ –∫–æ–∂–Ω—ñ–π –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω—ñ–π —Å—Ñ–µ—Ä—ñ ‚Äî –Ω–∞—É–∫–æ–≤—ñ–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ, —Å–æ—Ü—ñ–∞–ª—å–Ω–∏—Ö –Ω–∞–≤–∏—á–∫–∞—Ö, —Å—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω–æ–º—É –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—ñ —Ç–∞ –∑–∞–≥–∞–ª—å–Ω—ñ–π –º—É–¥—Ä–æ—Å—Ç—ñ. –Ø–∫—â–æ AGI –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –ª—é–¥—Å—å–∫–∏–º –∑–¥—ñ–±–Ω–æ—Å—Ç—è–º, ASI –ø–µ—Ä–µ–≤–∏—â—É—î —ó—Ö, –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ –Ω–∞ –≤–µ–ª–∏—á–µ–∑–Ω—É –≤–µ–ª–∏—á–∏–Ω—É. \"–°—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç\" –ù—ñ–∫–∞ –ë–æ—Å—Ç—Ä–æ–º–∞ (2014) —Ñ–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞–≤ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—é —Ç–∞ –ø–æ–≤'—è–∑–∞–Ω—ñ —Ä–∏–∑–∏–∫–∏.",
              "ASI ‚Äî –Ω–∞–π–±—ñ–ª—å—à —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–∞ —Ç–µ–º–∞ –≤ –®–Ü, –∞–ª–µ –π –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∞. –Ø–∫—â–æ –®–Ü –º–æ–∂–µ –≤–¥–æ—Å–∫–æ–Ω–∞–ª—é–≤–∞—Ç–∏ —Å–µ–±–µ, —Ä–æ–∑—Ä–∏–≤ –º—ñ–∂ –ª—é–¥—Å—å–∫–∏–º —Ç–∞ –º–∞—à–∏–Ω–Ω–∏–º —ñ–Ω—Ç–µ–ª–µ–∫—Ç–æ–º –º–æ–∂–µ —à–≤–∏–¥–∫–æ –∑—Ä–æ—Å—Ç–∞—Ç–∏. –¶–µ –ø–æ—Ä—É—à—É—î \"–ø—Ä–æ–±–ª–µ–º—É –∫–æ–Ω—Ç—Ä–æ–ª—é\" ‚Äî —è–∫ –∑–∞–±–µ–∑–ø–µ—á–∏—Ç–∏, —â–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç –∑–Ω–∞—á–Ω–æ –±—ñ–ª—å—à–∏–π –∑–∞ –≤–∞—à –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –≤–∏—Ä—ñ–≤–Ω—è–Ω–∏–º –∑ –≤–∞—à–∏–º–∏ —Ü—ñ–Ω–Ω–æ—Å—Ç—è–º–∏? –¶–µ –ø–∏—Ç–∞–Ω–Ω—è —Ä—É—Ö–∞—î –±—ñ–ª—å—à—ñ—Å—Ç—å –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å –±–µ–∑–ø–µ–∫–∏ –®–Ü."
            ]
          },
          "details": {
            "en": [
              {
                "text": "What Is Superintelligence",
                "desc": "An intellect that greatly exceeds the cognitive performance of humans in virtually all domains. Not just faster ‚Äî qualitatively superior in understanding, creativity, and strategic thinking. The difference between human and ASI could be like the gap between an ant and a human.",
                "links": [
                  {
                    "title": "AGI",
                    "href": "agi.html"
                  }
                ]
              },
              {
                "text": "Types of Superintelligence",
                "desc": "Speed superintelligence (human-level but much faster), quality superintelligence (qualitatively better reasoning), and collective superintelligence (many AIs coordinating). Current AI shows hints of speed superiority.",
                "links": []
              },
              {
                "text": "Paths to Superintelligence",
                "desc": "Recursive AI self-improvement, whole brain emulation, biological cognitive enhancement, brain-computer interfaces, or AI-AI collaboration at scale. Recursive self-improvement is considered the most likely near-term path.",
                "links": [
                  {
                    "title": "Intelligence Explosion",
                    "href": "intelligence-explosion.html"
                  }
                ]
              },
              {
                "text": "The Control Problem",
                "desc": "The central challenge: how do you control something smarter than you? A superintelligent AI could potentially outwit any containment measures humans design. This is not about malice but about goal misalignment ‚Äî an ASI optimizing for the \"wrong\" objective could be catastrophic.",
                "links": [
                  {
                    "title": "AI Alignment",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "Instrumental Convergence",
                "desc": "Regardless of final goals, a superintelligent agent would likely pursue self-preservation, resource acquisition, and goal preservation as instrumental sub-goals. This makes alignment critical regardless of what specific goal the ASI is given.",
                "links": []
              },
              {
                "text": "Bostrom's Analysis",
                "desc": "Nick Bostrom argued that superintelligence is likely the last invention humanity needs to make ‚Äî it would be capable of solving virtually any solvable problem. But the first superintelligence must be aligned correctly because there may be no opportunity to correct mistakes.",
                "links": []
              },
              {
                "text": "Beneficial Superintelligence",
                "desc": "Properly aligned ASI could solve humanity's greatest challenges: disease, climate change, energy scarcity, scientific breakthroughs. The potential upside is as transformative as the downside risk is existential.",
                "links": []
              },
              {
                "text": "Existential Risk",
                "desc": "ASI is considered one of the top existential risks to humanity. Not because AI would be \"evil\" but because misaligned optimization at superintelligent scale could have irreversible consequences for human civilization.",
                "links": [
                  {
                    "title": "AI Safety",
                    "href": "ai-safety.html"
                  }
                ]
              },
              {
                "text": "Current Relevance",
                "desc": "While ASI seems distant, the research needed to handle it must start now. Alignment techniques, interpretability research, and governance frameworks take time to develop and must be ready before ASI arrives.",
                "links": [
                  {
                    "title": "AI Alignment",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "The Optimist vs Pessimist Debate",
                "desc": "Techno-optimists argue ASI will be humanity's greatest achievement. Pessimists warn it could be our last. Most researchers advocate a middle path: pursue powerful AI carefully, with strong safety research running ahead of capabilities.",
                "links": [
                  {
                    "title": "Techno-Optimists",
                    "href": "accelerationists.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–©–æ —Ç–∞–∫–µ —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç",
                "desc": "–Ü–Ω—Ç–µ–ª–µ–∫—Ç, —â–æ –∑–Ω–∞—á–Ω–æ –ø–µ—Ä–µ–≤–∏—â—É—î –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω—É –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –ª—é–¥–µ–π –ø—Ä–∞–∫—Ç–∏—á–Ω–æ —É –≤—Å—ñ—Ö —Å—Ñ–µ—Ä–∞—Ö. –ù–µ –ø—Ä–æ—Å—Ç–æ —à–≤–∏–¥—à–∏–π ‚Äî —è–∫—ñ—Å–Ω–æ –∫—Ä–∞—â–∏–π —É —Ä–æ–∑—É–º—ñ–Ω–Ω—ñ, –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ —Ç–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω–æ–º—É –º–∏—Å–ª–µ–Ω–Ω—ñ. –†—ñ–∑–Ω–∏—Ü—è –º–æ–∂–µ –±—É—Ç–∏ —è–∫ –º—ñ–∂ –º—É—Ä–∞—Ö–æ—é —Ç–∞ –ª—é–¥–∏–Ω–æ—é.",
                "links": [
                  {
                    "title": "AGI",
                    "href": "agi.html"
                  }
                ]
              },
              {
                "text": "–¢–∏–ø–∏ —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç—É",
                "desc": "–®–≤–∏–¥–∫—ñ—Å–Ω–∏–π —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç (–ª—é–¥—Å—å–∫–∏–π —Ä—ñ–≤–µ–Ω—å, –∞–ª–µ –Ω–∞–±–∞–≥–∞—Ç–æ —à–≤–∏–¥—à–∏–π), —è–∫—ñ—Å–Ω–∏–π (—è–∫—ñ—Å–Ω–æ –∫—Ä–∞—â–µ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è), —Ç–∞ –∫–æ–ª–µ–∫—Ç–∏–≤–Ω–∏–π (–∫–æ–æ—Ä–¥–∏–Ω–∞—Ü—ñ—è –±–∞–≥–∞—Ç—å–æ—Ö –®–Ü). –ü–æ—Ç–æ—á–Ω–∏–π –®–Ü –ø–æ–∫–∞–∑—É—î –Ω–∞—Ç—è–∫–∏ –Ω–∞ —à–≤–∏–¥–∫—ñ—Å–Ω—É –ø–µ—Ä–µ–≤–∞–≥—É.",
                "links": []
              },
              {
                "text": "–®–ª—è—Ö–∏ –¥–æ —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç—É",
                "desc": "–†–µ–∫—É—Ä—Å–∏–≤–Ω–µ —Å–∞–º–æ–≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è –®–Ü, –ø–æ–≤–Ω–∞ –µ–º—É–ª—è—Ü—ñ—è –º–æ–∑–∫—É, –±—ñ–æ–ª–æ–≥—ñ—á–Ω–µ –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è, —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏ –º–æ–∑–æ–∫-–∫–æ–º–ø'—é—Ç–µ—Ä, –∞–±–æ –®–Ü-–®–Ü —Å–ø—ñ–≤–ø—Ä–∞—Ü—è —É –º–∞—Å—à—Ç–∞–±—ñ. –†–µ–∫—É—Ä—Å–∏–≤–Ω–µ —Å–∞–º–æ–≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è –≤–≤–∞–∂–∞—î—Ç—å—Å—è –Ω–∞–π—ñ–º–æ–≤—ñ—Ä–Ω—ñ—à–∏–º –Ω–∞–π–±–ª–∏–∂—á–∏–º —à–ª—è—Ö–æ–º.",
                "links": [
                  {
                    "title": "–í–∏–±—É—Ö —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É",
                    "href": "intelligence-explosion.html"
                  }
                ]
              },
              {
                "text": "–ü—Ä–æ–±–ª–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—é",
                "desc": "–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∏–π –≤–∏–∫–ª–∏–∫: —è–∫ –∫–æ–Ω—Ç—Ä–æ–ª—é–≤–∞—Ç–∏ —â–æ—Å—å —Ä–æ–∑—É–º–Ω—ñ—à–µ –∑–∞ –≤–∞—Å? ASI –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ –º–æ–∂–µ –æ–±—Ö–∏—Ç—Ä–∏—Ç–∏ –±—É–¥—å-—è–∫—ñ –∑–∞—Ö–æ–¥–∏ —Å—Ç—Ä–∏–º—É–≤–∞–Ω–Ω—è. –¶–µ –Ω–µ –ø—Ä–æ –∑–ª–æ–±—É, –∞ –ø—Ä–æ –Ω–µ–≤–∏—Ä—ñ–≤–Ω—è–Ω—ñ—Å—Ç—å —Ü—ñ–ª–µ–π ‚Äî ASI, —â–æ –æ–ø—Ç–∏–º—ñ–∑—É—î \"–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É\" –º–µ—Ç—É, –º–æ–∂–µ –±—É—Ç–∏ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ñ—á–Ω–∏–º.",
                "links": [
                  {
                    "title": "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –®–Ü",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "–Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü—ñ—è",
                "desc": "–ù–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –∫—ñ–Ω—Ü–µ–≤–∏—Ö —Ü—ñ–ª–µ–π, —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∏–π –∞–≥–µ–Ω—Ç –π–º–æ–≤—ñ—Ä–Ω–æ –±—É–¥–µ –ø–µ—Ä–µ—Å–ª—ñ–¥—É–≤–∞—Ç–∏ —Å–∞–º–æ–∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è, –Ω–∞–∫–æ–ø–∏—á–µ–Ω–Ω—è —Ä–µ—Å—É—Ä—Å—ñ–≤ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ü—ñ–ª–µ–π —è–∫ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –ø—ñ–¥-—Ü—ñ–ª—ñ. –¶–µ —Ä–æ–±–∏—Ç—å –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –∫—Ä–∏—Ç–∏—á–Ω–∏–º –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –º–µ—Ç–∏.",
                "links": []
              },
              {
                "text": "–ê–Ω–∞–ª—ñ–∑ –ë–æ—Å—Ç—Ä–æ–º–∞",
                "desc": "–ù—ñ–∫ –ë–æ—Å—Ç—Ä–æ–º –∞—Ä–≥—É–º–µ–Ω—Ç—É–≤–∞–≤, —â–æ —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç –π–º–æ–≤—ñ—Ä–Ω–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π –≤–∏–Ω–∞—Ö—ñ–¥ –ª—é–¥—Å—Ç–≤–∞ ‚Äî –≤—ñ–Ω –∑–¥–∞—Ç–Ω–∏–π —Ä–æ–∑–≤'—è–∑–∞—Ç–∏ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ –±—É–¥—å-—è–∫—É –ø—Ä–æ–±–ª–µ–º—É. –ê–ª–µ –ø–µ—Ä—à–∏–π —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –≤–∏—Ä—ñ–≤–Ω—è–Ω–∏–π –ø—Ä–∞–≤–∏–ª—å–Ω–æ, –±–æ –¥—Ä—É–≥–æ–≥–æ —à–∞–Ω—Å—É –º–æ–∂–µ –Ω–µ –±—É—Ç–∏.",
                "links": []
              },
              {
                "text": "–ö–æ—Ä–∏—Å–Ω–∏–π —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç",
                "desc": "–ü—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∏—Ä—ñ–≤–Ω—è–Ω–∏–π ASI –º–æ–∂–µ —Ä–æ–∑–≤'—è–∑–∞—Ç–∏ –Ω–∞–π–±—ñ–ª—å—à—ñ –≤–∏–∫–ª–∏–∫–∏ –ª—é–¥—Å—Ç–≤–∞: —Ö–≤–æ—Ä–æ–±–∏, –∑–º—ñ–Ω—É –∫–ª—ñ–º–∞—Ç—É, –¥–µ—Ñ—ñ—Ü–∏—Ç –µ–Ω–µ—Ä–≥—ñ—ó, –Ω–∞—É–∫–æ–≤—ñ –≤—ñ–¥–∫—Ä–∏—Ç—Ç—è. –ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –ø–æ–∑–∏—Ç–∏–≤ –Ω–∞—Å—Ç—ñ–ª—å–∫–∏ –∂ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∏–π, —è–∫ —Ä–∏–∑–∏–∫ ‚Äî –µ–∫–∑–∏—Å—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π.",
                "links": []
              },
              {
                "text": "–ï–∫–∑–∏—Å—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π —Ä–∏–∑–∏–∫",
                "desc": "ASI –≤–≤–∞–∂–∞—î—Ç—å—Å—è –æ–¥–Ω–∏–º –∑ –≥–æ–ª–æ–≤–Ω–∏—Ö –µ–∫–∑–∏—Å—Ç–µ–Ω—Ü—ñ–π–Ω–∏—Ö —Ä–∏–∑–∏–∫—ñ–≤ –ª—é–¥—Å—Ç–≤–∞. –ù–µ —Ç–æ–º—É —â–æ –®–Ü –±—É–¥–µ \"–∑–ª–∏–º\", –∞ —Ç–æ–º—É —â–æ –Ω–µ–≤–∏—Ä—ñ–≤–Ω—è–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –Ω–∞ —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–º—É —Ä—ñ–≤–Ω—ñ –º–æ–∂–µ –º–∞—Ç–∏ –Ω–µ–∑–≤–æ—Ä–æ—Ç–Ω—ñ –Ω–∞—Å–ª—ñ–¥–∫–∏ –¥–ª—è —Ü–∏–≤—ñ–ª—ñ–∑–∞—Ü—ñ—ó.",
                "links": [
                  {
                    "title": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü",
                    "href": "ai-safety.html"
                  }
                ]
              },
              {
                "text": "–ü–æ—Ç–æ—á–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å",
                "desc": "–•–æ—á–∞ ASI –∑–¥–∞—î—Ç—å—Å—è –¥–∞–ª–µ–∫–∏–º, –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –¥–ª—è –π–æ–≥–æ –æ–±—Ä–æ–±–∫–∏ –ø–æ–≤–∏–Ω–Ω—ñ –ø–æ—á–∏–Ω–∞—Ç–∏—Å—è –∑–∞—Ä–∞–∑. –¢–µ—Ö–Ω—ñ–∫–∏ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è, –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–æ—Å—Ç—ñ —Ç–∞ —Ä–∞–º–∫–∏ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –ø–æ—Ç—Ä–µ–±—É—é—Ç—å —á–∞—Å—É —Ç–∞ –ø–æ–≤–∏–Ω–Ω—ñ –±—É—Ç–∏ –≥–æ—Ç–æ–≤—ñ –¥–æ –ø—Ä–∏—Ö–æ–¥—É ASI.",
                "links": [
                  {
                    "title": "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –®–Ü",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "–î–µ–±–∞—Ç–∏ –æ–ø—Ç–∏–º—ñ—Å—Ç—ñ–≤ —Ç–∞ –ø–µ—Å–∏–º—ñ—Å—Ç—ñ–≤",
                "desc": "–¢–µ—Ö–Ω–æ-–æ–ø—Ç–∏–º—ñ—Å—Ç–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—É—é—Ç—å —â–æ ASI –±—É–¥–µ –Ω–∞–π–±—ñ–ª—å—à–∏–º –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è–º –ª—é–¥—Å—Ç–≤–∞. –ü–µ—Å–∏–º—ñ—Å—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–∂–∞—é—Ç—å —â–æ –º–æ–∂–µ –±—É—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ–º. –ë—ñ–ª—å—à—ñ—Å—Ç—å –¥–æ—Å–ª—ñ–¥–Ω–∏–∫—ñ–≤ –∑–∞ —Å–µ—Ä–µ–¥–Ω—ñ–π —à–ª—è—Ö: –ø–æ—Ç—É–∂–Ω–∏–π –®–Ü –æ–±–µ—Ä–µ–∂–Ω–æ, –∑ –±–µ–∑–ø–µ–∫–æ—é –ø–æ–ø–µ—Ä–µ–¥—É –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π.",
                "links": [
                  {
                    "title": "–¢–µ—Ö–Ω–æ-–æ–ø—Ç–∏–º—ñ—Å—Ç–∏",
                    "href": "accelerationists.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "ASI",
                "def": "Artificial Superintelligence ‚Äî AI that vastly exceeds the best human cognitive abilities across all domains."
              },
              {
                "term": "Control Problem",
                "def": "The challenge of ensuring a superintelligent AI remains aligned with human values and under human control."
              },
              {
                "term": "Instrumental Convergence",
                "def": "The tendency for any sufficiently intelligent agent to pursue self-preservation and resource acquisition regardless of its final goals."
              },
              {
                "term": "Existential Risk",
                "def": "Risk of human extinction or irreversible civilizational collapse ‚Äî ASI is considered a primary source."
              }
            ],
            "uk": [
              {
                "term": "ASI",
                "def": "–®—Ç—É—á–Ω–∏–π —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç ‚Äî –®–Ü, —â–æ –∑–Ω–∞—á–Ω–æ –ø–µ—Ä–µ–≤–∏—â—É—î –Ω–∞–π–∫—Ä–∞—â—ñ –ª—é–¥—Å—å–∫—ñ –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω—ñ –∑–¥—ñ–±–Ω–æ—Å—Ç—ñ —É –≤—Å—ñ—Ö —Å—Ñ–µ—Ä–∞—Ö."
              },
              {
                "term": "–ü—Ä–æ–±–ª–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—é",
                "def": "–í–∏–∫–ª–∏–∫ –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è —Ç–æ–≥–æ, —â–æ —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∏–π –®–Ü –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –≤–∏—Ä—ñ–≤–Ω—è–Ω–∏–º –∑ –ª—é–¥—Å—å–∫–∏–º–∏ —Ü—ñ–Ω–Ω–æ—Å—Ç—è–º–∏ —Ç–∞ –ø—ñ–¥ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º."
              },
              {
                "term": "–Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü—ñ—è",
                "def": "–¢–µ–Ω–¥–µ–Ω—Ü—ñ—è –±—É–¥—å-—è–∫–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –ø–µ—Ä–µ—Å–ª—ñ–¥—É–≤–∞—Ç–∏ —Å–∞–º–æ–∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∞ —Ä–µ—Å—É—Ä—Å–∏ –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –∫—ñ–Ω—Ü–µ–≤–∏—Ö —Ü—ñ–ª–µ–π."
              },
              {
                "term": "–ï–∫–∑–∏—Å—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π —Ä–∏–∑–∏–∫",
                "def": "–†–∏–∑–∏–∫ –≤–∏–º–∏—Ä–∞–Ω–Ω—è –ª—é–¥—Å—Ç–≤–∞ –∞–±–æ –Ω–µ–∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ —Ü–∏–≤—ñ–ª—ñ–∑–∞—Ü—ñ–π–Ω–æ–≥–æ –∫–æ–ª–∞–ø—Å—É ‚Äî ASI –≤–≤–∞–∂–∞—î—Ç—å—Å—è –æ—Å–Ω–æ–≤–Ω–∏–º –¥–∂–µ—Ä–µ–ª–æ–º."
              }
            ]
          },
          "tips": {
            "en": [
              "Read Nick Bostrom's \"Superintelligence\" and Stuart Russell's \"Human Compatible\" for the foundational arguments on ASI risk",
              "ASI discussions are speculative but the principles inform practical AI safety work happening today",
              "The control problem applies at all capability levels ‚Äî solving it for narrow AI helps prepare for more general systems"
            ],
            "uk": [
              "–ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ \"–°—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç\" –ë–æ—Å—Ç—Ä–æ–º–∞ —Ç–∞ \"Human Compatible\" –†–∞—Å—Å–µ–ª–∞ –¥–ª—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –∞—Ä–≥—É–º–µ–Ω—Ç—ñ–≤ —â–æ–¥–æ —Ä–∏–∑–∏–∫—ñ–≤ ASI",
              "–î–∏—Å–∫—É—Å—ñ—ó –ø—Ä–æ ASI —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω—ñ, –∞–ª–µ –ø—Ä–∏–Ω—Ü–∏–ø–∏ —ñ–Ω—Ñ–æ—Ä–º—É—é—Ç—å –ø—Ä–∞–∫—Ç–∏—á–Ω—É —Ä–æ–±–æ—Ç—É –∑ –±–µ–∑–ø–µ–∫–∏ –®–Ü —Å—å–æ–≥–æ–¥–Ω—ñ",
              "–ü—Ä–æ–±–ª–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—é –∑–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –Ω–∞ –≤—Å—ñ—Ö —Ä—ñ–≤–Ω—è—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π ‚Äî —ó—ó –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –¥–ª—è –≤—É–∑—å–∫–æ–≥–æ –®–Ü –≥–æ—Ç—É—î –¥–æ –±—ñ–ª—å—à –∑–∞–≥–∞–ª—å–Ω–∏—Ö —Å–∏—Å—Ç–µ–º"
            ]
          },
          "related": [
            "Feed",
            "Video Content"
          ]
        },
        {
          "slug": "singularity",
          "title": {
            "en": "Technological Singularity",
            "uk": "–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∞ —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ñ—Å—Ç—å"
          },
          "desc": {
            "en": "The hypothetical point where AI improvement becomes self-sustaining.",
            "uk": "–ì—ñ–ø–æ—Ç–µ—Ç–∏—á–Ω–∞ —Ç–æ—á–∫–∞, –¥–µ –≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è –®–Ü —Å—Ç–∞—î —Å–∞–º–æ–ø—ñ–¥—Ç—Ä–∏–º—É—é—á–∏–º."
          },
          "overview": {
            "en": [
              "The Technological Singularity is the hypothetical future point where technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization. The concept, popularized by Vernor Vinge (1993) and Ray Kurzweil (2005), predicts that once AI surpasses human intelligence, it will rapidly improve itself, creating a feedback loop of ever-accelerating advancement.",
              "The singularity is often described as an \"event horizon\" beyond which predictions become impossible. Just as you cannot see past a black hole's event horizon, we cannot predict what a post-singularity world looks like. Whether you view it as utopia, catastrophe, or fantasy, the singularity concept forces us to think about what happens when the pace of change exceeds our ability to adapt."
            ],
            "uk": [
              "–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∞ —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ñ—Å—Ç—å ‚Äî —Ü–µ –≥—ñ–ø–æ—Ç–µ—Ç–∏—á–Ω–∞ –º–∞–π–±—É—Ç–Ω—è —Ç–æ—á–∫–∞, –¥–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–µ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è —Å—Ç–∞—î –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª—å–æ–≤–∞–Ω–∏–º —Ç–∞ –Ω–µ–∑–≤–æ—Ä–æ—Ç–Ω–∏–º, –ø—Ä–∏–∑–≤–æ–¥—è—á–∏ –¥–æ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω–∏—Ö –∑–º—ñ–Ω —Ü–∏–≤—ñ–ª—ñ–∑–∞—Ü—ñ—ó. –ö–æ–Ω—Ü–µ–ø—Ü—ñ—è, –ø–æ–ø—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–∞ –í–µ—Ä–Ω–æ—Ä–æ–º –í—ñ–Ω–¥–∂–µ–º (1993) —Ç–∞ –†–µ—î–º –ö—É—Ä—Ü–≤–µ–π–ª–æ–º (2005), –ø–µ—Ä–µ–¥–±–∞—á–∞—î —â–æ –∫–æ–ª–∏ –®–Ü –ø–µ—Ä–µ–≤–µ—Ä—à–∏—Ç—å –ª—é–¥—Å—å–∫–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç, –≤—ñ–Ω —à–≤–∏–¥–∫–æ –≤–¥–æ—Å–∫–æ–Ω–∞–ª–∏—Ç—å —Å–µ–±–µ, —Å—Ç–≤–æ—Ä—é—é—á–∏ –ø–µ—Ç–ª—é –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –∑–≤'—è–∑–∫—É –≤—Å–µ –±—ñ–ª—å—à –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–æ–≥–æ —Ä–æ–∑–≤–∏—Ç–∫—É.",
              "–°–∏–Ω–≥—É–ª—è—Ä–Ω—ñ—Å—Ç—å —á–∞—Å—Ç–æ –æ–ø–∏—Å—É—é—Ç—å —è–∫ \"–≥–æ—Ä–∏–∑–æ–Ω—Ç –ø–æ–¥—ñ–π\", –∑–∞ —è–∫–∏–º –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è —Å—Ç–∞—é—Ç—å –Ω–µ–º–æ–∂–ª–∏–≤–∏–º–∏. –Ø–∫ –Ω–µ –º–æ–∂–Ω–∞ –ø–æ–±–∞—á–∏—Ç–∏ –∑–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–º –ø–æ–¥—ñ–π —á–æ—Ä–Ω–æ—ó –¥—ñ—Ä–∏, –º–∏ –Ω–µ –º–æ–∂–µ–º–æ –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ –ø–æ—Å—Ç-—Å–∏–Ω–≥—É–ª—è—Ä–Ω–∏–π —Å–≤—ñ—Ç. –ù–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ç–æ–≥–æ, —á–∏ –≤–∏ –±–∞—á–∏—Ç–µ —Ü–µ —è–∫ —É—Ç–æ–ø—ñ—é, –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—É —á–∏ —Ñ–∞–Ω—Ç–∞–∑—ñ—é, –∫–æ–Ω—Ü–µ–ø—Ü—ñ—è –∑–º—É—à—É—î –¥—É–º–∞—Ç–∏ —â–æ —Å—Ç–∞–Ω–µ—Ç—å—Å—è –∫–æ–ª–∏ —Ç–µ–º–ø –∑–º—ñ–Ω –ø–µ—Ä–µ–≤–∏—â–∏—Ç—å –Ω–∞—à—É –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –∞–¥–∞–ø—Ç—É–≤–∞—Ç–∏—Å—è."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Vinge's Prediction",
                "desc": "Vernor Vinge (1993) argued that within 30 years, we would create superhuman intelligence, and shortly after, the human era would end. He identified AI, brain-computer interfaces, and biological enhancement as possible paths.",
                "links": [
                  {
                    "title": "AGI",
                    "href": "agi.html"
                  }
                ]
              },
              {
                "text": "Kurzweil's Law of Accelerating Returns",
                "desc": "Ray Kurzweil predicts the singularity around 2045, based on exponential trends in computing power. Each generation of technology enables the next to develop faster. AI development follows this exponential curve.",
                "links": []
              },
              {
                "text": "Recursive Self-Improvement",
                "desc": "The core mechanism: AI improves its own design, creating a smarter AI, which improves itself further. Each cycle produces a more capable system in less time. This positive feedback loop drives the \"explosion\" of intelligence.",
                "links": [
                  {
                    "title": "Intelligence Explosion",
                    "href": "intelligence-explosion.html"
                  }
                ]
              },
              {
                "text": "The Event Horizon Analogy",
                "desc": "Just as physics breaks down at a black hole's event horizon, our ability to predict breaks down at the singularity. The post-singularity world may be as incomprehensible to us as modern technology would be to a medieval person.",
                "links": []
              },
              {
                "text": "Pre-Singularity Signs",
                "desc": "Accelerating pace of AI breakthroughs, AI-designed AI architectures (NAS), AI writing code that improves AI, shortening time between capability milestones. Some argue we are already in the early stages of the transition.",
                "links": []
              },
              {
                "text": "Post-Singularity Scenarios",
                "desc": "Utopian: AI solves all problems, abundance for all, immortality. Dystopian: human irrelevance, loss of control, extinction. Mixed: radical transformation with both benefits and challenges. The outcome depends on alignment and governance.",
                "links": [
                  {
                    "title": "AI Alignment",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "Hard vs Soft Singularity",
                "desc": "Hard singularity: sudden, dramatic transition driven by recursive self-improvement. Soft singularity: gradual acceleration of AI capabilities over decades, with humans adapting incrementally. Current trends suggest a softer transition.",
                "links": []
              },
              {
                "text": "Criticism of Singularity Theory",
                "desc": "Skeptics argue: exponential growth always hits limits, self-improvement may have diminishing returns, intelligence may not be computable above a threshold, and social/economic factors constrain pure technological acceleration.",
                "links": []
              },
              {
                "text": "Economic Singularity",
                "desc": "Even without ASI, an \"economic singularity\" could occur when AI automates most cognitive work. The resulting economic transformation could be as disruptive as the original concept, regardless of whether true superintelligence emerges.",
                "links": []
              },
              {
                "text": "Preparing for Radical Change",
                "desc": "Regardless of whether the \"singularity\" happens as predicted, rapid AI advancement requires preparation: adaptable education systems, social safety nets, governance frameworks, and ongoing safety research.",
                "links": [
                  {
                    "title": "AI Safety",
                    "href": "ai-safety.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–ü—Ä–æ–≥–Ω–æ–∑ –í—ñ–Ω–¥–∂–∞",
                "desc": "–í–µ—Ä–Ω–æ—Ä –í—ñ–Ω–¥–∂ (1993) –∞—Ä–≥—É–º–µ–Ω—Ç—É–≤–∞–≤ —â–æ –∑–∞ 30 —Ä–æ–∫—ñ–≤ –º–∏ —Å—Ç–≤–æ—Ä–∏–º–æ –Ω–∞–¥–ª—é–¥—Å—å–∫–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç, —ñ –Ω–µ–≤–¥–æ–≤–∑—ñ –ø—ñ—Å–ª—è —Ü—å–æ–≥–æ –ª—é–¥—Å—å–∫–∞ –µ—Ä–∞ –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è. –í—ñ–Ω –≤–∏–∑–Ω–∞—á–∏–≤ –®–Ü, —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏ –º–æ–∑–æ–∫-–∫–æ–º–ø'—é—Ç–µ—Ä —Ç–∞ –±—ñ–æ–ª–æ–≥—ñ—á–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —è–∫ –º–æ–∂–ª–∏–≤—ñ —à–ª—è—Ö–∏.",
                "links": [
                  {
                    "title": "AGI",
                    "href": "agi.html"
                  }
                ]
              },
              {
                "text": "–ó–∞–∫–æ–Ω –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –ö—É—Ä—Ü–≤–µ–π–ª–∞",
                "desc": "–†–µ–π –ö—É—Ä—Ü–≤–µ–π–ª –ø—Ä–æ–≥–Ω–æ–∑—É—î —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ñ—Å—Ç—å –±–ª–∏–∑—å–∫–æ 2045, –±–∞–∑—É—é—á–∏—Å—å –Ω–∞ –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–∏—Ö —Ç—Ä–µ–Ω–¥–∞—Ö –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–æ—ó –ø–æ—Ç—É–∂–Ω–æ—Å—Ç—ñ. –ö–æ–∂–Ω–µ –ø–æ–∫–æ–ª—ñ–Ω–Ω—è —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π –¥–æ–∑–≤–æ–ª—è—î –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É —Ä–æ–∑–≤–∏–≤–∞—Ç–∏—Å—è —à–≤–∏–¥—à–µ.",
                "links": []
              },
              {
                "text": "–†–µ–∫—É—Ä—Å–∏–≤–Ω–µ —Å–∞–º–æ–≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è",
                "desc": "–û—Å–Ω–æ–≤–Ω–∏–π –º–µ—Ö–∞–Ω—ñ–∑–º: –®–Ü –ø–æ–∫—Ä–∞—â—É—î –≤–ª–∞—Å–Ω–∏–π –¥–∏–∑–∞–π–Ω, —Å—Ç–≤–æ—Ä—é—é—á–∏ —Ä–æ–∑—É–º–Ω—ñ—à–∏–π –®–Ü, —è–∫–∏–π –ø–æ–∫—Ä–∞—â—É—î —Å–µ–±–µ –¥–∞–ª—ñ. –ö–æ–∂–µ–Ω —Ü–∏–∫–ª –≤–∏—Ä–æ–±–ª—è—î –±—ñ–ª—å—à –∑–¥–∞—Ç–Ω—É —Å–∏—Å—Ç–µ–º—É –∑–∞ –º–µ–Ω—à–∏–π —á–∞—Å. –¶—è –ø–æ–∑–∏—Ç–∏–≤–Ω–∞ –ø–µ—Ç–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –∑–≤'—è–∑–∫—É —Ä—É—Ö–∞—î \"–≤–∏–±—É—Ö\" —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.",
                "links": [
                  {
                    "title": "–í–∏–±—É—Ö —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É",
                    "href": "intelligence-explosion.html"
                  }
                ]
              },
              {
                "text": "–ê–Ω–∞–ª–æ–≥—ñ—è –≥–æ—Ä–∏–∑–æ–Ω—Ç—É –ø–æ–¥—ñ–π",
                "desc": "–Ø–∫ —Ñ—ñ–∑–∏–∫–∞ —Ä—É–π–Ω—É—î—Ç—å—Å—è –Ω–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ñ –ø–æ–¥—ñ–π —á–æ—Ä–Ω–æ—ó –¥—ñ—Ä–∏, –Ω–∞—à–∞ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –ø–µ—Ä–µ–¥–±–∞—á–∞—Ç–∏ —Ä—É–π–Ω—É—î—Ç—å—Å—è –Ω–∞ —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ—Å—Ç—ñ. –ü–æ—Å—Ç-—Å–∏–Ω–≥—É–ª—è—Ä–Ω–∏–π —Å–≤—ñ—Ç –º–æ–∂–µ –±—É—Ç–∏ –Ω–∞—Å—Ç—ñ–ª—å–∫–∏ –∂ –Ω–µ–∑—Ä–æ–∑—É–º—ñ–ª–∏–º –Ω–∞–º, —è–∫ —Å—É—á–∞—Å–Ω—ñ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó —Å–µ—Ä–µ–¥–Ω—å–æ–≤—ñ—á–Ω—ñ–π –ª—é–¥–∏–Ω—ñ.",
                "links": []
              },
              {
                "text": "–û–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–¥ —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ñ—Å—Ç—é",
                "desc": "–ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è —Ç–µ–º–ø—É –ø—Ä–æ—Ä–∏–≤—ñ–≤ –®–Ü, –®–Ü-—Ä–æ–∑—Ä–æ–±–ª–µ–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –®–Ü (NAS), –®–Ü –ø–∏—à–µ –∫–æ–¥ —â–æ –ø–æ–∫—Ä–∞—â—É—î –®–Ü, —Å–∫–æ—Ä–æ—á–µ–Ω–Ω—è —á–∞—Å—É –º—ñ–∂ –≤—ñ—Ö–∞–º–∏ –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π. –î–µ—è–∫—ñ –∞—Ä–≥—É–º–µ–Ω—Ç—É—é—Ç—å —â–æ –º–∏ –≤–∂–µ –Ω–∞ —Ä–∞–Ω–Ω—ñ—Ö —Å—Ç–∞–¥—ñ—è—Ö –ø–µ—Ä–µ—Ö–æ–¥—É.",
                "links": []
              },
              {
                "text": "–ü–æ—Å—Ç-—Å–∏–Ω–≥—É–ª—è—Ä–Ω—ñ —Å—Ü–µ–Ω–∞—Ä—ñ—ó",
                "desc": "–£—Ç–æ–ø—ñ—á–Ω–∏–π: –®–Ü –≤–∏—Ä—ñ—à—É—î –≤—Å—ñ –ø—Ä–æ–±–ª–µ–º–∏, –¥–æ—Å—Ç–∞—Ç–æ–∫ –¥–ª—è –≤—Å—ñ—Ö, –±–µ–∑—Å–º–µ—Ä—Ç—è. –ê–Ω—Ç–∏—É—Ç–æ–ø—ñ—á–Ω–∏–π: –ª—é–¥—Å—å–∫–∞ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å, –≤—Ç—Ä–∞—Ç–∞ –∫–æ–Ω—Ç—Ä–æ–ª—é. –ó–º—ñ—à–∞–Ω–∏–π: —Ä–∞–¥–∏–∫–∞–ª—å–Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è –∑ –≤–∏–≥–æ–¥–∞–º–∏ —Ç–∞ –≤–∏–∫–ª–∏–∫–∞–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è —Ç–∞ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –®–Ü",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "–ñ–æ—Ä—Å—Ç–∫–∞ —Ç–∞ –º'—è–∫–∞ —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ñ—Å—Ç—å",
                "desc": "–ñ–æ—Ä—Å—Ç–∫–∞: —Ä–∞–ø—Ç–æ–≤–∏–π, –¥—Ä–∞–º–∞—Ç–∏—á–Ω–∏–π –ø–µ—Ä–µ—Ö—ñ–¥ —á–µ—Ä–µ–∑ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–µ —Å–∞–º–æ–≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è. –ú'—è–∫–∞: –ø–æ—Å—Ç—É–ø–æ–≤–µ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –®–Ü –ø—Ä–æ—Ç—è–≥–æ–º –¥–µ—Å—è—Ç–∏–ª—ñ—Ç—å –∑ –ø–æ—Å—Ç—É–ø–æ–≤–æ—é –∞–¥–∞–ø—Ç–∞—Ü—ñ—î—é –ª—é–¥–µ–π. –ü–æ—Ç–æ—á–Ω—ñ —Ç—Ä–µ–Ω–¥–∏ –≤–∫–∞–∑—É—é—Ç—å –Ω–∞ –º'—è–∫—à–∏–π –ø–µ—Ä–µ—Ö—ñ–¥.",
                "links": []
              },
              {
                "text": "–ö—Ä–∏—Ç–∏–∫–∞ —Ç–µ–æ—Ä—ñ—ó —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ—Å—Ç—ñ",
                "desc": "–°–∫–µ–ø—Ç–∏–∫–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—É—é—Ç—å: –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–µ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è –∑–∞–≤–∂–¥–∏ –¥–æ—Å—è–≥–∞—î –ª—ñ–º—ñ—Ç—ñ–≤, —Å–∞–º–æ–≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è –º–æ–∂–µ –º–∞—Ç–∏ —Å–ø–∞–¥–Ω—É –≤—ñ–¥–¥–∞—á—É, —ñ–Ω—Ç–µ–ª–µ–∫—Ç –º–æ–∂–µ –±—É—Ç–∏ –Ω–µ–æ–±—á–∏—Å–ª—é–≤–∞–Ω–∏–º –≤–∏—â–µ –ø–æ—Ä–æ–≥—É, —Å–æ—Ü—ñ–∞–ª—å–Ω–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω—ñ —Ñ–∞–∫—Ç–æ—Ä–∏ –æ–±–º–µ–∂—É—é—Ç—å –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è.",
                "links": []
              },
              {
                "text": "–ï–∫–æ–Ω–æ–º—ñ—á–Ω–∞ —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ñ—Å—Ç—å",
                "desc": "–ù–∞–≤—ñ—Ç—å –±–µ–∑ ASI, \"–µ–∫–æ–Ω–æ–º—ñ—á–Ω–∞ —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ñ—Å—Ç—å\" –º–æ–∂–µ –Ω–∞—Å—Ç–∞—Ç–∏ –∫–æ–ª–∏ –®–Ü –∞–≤—Ç–æ–º–∞—Ç–∏–∑—É—î –±—ñ–ª—å—à—ñ—Å—Ç—å –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω–æ—ó —Ä–æ–±–æ—Ç–∏. –†–µ–∑—É–ª—å—Ç—É—é—á–∞ –µ–∫–æ–Ω–æ–º—ñ—á–Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è –º–æ–∂–µ –±—É—Ç–∏ –Ω–∞—Å—Ç—ñ–ª—å–∫–∏ –∂ —Ä—É–π–Ω—ñ–≤–Ω–æ—é –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –ø–æ—è–≤–∏ —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.",
                "links": []
              },
              {
                "text": "–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ —Ä–∞–¥–∏–∫–∞–ª—å–Ω–∏—Ö –∑–º—ñ–Ω",
                "desc": "–ù–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ç–æ–≥–æ —á–∏ \"—Å–∏–Ω–≥—É–ª—è—Ä–Ω—ñ—Å—Ç—å\" –Ω–∞—Å—Ç–∞–Ω–µ —è–∫ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–æ, —à–≤–∏–¥–∫–∏–π —Ä–æ–∑–≤–∏—Ç–æ–∫ –®–Ü –≤–∏–º–∞–≥–∞—î –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏: –∞–¥–∞–ø—Ç–∏–≤–Ω—ñ —Å–∏—Å—Ç–µ–º–∏ –æ—Å–≤—ñ—Ç–∏, —Å–æ—Ü—ñ–∞–ª—å–Ω–∏–π –∑–∞—Ö–∏—Å—Ç, —Ä–∞–º–∫–∏ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —Ç–∞ –ø–æ—Å—Ç—ñ–π–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –±–µ–∑–ø–µ–∫–∏.",
                "links": [
                  {
                    "title": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü",
                    "href": "ai-safety.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Technological Singularity",
                "def": "Hypothetical point where AI self-improvement becomes uncontrollable, leading to unpredictable civilizational changes."
              },
              {
                "term": "Law of Accelerating Returns",
                "def": "Kurzweil's observation that the rate of technological change itself accelerates exponentially over time."
              },
              {
                "term": "Event Horizon",
                "def": "The point beyond which predictions become impossible ‚Äî borrowed from black hole physics as a metaphor for the singularity."
              },
              {
                "term": "Recursive Self-Improvement",
                "def": "AI improving its own design, creating a positive feedback loop of ever-increasing intelligence."
              }
            ],
            "uk": [
              {
                "term": "–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∞ —Å–∏–Ω–≥—É–ª—è—Ä–Ω—ñ—Å—Ç—å",
                "def": "–ì—ñ–ø–æ—Ç–µ—Ç–∏—á–Ω–∞ —Ç–æ—á–∫–∞ –¥–µ —Å–∞–º–æ–≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è –®–Ü —Å—Ç–∞—î –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª—å–æ–≤–∞–Ω–∏–º, –≤–µ–¥—É—á–∏ –¥–æ –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω–∏—Ö —Ü–∏–≤—ñ–ª—ñ–∑–∞—Ü—ñ–π–Ω–∏—Ö –∑–º—ñ–Ω."
              },
              {
                "term": "–ó–∞–∫–æ–Ω –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è",
                "def": "–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è –ö—É—Ä—Ü–≤–µ–π–ª–∞ —â–æ —Ç–µ–º–ø —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∏—Ö –∑–º—ñ–Ω —Å–∞–º –ø—Ä–∏—Å–∫–æ—Ä—é—î—Ç—å—Å—è –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–æ –∑ —á–∞—Å–æ–º."
              },
              {
                "term": "–ì–æ—Ä–∏–∑–æ–Ω—Ç –ø–æ–¥—ñ–π",
                "def": "–¢–æ—á–∫–∞ –∑–∞ —è–∫–æ—é –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è —Å—Ç–∞—é—Ç—å –Ω–µ–º–æ–∂–ª–∏–≤–∏–º–∏ ‚Äî –º–µ—Ç–∞—Ñ–æ—Ä–∞ –∑ —Ñ—ñ–∑–∏–∫–∏ —á–æ—Ä–Ω–∏—Ö –¥—ñ—Ä –¥–ª—è —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ—Å—Ç—ñ."
              },
              {
                "term": "–†–µ–∫—É—Ä—Å–∏–≤–Ω–µ —Å–∞–º–æ–≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è",
                "def": "–®–Ü –ø–æ–∫—Ä–∞—â—É—î –≤–ª–∞—Å–Ω–∏–π –¥–∏–∑–∞–π–Ω, —Å—Ç–≤–æ—Ä—é—é—á–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω—É –ø–µ—Ç–ª—é –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –∑–≤'—è–∑–∫—É –∑—Ä–æ—Å—Ç–∞—é—á–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É."
              }
            ]
          },
          "tips": {
            "en": [
              "The singularity is a useful thought experiment even if you are skeptical ‚Äî it forces you to think about accelerating change",
              "Focus on the \"economic singularity\" of AI automation rather than speculative superintelligence ‚Äî it is more actionable and imminent",
              "Read both optimists (Kurzweil) and skeptics (Gary Marcus) to form a balanced view of AI trajectory"
            ],
            "uk": [
              "–°–∏–Ω–≥—É–ª—è—Ä–Ω—ñ—Å—Ç—å ‚Äî –∫–æ—Ä–∏—Å–Ω–∏–π –º–∏—Å–ª–µ–Ω–Ω—î–≤–∏–π –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –≤–∏ —Å–∫–µ–ø—Ç–∏–∫ ‚Äî –≤–æ–Ω–∞ –∑–º—É—à—É—î –¥—É–º–∞—Ç–∏ –ø—Ä–æ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –∑–º—ñ–Ω",
              "–ó–æ—Å–µ—Ä–µ–¥—å—Ç–µ—Å—å –Ω–∞ \"–µ–∫–æ–Ω–æ–º—ñ—á–Ω—ñ–π —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ—Å—Ç—ñ\" –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –®–Ü –∑–∞–º—ñ—Å—Ç—å —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç—É ‚Äî —Ü–µ –±—ñ–ª—å—à –ø—Ä–∞–∫—Ç–∏—á–Ω–æ —Ç–∞ –Ω–µ–≤—ñ–¥–∫–ª–∞–¥–Ω–æ",
              "–ß–∏—Ç–∞–π—Ç–µ —ñ –æ–ø—Ç–∏–º—ñ—Å—Ç—ñ–≤ (–ö—É—Ä—Ü–≤–µ–π–ª) —ñ —Å–∫–µ–ø—Ç–∏–∫—ñ–≤ (–ì–µ—Ä—ñ –ú–∞—Ä–∫—É—Å) –¥–ª—è —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è –∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–æ–≥–æ –ø–æ–≥–ª—è–¥—É –Ω–∞ —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ—é –®–Ü"
            ]
          },
          "related": [
            "Feed",
            "Video Content"
          ]
        },
        {
          "slug": "intelligence-explosion",
          "title": {
            "en": "Intelligence Explosion",
            "uk": "–í–∏–±—É—Ö —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É"
          },
          "desc": {
            "en": "The rapid, recursive improvement of AI capabilities.",
            "uk": "–®–≤–∏–¥–∫–µ, —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –®–Ü."
          },
          "overview": {
            "en": [
              "The intelligence explosion concept, first proposed by I.J. Good in 1965, describes a scenario where an AI system capable of improving its own intelligence triggers a rapid cascade of self-improvements. Each improvement makes the next improvement easier, leading to an exponential acceleration of capability that quickly surpasses human intelligence.",
              "This idea is central to both AI safety concerns and techno-optimist visions. Today, we can already see early hints: AI helping design better AI architectures, LLMs writing code that improves LLM training, and AI-guided chip design. Whether these trends lead to a true \"explosion\" or plateau at some point is one of the most important open questions in AI."
            ],
            "uk": [
              "–ö–æ–Ω—Ü–µ–ø—Ü—ñ—è –≤–∏–±—É—Ö—É —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É, –≤–ø–µ—Ä—à–µ –∑–∞–ø—Ä–æ–ø–æ–Ω–æ–≤–∞–Ω–∞ –Ü.–î–∂. –ì—É–¥–æ–º —É 1965, –æ–ø–∏—Å—É—î —Å—Ü–µ–Ω–∞—Ä—ñ–π –¥–µ —Å–∏—Å—Ç–µ–º–∞ –®–Ü, –∑–¥–∞—Ç–Ω–∞ –ø–æ–∫—Ä–∞—â—É–≤–∞—Ç–∏ –≤–ª–∞—Å–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç, –∑–∞–ø—É—Å–∫–∞—î —à–≤–∏–¥–∫–∏–π –∫–∞—Å–∫–∞–¥ —Å–∞–º–æ–≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω—å. –ö–æ–∂–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ä–æ–±–∏—Ç—å –Ω–∞—Å—Ç—É–ø–Ω–µ –ª–µ–≥—à–∏–º, –≤–µ–¥—É—á–∏ –¥–æ –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–æ–≥–æ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π —â–æ —à–≤–∏–¥–∫–æ –ø–µ—Ä–µ–≤–∏—â—É—î –ª—é–¥—Å—å–∫–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç.",
              "–¶—è —ñ–¥–µ—è —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞ —è–∫ –¥–ª—è –∑–∞–Ω–µ–ø–æ–∫–æ—î–Ω—å –±–µ–∑–ø–µ–∫–∏ –®–Ü, —Ç–∞–∫ —ñ –¥–ª—è —Ç–µ—Ö–Ω–æ-–æ–ø—Ç–∏–º—ñ—Å—Ç–∏—á–Ω–∏—Ö –≤—ñ–∑—ñ–π. –°—å–æ–≥–æ–¥–Ω—ñ –≤–∂–µ –≤–∏–¥–Ω–æ —Ä–∞–Ω–Ω—ñ –Ω–∞—Ç—è–∫–∏: –®–Ü –¥–æ–ø–æ–º–∞–≥–∞—î –ø—Ä–æ—î–∫—Ç—É–≤–∞—Ç–∏ –∫—Ä–∞—â—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –®–Ü, LLM –ø–∏—à—É—Ç—å –∫–æ–¥ —â–æ –ø–æ–∫—Ä–∞—â—É—î –Ω–∞–≤—á–∞–Ω–Ω—è LLM, –®–Ü-–∫–µ—Ä–æ–≤–∞–Ω–µ –ø—Ä–æ—î–∫—Ç—É–≤–∞–Ω–Ω—è —á—ñ–ø—ñ–≤. –ß–∏ —Ü—ñ —Ç—Ä–µ–Ω–¥–∏ –≤–µ–¥—É—Ç—å –¥–æ —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ \"–≤–∏–±—É—Ö—É\" —á–∏ –≤–∏—Ö–æ–¥—è—Ç—å –Ω–∞ –ø–ª–∞—Ç–æ ‚Äî –æ–¥–Ω–µ –∑ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö –≤—ñ–¥–∫—Ä–∏—Ç–∏—Ö –ø–∏—Ç–∞–Ω—å."
            ]
          },
          "details": {
            "en": [
              {
                "text": "I.J. Good's Original Concept",
                "desc": "In 1965, mathematician I.J. Good wrote: \"An ultraintelligent machine could design even better machines; there would then unquestionably be an intelligence explosion.\" He called it \"the last invention that man need ever make.\"",
                "links": [
                  {
                    "title": "ASI",
                    "href": "asi.html"
                  }
                ]
              },
              {
                "text": "The Feedback Loop",
                "desc": "The core mechanism: AI designs better AI ‚Üí better AI designs even better AI ‚Üí repeat. Each cycle is faster than the previous because the designer is more intelligent. This is positive feedback at its most powerful.",
                "links": []
              },
              {
                "text": "AI Designing AI Today",
                "desc": "Neural Architecture Search (NAS) uses AI to find optimal model architectures. AlphaChip designs better computer chips. LLMs help write ML research code. We are in the early stages of AI-assisted AI development.",
                "links": []
              },
              {
                "text": "AI Helping Build Better AI",
                "desc": "Current examples: AI-generated synthetic training data, LLMs writing and debugging ML code, AI optimizing hyperparameters, AI-guided chip design for AI hardware. The loop is already partially closed.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Speed of Takeoff",
                "desc": "Fast takeoff (hard): explosion happens in days/weeks, humans cannot intervene. Slow takeoff (soft): gradual acceleration over years, allowing adaptation. Most AI researchers now lean toward a slower, more gradual transition.",
                "links": [
                  {
                    "title": "Singularity",
                    "href": "singularity.html"
                  }
                ]
              },
              {
                "text": "Bottlenecks Preventing Explosion",
                "desc": "Hardware limitations (chip fabrication takes months), data constraints (new data is not instantly available), energy requirements, physical world interactions, and diminishing returns from scaling. These bottlenecks may prevent a sudden explosion.",
                "links": [
                  {
                    "title": "Hardware Basics",
                    "href": "../level-4/hardware.html"
                  }
                ]
              },
              {
                "text": "Compute Overhang",
                "desc": "A dangerous scenario where algorithmic improvements allow existing hardware to produce much more capable AI overnight. This could cause a fast takeoff without the gradual adaptation period of hardware-limited scaling.",
                "links": []
              },
              {
                "text": "The FOOM Debate",
                "desc": "Eliezer Yudkowsky argues for \"FOOM\" ‚Äî rapid, uncontrollable takeoff. Robin Hanson argues for gradual improvement. The debate centers on whether intelligence improvements face diminishing returns or compound exponentially.",
                "links": []
              },
              {
                "text": "Safety Implications",
                "desc": "If an intelligence explosion is possible, alignment must be solved before it begins ‚Äî there may be no time to correct mistakes during a rapid takeoff. This urgency drives much of the AI alignment research agenda.",
                "links": [
                  {
                    "title": "AI Alignment",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "Current Trajectory",
                "desc": "AI capabilities are doubling roughly every 6-12 months. AI is increasingly used in AI development. The question is not whether AI helps build better AI ‚Äî it already does ‚Äî but whether this leads to a discontinuous jump or continued gradual progress.",
                "links": [
                  {
                    "title": "AGI",
                    "href": "agi.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—è –ì—É–¥–∞",
                "desc": "–£ 1965 –º–∞—Ç–µ–º–∞—Ç–∏–∫ –Ü.–î–∂. –ì—É–¥ –Ω–∞–ø–∏—Å–∞–≤: \"–ù–∞–¥—ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞ –º–∞—à–∏–Ω–∞ –º–æ–≥–ª–∞ –± –ø—Ä–æ—î–∫—Ç—É–≤–∞—Ç–∏ —â–µ –∫—Ä–∞—â—ñ –º–∞—à–∏–Ω–∏; –±–µ–∑—Å—É–º–Ω—ñ–≤–Ω–æ –Ω–∞—Å—Ç–∞–≤ –±–∏ –≤–∏–±—É—Ö —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.\" –í—ñ–Ω –Ω–∞–∑–≤–∞–≤ —Ü–µ \"–æ—Å—Ç–∞–Ω–Ω—ñ–º –≤–∏–Ω–∞—Ö–æ–¥–æ–º, —è–∫–∏–π –ª—é–¥–∏–Ω–∞ –∫–æ–ª–∏-–Ω–µ–±—É–¥—å –ø–æ–≤–∏–Ω–Ω–∞ –∑—Ä–æ–±–∏—Ç–∏.\"",
                "links": [
                  {
                    "title": "ASI",
                    "href": "asi.html"
                  }
                ]
              },
              {
                "text": "–ü–µ—Ç–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –∑–≤'—è–∑–∫—É",
                "desc": "–û—Å–Ω–æ–≤–Ω–∏–π –º–µ—Ö–∞–Ω—ñ–∑–º: –®–Ü –ø—Ä–æ—î–∫—Ç—É—î –∫—Ä–∞—â–∏–π –®–Ü ‚Üí –∫—Ä–∞—â–∏–π –®–Ü –ø—Ä–æ—î–∫—Ç—É—î —â–µ –∫—Ä–∞—â–∏–π ‚Üí –ø–æ–≤—Ç–æ—Ä. –ö–æ–∂–µ–Ω —Ü–∏–∫–ª —à–≤–∏–¥—à–∏–π –∑–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –±–æ –¥–∏–∑–∞–π–Ω–µ—Ä —Ä–æ–∑—É–º–Ω—ñ—à–∏–π. –¶–µ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π –∑–≤–æ—Ä–æ—Ç–Ω–∏–π –∑–≤'—è–∑–æ–∫ —É –Ω–∞–π–ø–æ—Ç—É–∂–Ω—ñ—à—ñ–π —Ñ–æ—Ä–º—ñ.",
                "links": []
              },
              {
                "text": "–®–Ü –ø—Ä–æ—î–∫—Ç—É—î –®–Ü —Å—å–æ–≥–æ–¥–Ω—ñ",
                "desc": "Neural Architecture Search (NAS) –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –®–Ü –¥–ª—è –ø–æ—à—É–∫—É –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏—Ö –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä. AlphaChip –ø—Ä–æ—î–∫—Ç—É—î –∫—Ä–∞—â—ñ —á—ñ–ø–∏. LLM –¥–æ–ø–æ–º–∞–≥–∞—é—Ç—å –ø–∏—Å–∞—Ç–∏ –∫–æ–¥ ML-–¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å. –ú–∏ –Ω–∞ —Ä–∞–Ω–Ω—ñ—Ö —Å—Ç–∞–¥—ñ—è—Ö –®–Ü-–∞—Å–∏—Å—Ç–æ–≤–∞–Ω–æ—ó —Ä–æ–∑—Ä–æ–±–∫–∏ –®–Ü.",
                "links": []
              },
              {
                "text": "–®–Ü –¥–æ–ø–æ–º–∞–≥–∞—î –±—É–¥—É–≤–∞—Ç–∏ –∫—Ä–∞—â–∏–π –®–Ü",
                "desc": "–ü–æ—Ç–æ—á–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏: —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω—ñ –¥–∞–Ω—ñ –≤—ñ–¥ –®–Ü, LLM –ø–∏—à—É—Ç—å —Ç–∞ –¥–µ–±–∞–∂–∞—Ç—å ML-–∫–æ–¥, –®–Ü –æ–ø—Ç–∏–º—ñ–∑—É—î –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏, –®–Ü-–∫–µ—Ä–æ–≤–∞–Ω–µ –ø—Ä–æ—î–∫—Ç—É–≤–∞–Ω–Ω—è —á—ñ–ø—ñ–≤. –ü–µ—Ç–ª—è –≤–∂–µ —á–∞—Å—Ç–∫–æ–≤–æ –∑–∞–º–∫–Ω–µ–Ω–∞.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "–®–≤–∏–¥–∫—ñ—Å—Ç—å –ø—ñ–¥–π–æ–º—É",
                "desc": "–®–≤–∏–¥–∫–∏–π –ø—ñ–¥–π–æ–º (–∂–æ—Ä—Å—Ç–∫–∏–π): –≤–∏–±—É—Ö –∑–∞ –¥–Ω—ñ/—Ç–∏–∂–Ω—ñ, –ª—é–¥–∏ –Ω–µ –º–æ–∂—É—Ç—å –≤—Ç—Ä—É—Ç–∏—Ç–∏—Å—è. –ü–æ–≤—ñ–ª—å–Ω–∏–π (–º'—è–∫–∏–π): –ø–æ—Å—Ç—É–ø–æ–≤–µ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –∑–∞ —Ä–æ–∫–∏ –∑ –∞–¥–∞–ø—Ç–∞—Ü—ñ—î—é. –ë—ñ–ª—å—à—ñ—Å—Ç—å –¥–æ—Å–ª—ñ–¥–Ω–∏–∫—ñ–≤ —Ç–µ–ø–µ—Ä —Å—Ö–∏–ª—è—é—Ç—å—Å—è –¥–æ –ø–æ–≤—ñ–ª—å–Ω—ñ—à–æ–≥–æ, –±—ñ–ª—å—à –ø–æ—Å—Ç—É–ø–æ–≤–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥—É.",
                "links": [
                  {
                    "title": "–°–∏–Ω–≥—É–ª—è—Ä–Ω—ñ—Å—Ç—å",
                    "href": "singularity.html"
                  }
                ]
              },
              {
                "text": "–í—É–∑—å–∫—ñ –º—ñ—Å—Ü—è",
                "desc": "–û–±–º–µ–∂–µ–Ω–Ω—è –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è (–≤–∏—Ä–æ–±–Ω–∏—Ü—Ç–≤–æ —á—ñ–ø—ñ–≤ –∑–∞–π–º–∞—î –º—ñ—Å—è—Ü—ñ), –æ–±–º–µ–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö, –≤–∏–º–æ–≥–∏ –¥–æ –µ–Ω–µ—Ä–≥—ñ—ó, –≤–∑–∞—î–º–æ–¥—ñ—è –∑ —Ñ—ñ–∑–∏—á–Ω–∏–º —Å–≤—ñ—Ç–æ–º —Ç–∞ —Å–ø–∞–¥–Ω–∞ –≤—ñ–¥–¥–∞—á–∞ –≤—ñ–¥ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è. –¶—ñ –≤—É–∑—å–∫—ñ –º—ñ—Å—Ü—è –º–æ–∂—É—Ç—å –∑–∞–ø–æ–±—ñ–≥—Ç–∏ —Ä–∞–ø—Ç–æ–≤–æ–º—É –≤–∏–±—É—Ö—É.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è",
                    "href": "../level-4/hardware.html"
                  }
                ]
              },
              {
                "text": "–û–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏–π –Ω–∞–≤—ñ—Å",
                "desc": "–ù–µ–±–µ–∑–ø–µ—á–Ω–∏–π —Å—Ü–µ–Ω–∞—Ä—ñ–π –¥–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –¥–æ–∑–≤–æ–ª—è—é—Ç—å —ñ—Å–Ω—É—é—á–æ–º—É –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—é –≤–∏—Ä–æ–±–∏—Ç–∏ –∑–Ω–∞—á–Ω–æ –±—ñ–ª—å—à –∑–¥–∞—Ç–Ω–∏–π –®–Ü –∑–∞ –Ω—ñ—á. –¶–µ –º–æ–∂–µ —Å–ø—Ä–∏—á–∏–Ω–∏—Ç–∏ —à–≤–∏–¥–∫–∏–π –ø—ñ–¥–π–æ–º –±–µ–∑ –ø–æ—Å—Ç—É–ø–æ–≤–æ–≥–æ –ø–µ—Ä—ñ–æ–¥—É –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó.",
                "links": []
              },
              {
                "text": "–î–µ–±–∞—Ç–∏ –ø—Ä–æ FOOM",
                "desc": "–ï–ª—ñ–∑–µ—Ä –Æ–¥–∫–æ–≤—Å—å–∫–∏–π –∞—Ä–≥—É–º–µ–Ω—Ç—É—î –∑–∞ \"FOOM\" ‚Äî —à–≤–∏–¥–∫–∏–π, –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª—å–æ–≤–∞–Ω–∏–π –ø—ñ–¥–π–æ–º. –†–æ–±—ñ–Ω –•–∞–Ω—Å–æ–Ω –∞—Ä–≥—É–º–µ–Ω—Ç—É—î –∑–∞ –ø–æ—Å—Ç—É–ø–æ–≤–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è. –î–µ–±–∞—Ç–∏ –∑–æ—Å–µ—Ä–µ–¥–∂–µ–Ω—ñ –Ω–∞ —Ç–æ–º—É —á–∏ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É –º–∞—é—Ç—å —Å–ø–∞–¥–Ω—É –≤—ñ–¥–¥–∞—á—É —á–∏ –∫–æ–º–ø–∞—É–Ω–¥—É—é—Ç—å –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–æ.",
                "links": []
              },
              {
                "text": "–ù–∞—Å–ª—ñ–¥–∫–∏ –¥–ª—è –±–µ–∑–ø–µ–∫–∏",
                "desc": "–Ø–∫—â–æ –≤–∏–±—É—Ö —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É –º–æ–∂–ª–∏–≤–∏–π, –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –ø–æ–≤–∏–Ω–Ω–æ –±—É—Ç–∏ –≤–∏—Ä—ñ—à–µ–Ω–µ –¥–æ –π–æ–≥–æ –ø–æ—á–∞—Ç–∫—É ‚Äî –º–æ–∂–µ –Ω–µ –±—É—Ç–∏ —á–∞—Å—É –Ω–∞ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫ –ø—ñ–¥ —á–∞—Å —à–≤–∏–¥–∫–æ–≥–æ –ø—ñ–¥–π–æ–º—É. –¶—è —Ç–µ—Ä–º—ñ–Ω–æ–≤—ñ—Å—Ç—å —Ä—É—Ö–∞—î –ø–æ—Ä—è–¥–æ–∫ –¥–µ–Ω–Ω–∏–π –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –®–Ü",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "–ü–æ—Ç–æ—á–Ω–∞ —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ—è",
                "desc": "–ú–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –®–Ü –ø–æ–¥–≤–æ—é—é—Ç—å—Å—è –ø—Ä–∏–±–ª–∏–∑–Ω–æ –∫–æ–∂–Ω—ñ 6-12 –º—ñ—Å—è—Ü—ñ–≤. –®–Ü –≤—Å–µ –±—ñ–ª—å—à–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —É —Ä–æ–∑—Ä–æ–±—Ü—ñ –®–Ü. –ü–∏—Ç–∞–Ω–Ω—è –Ω–µ –≤ —Ç–æ–º—É —á–∏ –®–Ü –¥–æ–ø–æ–º–∞–≥–∞—î –±—É–¥—É–≤–∞—Ç–∏ –∫—Ä–∞—â–∏–π –®–Ü ‚Äî –≤—ñ–Ω –≤–∂–µ —Ü–µ —Ä–æ–±–∏—Ç—å ‚Äî –∞ —á–∏ —Ü–µ –≤–µ–¥–µ –¥–æ —Ä–æ–∑—Ä–∏–≤–Ω–æ–≥–æ —Å—Ç—Ä–∏–±–∫–∞ —á–∏ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –ø–æ—Å—Ç—É–ø–æ–≤–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—É.",
                "links": [
                  {
                    "title": "AGI",
                    "href": "agi.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Intelligence Explosion",
                "def": "Rapid, recursive self-improvement of AI leading to superintelligence in a short timeframe."
              },
              {
                "term": "Fast Takeoff",
                "def": "Scenario where AI self-improvement happens so rapidly (days/weeks) that humans cannot intervene or adapt."
              },
              {
                "term": "Slow Takeoff",
                "def": "Gradual AI capability acceleration over years or decades, allowing human adaptation and course correction."
              },
              {
                "term": "Compute Overhang",
                "def": "Situation where algorithmic breakthroughs unlock much greater AI capability on existing hardware."
              }
            ],
            "uk": [
              {
                "term": "–í–∏–±—É—Ö —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É",
                "def": "–®–≤–∏–¥–∫–µ, —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–µ —Å–∞–º–æ–≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è –®–Ü, —â–æ –≤–µ–¥–µ –¥–æ —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç—É –∑–∞ –∫–æ—Ä–æ—Ç–∫–∏–π —á–∞—Å."
              },
              {
                "term": "–®–≤–∏–¥–∫–∏–π –ø—ñ–¥–π–æ–º",
                "def": "–°—Ü–µ–Ω–∞—Ä—ñ–π –¥–µ —Å–∞–º–æ–≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è –®–Ü –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –Ω–∞—Å—Ç—ñ–ª—å–∫–∏ —à–≤–∏–¥–∫–æ (–¥–Ω—ñ/—Ç–∏–∂–Ω—ñ) —â–æ –ª—é–¥–∏ –Ω–µ –º–æ–∂—É—Ç—å –≤—Ç—Ä—É—Ç–∏—Ç–∏—Å—è."
              },
              {
                "term": "–ü–æ–≤—ñ–ª—å–Ω–∏–π –ø—ñ–¥–π–æ–º",
                "def": "–ü–æ—Å—Ç—É–ø–æ–≤–µ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –®–Ü –∑–∞ —Ä–æ–∫–∏ –∞–±–æ –¥–µ—Å—è—Ç–∏–ª—ñ—Ç—Ç—è –∑ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—é –ª—é–¥—Å—å–∫–æ—ó –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó —Ç–∞ –∫–æ—Ä–µ–∫—Ü—ñ—ó –∫—É—Ä—Å—É."
              },
              {
                "term": "–û–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏–π –Ω–∞–≤—ñ—Å",
                "def": "–°–∏—Ç—É–∞—Ü—ñ—è –¥–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω—ñ –ø—Ä–æ—Ä–∏–≤–∏ —Ä–æ–∑–±–ª–æ–∫–æ–≤—É—é—Ç—å –∑–Ω–∞—á–Ω–æ –±—ñ–ª—å—à—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –®–Ü –Ω–∞ —ñ—Å–Ω—É—é—á–æ–º—É –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—ñ."
              }
            ]
          },
          "tips": {
            "en": [
              "Watch for AI-assisted AI development as a leading indicator ‚Äî it is the intelligence explosion in its earliest, mildest form",
              "The debate between fast and slow takeoff has practical implications for how much time we have to solve alignment",
              "Even without a dramatic \"explosion,\" the accelerating pace of AI progress demands continuous learning and adaptation"
            ],
            "uk": [
              "–°–ª—ñ–¥–∫—É–π—Ç–µ –∑–∞ –®–Ü-–∞—Å–∏—Å—Ç–æ–≤–∞–Ω–æ—é —Ä–æ–∑—Ä–æ–±–∫–æ—é –®–Ü —è–∫ –≤–∏–ø–µ—Ä–µ–¥–∂–∞—é—á–∏–º —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º ‚Äî —Ü–µ –≤–∏–±—É—Ö —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É —É –Ω–∞–π—Ä–∞–Ω—ñ—à—ñ–π, –Ω–∞–π–º'—è–∫—à—ñ–π —Ñ–æ—Ä–º—ñ",
              "–î–µ–±–∞—Ç–∏ –º—ñ–∂ —à–≤–∏–¥–∫–∏–º —Ç–∞ –ø–æ–≤—ñ–ª—å–Ω–∏–º –ø—ñ–¥–π–æ–º–æ–º –º–∞—é—Ç—å –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –Ω–∞—Å–ª—ñ–¥–∫–∏ –¥–ª—è —Ç–æ–≥–æ —Å–∫—ñ–ª—å–∫–∏ —á–∞—Å—É –º–∞—î–º–æ –Ω–∞ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è",
              "–ù–∞–≤—ñ—Ç—å –±–µ–∑ –¥—Ä–∞–º–∞—Ç–∏—á–Ω–æ–≥–æ \"–≤–∏–±—É—Ö—É\", –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è —Ç–µ–º–ø—É –ø—Ä–æ–≥—Ä–µ—Å—É –®–Ü –≤–∏–º–∞–≥–∞—î –±–µ–∑–ø–µ—Ä–µ—Ä–≤–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó"
            ]
          },
          "related": [
            "Feed"
          ]
        },
        {
          "slug": "transhumanism",
          "title": {
            "en": "Transhumanism",
            "uk": "–¢—Ä–∞–Ω—Å–≥—É–º–∞–Ω—ñ–∑–º"
          },
          "desc": {
            "en": "Human enhancement through technology and AI.",
            "uk": "–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –ª—é–¥–∏–Ω–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π —Ç–∞ –®–Ü."
          },
          "overview": {
            "en": [
              "Transhumanism is the philosophical and technological movement that advocates using technology to fundamentally enhance human capabilities ‚Äî cognitive, physical, and emotional. Brain-computer interfaces, genetic engineering, AI augmentation, and longevity research are all paths toward transcending current human limitations.",
              "AI plays a central role in the transhumanist vision: as a direct cognitive enhancer (AI copilots for thinking), as a tool for accelerating biomedical breakthroughs, and potentially as a merger partner through brain-computer interfaces. The question is not just \"can we enhance humans?\" but \"should we, and if so, how do we ensure equitable access?\""
            ],
            "uk": [
              "–¢—Ä–∞–Ω—Å–≥—É–º–∞–Ω—ñ–∑–º ‚Äî —Ü–µ —Ñ—ñ–ª–æ—Å–æ—Ñ—Å—å–∫–∏–π —Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∏–π —Ä—É—Ö, —â–æ –≤–∏—Å—Ç—É–ø–∞—î –∑–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π –¥–ª—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –ª—é–¥—Å—å–∫–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π ‚Äî –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω–∏—Ö, —Ñ—ñ–∑–∏—á–Ω–∏—Ö —Ç–∞ –µ–º–æ—Ü—ñ–π–Ω–∏—Ö. –Ü–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏ –º–æ–∑–æ–∫-–∫–æ–º–ø'—é—Ç–µ—Ä, –≥–µ–Ω–Ω–∞ —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—è, –®–Ü-–∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—è —Ç–∞ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –¥–æ–≤–≥–æ–ª—ñ—Ç—Ç—è ‚Äî –≤—Å—ñ —à–ª—è—Ö–∏ –¥–æ –ø–æ–¥–æ–ª–∞–Ω–Ω—è –ø–æ—Ç–æ—á–Ω–∏—Ö –ª—é–¥—Å—å–∫–∏—Ö –æ–±–º–µ–∂–µ–Ω—å.",
              "–®–Ü –≥—Ä–∞—î —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É —Ä–æ–ª—å —É —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω—ñ—Å—Ç–∏—á–Ω—ñ–π –≤—ñ–∑—ñ—ó: —è–∫ –ø—Ä—è–º–∏–π –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω–∏–π –ø—ñ–¥—Å–∏–ª—é–≤–∞—á (–®–Ü-–∫–æ–ø—ñ–ª–æ—Ç–∏ –¥–ª—è –º–∏—Å–ª–µ–Ω–Ω—è), —è–∫ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –±—ñ–æ–º–µ–¥–∏—á–Ω–∏—Ö –ø—Ä–æ—Ä–∏–≤—ñ–≤, —Ç–∞ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ —è–∫ –ø–∞—Ä—Ç–Ω–µ—Ä –¥–ª—è –∑–ª–∏—Ç—Ç—è —á–µ—Ä–µ–∑ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏ –º–æ–∑–æ–∫-–∫–æ–º–ø'—é—Ç–µ—Ä. –ü–∏—Ç–∞–Ω–Ω—è –Ω–µ –ª–∏—à–µ \"—á–∏ –º–æ–∂–µ–º–æ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ –ª—é–¥–µ–π?\" –∞ \"—á–∏ –≤–∞—Ä—Ç–æ, —ñ —è–∫—â–æ —Ç–∞–∫, —è–∫ –∑–∞–±–µ–∑–ø–µ—á–∏—Ç–∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∏–π –¥–æ—Å—Ç—É–ø?\""
            ]
          },
          "details": {
            "en": [
              {
                "text": "Brain-Computer Interfaces",
                "desc": "Neuralink, Synchron, and others developing direct neural interfaces. Current BCIs restore lost function (paralysis, vision). Future: enhancing normal cognition, direct brain-to-AI communication, memory augmentation.",
                "links": []
              },
              {
                "text": "Cognitive Enhancement via AI",
                "desc": "AI as an external cognitive layer: Copilot for coding, AI for research, automated reasoning assistance. This is already here ‚Äî knowledge workers with AI are more productive than those without. The enhancement is external but real.",
                "links": [
                  {
                    "title": "Vibecoding",
                    "href": "../level-2/vibecoding.html"
                  }
                ]
              },
              {
                "text": "Human-AI Symbiosis",
                "desc": "Licklider's 1960 vision of \"man-computer symbiosis\" is being realized. Humans provide intent, creativity, and judgment; AI provides speed, breadth, and tireless execution. The combination outperforms either alone.",
                "links": [
                  {
                    "title": "Agents",
                    "href": "../level-4/agents.html"
                  }
                ]
              },
              {
                "text": "Genetic Engineering and AI",
                "desc": "AI accelerates gene therapy research, drug discovery, and protein design (AlphaFold). CRISPR combined with AI analysis could enable targeted genetic enhancements. Ethical debates around germline editing are intense.",
                "links": []
              },
              {
                "text": "Longevity Research",
                "desc": "AI is accelerating aging research: identifying longevity genes, designing anti-aging drugs, modeling biological pathways. Companies like Calico (Google), Altos Labs, and Insilico Medicine use AI to target aging mechanisms.",
                "links": []
              },
              {
                "text": "Digital Minds",
                "desc": "Whole brain emulation (uploading): simulating a human brain in software. Currently theoretical, but if achievable, it would represent the ultimate human-AI merger. Raises profound questions about identity and consciousness.",
                "links": []
              },
              {
                "text": "Enhanced Senses",
                "desc": "AI-powered prosthetics that exceed natural ability, AR/VR providing superhuman information overlay, real-time language translation earbuds, AI-enhanced medical imaging giving doctors \"super-vision.\"",
                "links": []
              },
              {
                "text": "Ethical Considerations",
                "desc": "Enhancement access inequality (rich vs poor), consent issues (especially for children), identity questions (when does enhancement change \"who you are\"?), competitive pressure to enhance, and the risk of losing valued human qualities.",
                "links": []
              },
              {
                "text": "The Equality Challenge",
                "desc": "If cognitive enhancement becomes possible, unequal access could create unprecedented inequality ‚Äî a class division based on cognitive capability. Universal access frameworks and regulation would be essential.",
                "links": []
              },
              {
                "text": "Current State",
                "desc": "We are already transhumanist in mild ways: smartphones extend memory, AI extends cognition, medicine extends lifespan. The debate is about degree and speed of enhancement, not whether to enhance at all.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "–Ü–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏ –º–æ–∑–æ–∫-–∫–æ–º–ø'—é—Ç–µ—Ä",
                "desc": "Neuralink, Synchron —Ç–∞ —ñ–Ω—à—ñ —Ä–æ–∑—Ä–æ–±–ª—è—é—Ç—å –ø—Ä—è–º—ñ –Ω–µ–π—Ä–æ—ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏. –ü–æ—Ç–æ—á–Ω—ñ BCI –≤—ñ–¥–Ω–æ–≤–ª—é—é—Ç—å –≤—Ç—Ä–∞—á–µ–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó (–ø–∞—Ä–∞–ª—ñ—á, –∑—ñ—Ä). –ú–∞–π–±—É—Ç–Ω—î: –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ—ó –∫–æ–≥–Ω—ñ—Ü—ñ—ó, –ø—Ä—è–º–∞ –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—è –º–æ–∑–æ–∫-–®–Ü, –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –ø–∞–º'—è—Ç—ñ.",
                "links": []
              },
              {
                "text": "–ö–æ–≥–Ω—ñ—Ç–∏–≤–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —á–µ—Ä–µ–∑ –®–Ü",
                "desc": "–®–Ü —è–∫ –∑–æ–≤–Ω—ñ—à–Ω—ñ–π –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω–∏–π —à–∞—Ä: Copilot –¥–ª—è –∫–æ–¥—É–≤–∞–Ω–Ω—è, –®–Ü –¥–ª—è –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å, –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –¥–æ–ø–æ–º–æ–≥–∞ —É –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—ñ. –¶–µ –≤–∂–µ —Ç—É—Ç ‚Äî –ø—Ä–∞—Ü—ñ–≤–Ω–∏–∫–∏ –∑–Ω–∞–Ω—å –∑ –®–Ü –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—à—ñ –∑–∞ —Ç–∏—Ö –±–µ–∑. –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –∑–æ–≤–Ω—ñ—à–Ω—î, –∞–ª–µ —Ä–µ–∞–ª—å–Ω–µ.",
                "links": [
                  {
                    "title": "–í–∞–π–±–∫–æ–¥–∏–Ω–≥",
                    "href": "../level-2/vibecoding.html"
                  }
                ]
              },
              {
                "text": "–°–∏–º–±—ñ–æ–∑ –ª—é–¥–∏–Ω–∏ —Ç–∞ –®–Ü",
                "desc": "–í—ñ–∑—ñ—è \"—Å–∏–º–±—ñ–æ–∑—É –ª—é–¥–∏–Ω–∏ —Ç–∞ –∫–æ–º–ø'—é—Ç–µ—Ä–∞\" –õ—ñ–∫–ª–∞–π–¥–µ—Ä–∞ 1960 —Ä–æ–∫—É —Ä–µ–∞–ª—ñ–∑—É—î—Ç—å—Å—è. –õ—é–¥–∏ –Ω–∞–¥–∞—é—Ç—å –Ω–∞–º—ñ—Ä, –∫—Ä–µ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å —Ç–∞ —Å—É–¥–∂–µ–Ω–Ω—è; –®–Ü –Ω–∞–¥–∞—î —à–≤–∏–¥–∫—ñ—Å—Ç—å, —à–∏—Ä–æ—Ç—É —Ç–∞ –Ω–µ–≤—Ç–æ–º–Ω–µ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è. –ö–æ–º–±—ñ–Ω–∞—Ü—ñ—è –ø–µ—Ä–µ–≤–µ—Ä—à—É—î –∫–æ–∂–Ω–æ–≥–æ –æ–∫—Ä–µ–º–æ.",
                "links": [
                  {
                    "title": "–ê–≥–µ–Ω—Ç–∏",
                    "href": "../level-4/agents.html"
                  }
                ]
              },
              {
                "text": "–ì–µ–Ω–Ω–∞ —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—è —Ç–∞ –®–Ü",
                "desc": "–®–Ü –ø—Ä–∏—Å–∫–æ—Ä—é—î –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≥–µ–Ω–Ω–æ—ó —Ç–µ—Ä–∞–ø—ñ—ó, –≤—ñ–¥–∫—Ä–∏—Ç—Ç—è –ª—ñ–∫—ñ–≤ —Ç–∞ –¥–∏–∑–∞–π–Ω –±—ñ–ª–∫—ñ–≤ (AlphaFold). CRISPR —É –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó –∑ –®–Ü-–∞–Ω–∞–ª—ñ–∑–æ–º –º–æ–∂–µ –¥–æ–∑–≤–æ–ª–∏—Ç–∏ —Ü—ñ–ª–µ—Å–ø—Ä—è–º–æ–≤–∞–Ω—ñ –≥–µ–Ω–µ—Ç–∏—á–Ω—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è. –ï—Ç–∏—á–Ω—ñ –¥–µ–±–∞—Ç–∏ –Ω–∞–≤–∫–æ–ª–æ —Ä–µ–¥–∞–≥—É–≤–∞–Ω–Ω—è –∑–∞—Ä–æ–¥–∫–æ–≤–æ—ó –ª—ñ–Ω—ñ—ó —ñ–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ñ.",
                "links": []
              },
              {
                "text": "–î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –¥–æ–≤–≥–æ–ª—ñ—Ç—Ç—è",
                "desc": "–®–Ü –ø—Ä–∏—Å–∫–æ—Ä—é—î –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —Å—Ç–∞—Ä—ñ–Ω–Ω—è: —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è –≥–µ–Ω—ñ–≤ –¥–æ–≤–≥–æ–ª—ñ—Ç—Ç—è, –¥–∏–∑–∞–π–Ω –∞–Ω—Ç–∏–≤—ñ–∫–æ–≤–∏—Ö –ª—ñ–∫—ñ–≤, –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è –±—ñ–æ–ª–æ–≥—ñ—á–Ω–∏—Ö —à–ª—è—Ö—ñ–≤. –ö–æ–º–ø–∞–Ω—ñ—ó —è–∫ Calico (Google), Altos Labs –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –®–Ü –¥–ª—è –±–æ—Ä–æ—Ç—å–±–∏ –∑—ñ —Å—Ç–∞—Ä—ñ–Ω–Ω—è–º.",
                "links": []
              },
              {
                "text": "–¶–∏—Ñ—Ä–æ–≤—ñ —Ä–æ–∑—É–º–∏",
                "desc": "–ü–æ–≤–Ω–∞ –µ–º—É–ª—è—Ü—ñ—è –º–æ–∑–∫—É (–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è): —Å–∏–º—É–ª—è—Ü—ñ—è –ª—é–¥—Å—å–∫–æ–≥–æ –º–æ–∑–∫—É –≤ –ø—Ä–æ–≥—Ä–∞–º–Ω–æ–º—É –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—ñ. –ù–∞—Ä–∞–∑—ñ —Ç–µ–æ—Ä–µ—Ç–∏—á–Ω–æ, –∞–ª–µ —è–∫—â–æ –¥–æ—Å—è–∂–Ω–æ ‚Äî —Ü–µ –±—É–ª–æ –± –æ—Å—Ç–∞—Ç–æ—á–Ω–µ –∑–ª–∏—Ç—Ç—è –ª—é–¥–∏–Ω–∏ —Ç–∞ –®–Ü. –ü–æ—Ä—É—à—É—î –≥–ª–∏–±–æ–∫—ñ –ø–∏—Ç–∞–Ω–Ω—è —ñ–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—ñ —Ç–∞ —Å–≤—ñ–¥–æ–º–æ—Å—Ç—ñ.",
                "links": []
              },
              {
                "text": "–ü–æ–∫—Ä–∞—â–µ–Ω—ñ –≤—ñ–¥—á—É—Ç—Ç—è",
                "desc": "–®–Ü-–ø—Ä–æ—Ç–µ–∑–∏ —â–æ –ø–µ—Ä–µ–≤–∏—â—É—é—Ç—å –ø—Ä–∏—Ä–æ–¥–Ω—ñ –∑–¥—ñ–±–Ω–æ—Å—Ç—ñ, AR/VR –∑ –Ω–∞–¥–ª—é–¥—Å—å–∫–∏–º —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω–∏–º –Ω–∞—à–∞—Ä—É–≤–∞–Ω–Ω—è–º, –Ω–∞–≤—É—à–Ω–∏–∫–∏ –∑ —Ä–µ–∞–ª-—Ç–∞–π–º –ø–µ—Ä–µ–∫–ª–∞–¥–æ–º, –®–Ü-–ø–æ–∫—Ä–∞—â–µ–Ω–∞ –º–µ–¥–∏—á–Ω–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —â–æ –¥–∞—î –ª—ñ–∫–∞—Ä—è–º \"—Å—É–ø–µ—Ä–∑—ñ—Ä\".",
                "links": []
              },
              {
                "text": "–ï—Ç–∏—á–Ω—ñ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                "desc": "–ù–µ—Ä—ñ–≤–Ω—ñ—Å—Ç—å –¥–æ—Å—Ç—É–ø—É –¥–æ –ø–æ–∫—Ä–∞—â–µ–Ω—å (–±–∞–≥–∞—Ç—ñ –ø—Ä–æ—Ç–∏ –±—ñ–¥–Ω–∏—Ö), –ø–∏—Ç–∞–Ω–Ω—è –∑–≥–æ–¥–∏ (–æ—Å–æ–±–ª–∏–≤–æ –¥–ª—è –¥—ñ—Ç–µ–π), –ø–∏—Ç–∞–Ω–Ω—è —ñ–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—ñ (–∫–æ–ª–∏ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –∑–º—ñ–Ω—é—î \"—Ö—Ç–æ –≤–∏ —î\"?), –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏–π —Ç–∏—Å–∫ —Ç–∞ —Ä–∏–∑–∏–∫ –≤—Ç—Ä–∞—Ç–∏ —Ü—ñ–Ω–Ω–∏—Ö –ª—é–¥—Å—å–∫–∏—Ö —è–∫–æ—Å—Ç–µ–π.",
                "links": []
              },
              {
                "text": "–í–∏–∫–ª–∏–∫ —Ä—ñ–≤–Ω–æ—Å—Ç—ñ",
                "desc": "–Ø–∫—â–æ –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Å—Ç–∞–Ω–µ –º–æ–∂–ª–∏–≤–∏–º, –Ω–µ—Ä—ñ–≤–Ω–∏–π –¥–æ—Å—Ç—É–ø –º–æ–∂–µ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –±–µ–∑–ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–Ω—É –Ω–µ—Ä—ñ–≤–Ω—ñ—Å—Ç—å ‚Äî –∫–ª–∞—Å–æ–≤–∏–π –ø–æ–¥—ñ–ª –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π. –†–∞–º–∫–∏ —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø—É —Ç–∞ —Ä–µ–≥—É–ª—é–≤–∞–Ω–Ω—è –±—É–¥—É—Ç—å –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ.",
                "links": []
              },
              {
                "text": "–ü–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω",
                "desc": "–ú–∏ –≤–∂–µ —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω—ñ—Å—Ç–∏ —É –º'—è–∫—ñ–π —Ñ–æ—Ä–º—ñ: —Å–º–∞—Ä—Ç—Ñ–æ–Ω–∏ —Ä–æ–∑—à–∏—Ä—é—é—Ç—å –ø–∞–º'—è—Ç—å, –®–Ü —Ä–æ–∑—à–∏—Ä—é—î –∫–æ–≥–Ω—ñ—Ü—ñ—é, –º–µ–¥–∏—Ü–∏–Ω–∞ –ø–æ–¥–æ–≤–∂—É—î –∂–∏—Ç—Ç—è. –î–µ–±–∞—Ç–∏ –ø—Ä–æ —Å—Ç—É–ø—ñ–Ω—å —Ç–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è, –∞ –Ω–µ –ø—Ä–æ —Ç–µ —á–∏ –ø–æ–∫—Ä–∞—â—É–≤–∞—Ç–∏ –≤–∑–∞–≥–∞–ª—ñ.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Brain-Computer Interface",
                "def": "Direct communication pathway between the brain and external devices ‚Äî enabling thought-controlled computers."
              },
              {
                "term": "Cognitive Enhancement",
                "def": "Using technology to improve human mental capabilities beyond natural limits."
              },
              {
                "term": "Whole Brain Emulation",
                "def": "Hypothetical scanning and simulating of an entire human brain in software."
              },
              {
                "term": "Human-AI Symbiosis",
                "def": "Humans and AI working together as a combined system that outperforms either alone."
              }
            ],
            "uk": [
              {
                "term": "–Ü–Ω—Ç–µ—Ä—Ñ–µ–π—Å –º–æ–∑–æ–∫-–∫–æ–º–ø'—é—Ç–µ—Ä",
                "def": "–ü—Ä—è–º–∏–π –∫–∞–Ω–∞–ª –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—ó –º—ñ–∂ –º–æ–∑–∫–æ–º —Ç–∞ –∑–æ–≤–Ω—ñ—à–Ω—ñ–º–∏ –ø—Ä–∏—Å—Ç—Ä–æ—è–º–∏ ‚Äî –¥–ª—è –∫–µ—Ä—É–≤–∞–Ω–Ω—è –∫–æ–º–ø'—é—Ç–µ—Ä–æ–º –¥—É–º–∫–∞–º–∏."
              },
              {
                "term": "–ö–æ–≥–Ω—ñ—Ç–∏–≤–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è",
                "def": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –ª—é–¥—Å—å–∫–∏—Ö —Ä–æ–∑—É–º–æ–≤–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –∑–∞ –º–µ–∂—ñ –ø—Ä–∏—Ä–æ–¥–Ω–∏—Ö –ª—ñ–º—ñ—Ç—ñ–≤."
              },
              {
                "term": "–ü–æ–≤–Ω–∞ –µ–º—É–ª—è—Ü—ñ—è –º–æ–∑–∫—É",
                "def": "–ì—ñ–ø–æ—Ç–µ—Ç–∏—á–Ω–µ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ —Å–∏–º—É–ª—è—Ü—ñ—è —Ü—ñ–ª–æ–≥–æ –ª—é–¥—Å—å–∫–æ–≥–æ –º–æ–∑–∫—É –≤ –ø—Ä–æ–≥—Ä–∞–º–Ω–æ–º—É –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—ñ."
              },
              {
                "term": "–°–∏–º–±—ñ–æ–∑ –ª—é–¥–∏–Ω–∏ —Ç–∞ –®–Ü",
                "def": "–õ—é–¥–∏ —Ç–∞ –®–Ü –ø—Ä–∞—Ü—é—é—Ç—å —Ä–∞–∑–æ–º —è–∫ –∫–æ–º–±—ñ–Ω–æ–≤–∞–Ω–∞ —Å–∏—Å—Ç–µ–º–∞, —â–æ –ø–µ—Ä–µ–≤–µ—Ä—à—É—î –∫–æ–∂–Ω–æ–≥–æ –æ–∫—Ä–µ–º–æ."
              }
            ]
          },
          "tips": {
            "en": [
              "You are already a transhumanist in practice ‚Äî AI coding assistants and knowledge tools are cognitive enhancement by another name",
              "Follow Neuralink and Synchron progress for the most concrete BCI developments happening now",
              "The equity implications of cognitive enhancement should inform your thinking about AI access and policy today"
            ],
            "uk": [
              "–í–∏ –≤–∂–µ —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω—ñ—Å—Ç –Ω–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ ‚Äî –®–Ü-–∞—Å–∏—Å—Ç–µ–Ω—Ç–∏ –∫–æ–¥—É–≤–∞–Ω–Ω—è —Ç–∞ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –∑–Ω–∞–Ω—å —î –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω–∏–º –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º –ø—ñ–¥ —ñ–Ω—à–æ—é –Ω–∞–∑–≤–æ—é",
              "–°–ª—ñ–¥–∫—É–π—Ç–µ –∑–∞ –ø—Ä–æ–≥—Ä–µ—Å–æ–º Neuralink —Ç–∞ Synchron –¥–ª—è –Ω–∞–π–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ—à–∏—Ö —Ä–æ–∑—Ä–æ–±–æ–∫ BCI —â–æ –≤—ñ–¥–±—É–≤–∞—é—Ç—å—Å—è –∑–∞—Ä–∞–∑",
              "–ù–∞—Å–ª—ñ–¥–∫–∏ —Ä—ñ–≤–Ω–æ—Å—Ç—ñ –∫–æ–≥–Ω—ñ—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –ø–æ–≤–∏–Ω–Ω—ñ —ñ–Ω—Ñ–æ—Ä–º—É–≤–∞—Ç–∏ –≤–∞—à–µ –º–∏—Å–ª–µ–Ω–Ω—è –ø—Ä–æ –¥–æ—Å—Ç—É–ø –¥–æ –®–Ü —Ç–∞ –ø–æ–ª—ñ—Ç–∏–∫—É —Å—å–æ–≥–æ–¥–Ω—ñ"
            ]
          },
          "related": [
            "Feed"
          ]
        },
        {
          "slug": "spatial-intelligence",
          "title": {
            "en": "Spatial Intelligence",
            "uk": "–ü—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç"
          },
          "desc": {
            "en": "AI understanding of 3D space, physics, and physical world.",
            "uk": "–†–æ–∑—É–º—ñ–Ω–Ω—è –®–Ü 3D-–ø—Ä–æ—Å—Ç–æ—Ä—É, —Ñ—ñ–∑–∏–∫–∏ —Ç–∞ —Ñ—ñ–∑–∏—á–Ω–æ–≥–æ —Å–≤—ñ—Ç—É."
          },
          "overview": {
            "en": [
              "Spatial intelligence is AI's ability to understand and reason about the three-dimensional physical world ‚Äî perceiving space, predicting physical interactions, navigating environments, and manipulating objects. While LLMs excel at language, spatial intelligence addresses the gap between digital and physical understanding.",
              "This field is critical for robotics, autonomous vehicles, AR/VR, and any application where AI must interact with the physical world. Recent advances in 3D generation, physics simulation, and embodied AI are rapidly closing the gap between AI's language abilities and its understanding of physical space."
            ],
            "uk": [
              "–ü—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç ‚Äî —Ü–µ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –®–Ü —Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–∞ –º—ñ—Ä–∫—É–≤–∞—Ç–∏ –ø—Ä–æ —Ç—Ä–∏–≤–∏–º—ñ—Ä–Ω–∏–π —Ñ—ñ–∑–∏—á–Ω–∏–π —Å–≤—ñ—Ç ‚Äî —Å–ø—Ä–∏–π–º–∞—Ç–∏ –ø—Ä–æ—Å—Ç—ñ—Ä, –ø–µ—Ä–µ–¥–±–∞—á–∞—Ç–∏ —Ñ—ñ–∑–∏—á–Ω—ñ –≤–∑–∞—î–º–æ–¥—ñ—ó, –Ω–∞–≤—ñ–≥—É–≤–∞—Ç–∏ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞–º–∏ —Ç–∞ –º–∞–Ω—ñ–ø—É–ª—é–≤–∞—Ç–∏ –æ–±'—î–∫—Ç–∞–º–∏. –ü–æ–∫–∏ LLM –≤—ñ–¥–º—ñ–Ω–Ω—ñ —É –º–æ–≤—ñ, –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç –∑–∞–ø–æ–≤–Ω—é—î —Ä–æ–∑—Ä–∏–≤ –º—ñ–∂ —Ü–∏—Ñ—Ä–æ–≤–∏–º —Ç–∞ —Ñ—ñ–∑–∏—á–Ω–∏–º —Ä–æ–∑—É–º—ñ–Ω–Ω—è–º.",
              "–¶–µ –ø–æ–ª–µ –∫—Ä–∏—Ç–∏—á–Ω–µ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω—ñ–∫–∏, –∞–≤—Ç–æ–Ω–æ–º–Ω–∏—Ö —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∏—Ö –∑–∞—Å–æ–±—ñ–≤, AR/VR —Ç–∞ –±—É–¥—å-—è–∫–æ–≥–æ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –¥–µ –®–Ü –ø–æ–≤–∏–Ω–µ–Ω –≤–∑–∞—î–º–æ–¥—ñ—è—Ç–∏ –∑ —Ñ—ñ–∑–∏—á–Ω–∏–º —Å–≤—ñ—Ç–æ–º. –ù–µ—â–æ–¥–∞–≤–Ω—ñ –ø—Ä–æ—Ä–∏–≤–∏ —É 3D-–≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó, —Å–∏–º—É–ª—è—Ü—ñ—ó —Ñ—ñ–∑–∏–∫–∏ —Ç–∞ –≤—Ç—ñ–ª–µ–Ω–æ–º—É –®–Ü —à–≤–∏–¥–∫–æ —Å–∫–æ—Ä–æ—á—É—é—Ç—å —Ä–æ–∑—Ä–∏–≤ –º—ñ–∂ –º–æ–≤–Ω–∏–º–∏ –∑–¥—ñ–±–Ω–æ—Å—Ç—è–º–∏ –®–Ü —Ç–∞ —Ä–æ–∑—É–º—ñ–Ω–Ω—è–º —Ñ—ñ–∑–∏—á–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç–æ—Ä—É."
            ]
          },
          "details": {
            "en": [
              {
                "text": "What Is Spatial Intelligence",
                "desc": "The ability to understand 3D structure, spatial relationships, physics, and physical causality. Humans do this naturally ‚Äî AI must learn it from data. Fei-Fei Li calls it \"the next frontier of AI.\"",
                "links": []
              },
              {
                "text": "Computer Vision to 3D Understanding",
                "desc": "Evolution from 2D image recognition (ImageNet era) to 3D scene understanding. NeRF and Gaussian Splatting reconstruct 3D scenes from photos. Depth estimation, object pose detection, and scene graphs map spatial relationships.",
                "links": [
                  {
                    "title": "Multimodality",
                    "href": "../level-1/multimodality.html"
                  }
                ]
              },
              {
                "text": "3D Generation",
                "desc": "AI generating 3D objects and scenes from text or images. Point-E, Shap-E (OpenAI), DreamFusion (Google), Meshy, and others create 3D assets. Applications: gaming, architecture, product design, virtual worlds.",
                "links": [
                  {
                    "title": "Generative AI",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "Physics Simulation",
                "desc": "AI learning physical dynamics: how objects fall, collide, deform, and interact. Differentiable physics engines combine traditional simulation with neural networks. Enables more realistic prediction of physical outcomes.",
                "links": []
              },
              {
                "text": "Robotics and Embodied AI",
                "desc": "Robots that perceive, understand, and act in physical space. Foundation models for robotics (RT-2, Octo) use language-vision-action training. The challenge: bridging the sim-to-real gap between virtual training and physical deployment.",
                "links": [
                  {
                    "title": "Agents",
                    "href": "../level-4/agents.html"
                  }
                ]
              },
              {
                "text": "Autonomous Navigation",
                "desc": "Self-driving cars, drones, and delivery robots navigating complex 3D environments. Combines perception (cameras, LiDAR), mapping (SLAM), planning (path optimization), and real-time decision making.",
                "links": []
              },
              {
                "text": "World Models for Spatial AI",
                "desc": "AI systems that build internal models of how the physical world works. Predict \"what happens next\" in physical scenarios. Critical for planning physical actions and understanding consequences before acting.",
                "links": [
                  {
                    "title": "General World Model",
                    "href": "world-model.html"
                  }
                ]
              },
              {
                "text": "AR/VR and Spatial Computing",
                "desc": "Apple Vision Pro, Meta Quest, and spatial computing platforms need AI that understands 3D space. Real-time object recognition, scene understanding, hand tracking, and spatial anchoring all require spatial intelligence.",
                "links": []
              },
              {
                "text": "Multimodal Spatial Understanding",
                "desc": "Combining language with spatial reasoning: \"put the cup on the table to the left of the book.\" Requires grounding language in 3D space. Models like SpatialVLM and 3D-LLM bridge language and spatial understanding.",
                "links": []
              },
              {
                "text": "The Gap to Close",
                "desc": "A 2-year-old child has better spatial understanding than the most advanced AI. Closing this gap requires both better architectures (world models, embodied training) and better data (real-world interaction at scale).",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "–©–æ —Ç–∞–∫–µ –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç",
                "desc": "–ó–¥–∞—Ç–Ω—ñ—Å—Ç—å —Ä–æ–∑—É–º—ñ—Ç–∏ 3D-—Å—Ç—Ä—É–∫—Ç—É—Ä—É, –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤—ñ –≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è, —Ñ—ñ–∑–∏–∫—É —Ç–∞ —Ñ—ñ–∑–∏—á–Ω—É –ø—Ä–∏—á–∏–Ω–Ω—ñ—Å—Ç—å. –õ—é–¥–∏ —Ä–æ–±–ª—è—Ç—å —Ü–µ –ø—Ä–∏—Ä–æ–¥–Ω–æ ‚Äî –®–Ü –ø–æ–≤–∏–Ω–µ–Ω –≤—á–∏—Ç–∏—Å—è –∑ –¥–∞–Ω–∏—Ö. –§–µ–π-–§–µ–π –õ—ñ –Ω–∞–∑–∏–≤–∞—î —Ü–µ \"–Ω–∞—Å—Ç—É–ø–Ω–∏–º —Ñ—Ä–æ–Ω—Ç—ñ—Ä–æ–º –®–Ü.\"",
                "links": []
              },
              {
                "text": "–í—ñ–¥ –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É –¥–æ 3D-—Ä–æ–∑—É–º—ñ–Ω–Ω—è",
                "desc": "–ï–≤–æ–ª—é—Ü—ñ—è –≤—ñ–¥ 2D-—Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å (–µ—Ä–∞ ImageNet) –¥–æ 3D-—Ä–æ–∑—É–º—ñ–Ω–Ω—è —Å—Ü–µ–Ω. NeRF —Ç–∞ Gaussian Splatting —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É—é—é—Ç—å 3D-—Å—Ü–µ–Ω–∏ –∑ —Ñ–æ—Ç–æ. –û—Ü—ñ–Ω–∫–∞ –≥–ª–∏–±–∏–Ω–∏ —Ç–∞ –≥—Ä–∞—Ñ–∏ —Å—Ü–µ–Ω –≤—ñ–¥–æ–±—Ä–∞–∂–∞—é—Ç—å –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤—ñ –≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å",
                    "href": "../level-1/multimodality.html"
                  }
                ]
              },
              {
                "text": "3D-–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è",
                "desc": "–®–Ü –≥–µ–Ω–µ—Ä—É—î 3D-–æ–±'—î–∫—Ç–∏ —Ç–∞ —Å—Ü–µ–Ω–∏ –∑ —Ç–µ–∫—Å—Ç—É –∞–±–æ –∑–æ–±—Ä–∞–∂–µ–Ω—å. Point-E, Shap-E (OpenAI), DreamFusion (Google), Meshy —Å—Ç–≤–æ—Ä—é—é—Ç—å 3D-–∞—Å—Å–µ—Ç–∏. –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è: —ñ–≥—Ä–∏, –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞, –¥–∏–∑–∞–π–Ω –ø—Ä–æ–¥—É–∫—Ç—ñ–≤, –≤—ñ—Ä—Ç—É–∞–ª—å–Ω—ñ —Å–≤—ñ—Ç–∏.",
                "links": [
                  {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "–°–∏–º—É–ª—è—Ü—ñ—è —Ñ—ñ–∑–∏–∫–∏",
                "desc": "–®–Ü –≤—á–∏—Ç—å —Ñ—ñ–∑–∏—á–Ω—É –¥–∏–Ω–∞–º—ñ–∫—É: —è–∫ –æ–±'—î–∫—Ç–∏ –ø–∞–¥–∞—é—Ç—å, –∑—ñ—à—Ç–æ–≤—Ö—É—é—Ç—å—Å—è, –¥–µ—Ñ–æ—Ä–º—É—é—Ç—å—Å—è —Ç–∞ –≤–∑–∞—î–º–æ–¥—ñ—é—Ç—å. –î–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–π–æ–≤–∞–Ω—ñ —Ñ—ñ–∑–∏—á–Ω—ñ –¥–≤–∏–∂–∫–∏ –∫–æ–º–±—ñ–Ω—É—é—Ç—å —Ç—Ä–∞–¥–∏—Ü—ñ–π–Ω—É —Å–∏–º—É–ª—è—Ü—ñ—é –∑ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂–∞–º–∏.",
                "links": []
              },
              {
                "text": "–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω—ñ–∫–∞ —Ç–∞ –≤—Ç—ñ–ª–µ–Ω–∏–π –®–Ü",
                "desc": "–†–æ–±–æ—Ç–∏ —â–æ —Å–ø—Ä–∏–π–º–∞—é—Ç—å, —Ä–æ–∑—É–º—ñ—é—Ç—å —Ç–∞ –¥—ñ—é—Ç—å —É —Ñ—ñ–∑–∏—á–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ. –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω—ñ–∫–∏ (RT-2, Octo) –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–≤–∞-–≤—ñ–∑—ñ—è-–¥—ñ—è. –í–∏–∫–ª–∏–∫: –ø–æ–¥–æ–ª–∞–Ω–Ω—è —Ä–æ–∑—Ä–∏–≤—É –º—ñ–∂ –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è–º —Ç–∞ —Ñ—ñ–∑–∏—á–Ω–∏–º –¥–µ–ø–ª–æ—î–º.",
                "links": [
                  {
                    "title": "–ê–≥–µ–Ω—Ç–∏",
                    "href": "../level-4/agents.html"
                  }
                ]
              },
              {
                "text": "–ê–≤—Ç–æ–Ω–æ–º–Ω–∞ –Ω–∞–≤—ñ–≥–∞—Ü—ñ—è",
                "desc": "–ë–µ–∑–ø—ñ–ª–æ—Ç–Ω—ñ –∞–≤—Ç–æ–º–æ–±—ñ–ª—ñ, –¥—Ä–æ–Ω–∏ —Ç–∞ —Ä–æ–±–æ—Ç–∏-–¥–æ—Å—Ç–∞–≤–Ω–∏–∫–∏ –Ω–∞–≤—ñ–≥—É—é—Ç—å —Å–∫–ª–∞–¥–Ω–∏–º–∏ 3D-—Å–µ—Ä–µ–¥–æ–≤–∏—â–∞–º–∏. –ö–æ–º–±—ñ–Ω–∞—Ü—ñ—è —Å–ø—Ä–∏–π–Ω—è—Ç—Ç—è (–∫–∞–º–µ—Ä–∏, LiDAR), –∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ—É–≤–∞–Ω–Ω—è (SLAM), –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Ä—ñ—à–µ–Ω—å —É —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ.",
                "links": []
              },
              {
                "text": "–ú–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–æ–≥–æ –®–Ü",
                "desc": "–°–∏—Å—Ç–µ–º–∏ –®–Ü —â–æ –±—É–¥—É—é—Ç—å –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –º–æ–¥–µ–ª—ñ —Ä–æ–±–æ—Ç–∏ —Ñ—ñ–∑–∏—á–Ω–æ–≥–æ —Å–≤—ñ—Ç—É. –ü–µ—Ä–µ–¥–±–∞—á–∞—é—Ç—å \"—â–æ –±—É–¥–µ –¥–∞–ª—ñ\" —É —Ñ—ñ–∑–∏—á–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—è—Ö. –ö—Ä–∏—Ç–∏—á–Ω—ñ –¥–ª—è –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è —Ñ—ñ–∑–∏—á–Ω–∏—Ö –¥—ñ–π —Ç–∞ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –Ω–∞—Å–ª—ñ–¥–∫—ñ–≤ –ø–µ—Ä–µ–¥ –¥—ñ—è–º–∏.",
                "links": [
                  {
                    "title": "–ó–∞–≥–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å —Å–≤—ñ—Ç—É",
                    "href": "world-model.html"
                  }
                ]
              },
              {
                "text": "AR/VR —Ç–∞ –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è",
                "desc": "Apple Vision Pro, Meta Quest —Ç–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏ –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏—Ö –æ–±—á–∏—Å–ª–µ–Ω—å –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –®–Ü —â–æ —Ä–æ–∑—É–º—ñ—î 3D-–ø—Ä–æ—Å—Ç—ñ—Ä. –†–µ–∞–ª-—Ç–∞–π–º —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –æ–±'—î–∫—Ç—ñ–≤, —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Å—Ü–µ–Ω, —Ç—Ä–µ–∫—ñ–Ω–≥ —Ä—É–∫ ‚Äî –≤—Å–µ –≤–∏–º–∞–≥–∞—î –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.",
                "links": []
              },
              {
                "text": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–µ –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è",
                "desc": "–ö–æ–º–±—ñ–Ω–∞—Ü—ñ—è –º–æ–≤–∏ –∑ –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏–º –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è–º: \"–ø–æ—Å—Ç–∞–≤ —á–∞—à–∫—É –Ω–∞ —Å—Ç—ñ–ª –∑–ª—ñ–≤–∞ –≤—ñ–¥ –∫–Ω–∏–≥–∏.\" –í–∏–º–∞–≥–∞—î –∑–∞–∑–µ–º–ª–µ–Ω–Ω—è –º–æ–≤–∏ —É 3D-–ø—Ä–æ—Å—Ç–æ—Ä—ñ. –ú–æ–¥–µ–ª—ñ SpatialVLM —Ç–∞ 3D-LLM –ø–æ—î–¥–Ω—É—é—Ç—å –º–æ–≤—É —Ç–∞ –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è.",
                "links": []
              },
              {
                "text": "–†–æ–∑—Ä–∏–≤ —â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–∞–∫—Ä–∏—Ç–∏",
                "desc": "2-—Ä—ñ—á–Ω–∞ –¥–∏—Ç–∏–Ω–∞ –º–∞—î –∫—Ä–∞—â–µ –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –Ω—ñ–∂ –Ω–∞–π–ø—Ä–æ—Å—É–Ω—É—Ç—ñ—à–∏–π –®–Ü. –ó–∞–∫—Ä–∏—Ç—Ç—è —Ü—å–æ–≥–æ —Ä–æ–∑—Ä–∏–≤—É –≤–∏–º–∞–≥–∞—î –∫—Ä–∞—â–∏—Ö –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä (–º–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É, –≤—Ç—ñ–ª–µ–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è) —Ç–∞ –∫—Ä–∞—â–∏—Ö –¥–∞–Ω–∏—Ö (—Ä–µ–∞–ª—å–Ω–∞ –≤–∑–∞—î–º–æ–¥—ñ—è —É –º–∞—Å—à—Ç–∞–±—ñ).",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "NeRF",
                "def": "Neural Radiance Fields ‚Äî AI technique for reconstructing photorealistic 3D scenes from 2D photos."
              },
              {
                "term": "Embodied AI",
                "def": "AI systems with physical bodies (robots) that learn through real-world interaction."
              },
              {
                "term": "SLAM",
                "def": "Simultaneous Localization and Mapping ‚Äî building a map of an environment while tracking location within it."
              },
              {
                "term": "Sim-to-Real",
                "def": "Transferring AI skills learned in simulation to real-world physical environments."
              }
            ],
            "uk": [
              {
                "term": "NeRF",
                "def": "Neural Radiance Fields ‚Äî —Ç–µ—Ö–Ω—ñ–∫–∞ –®–Ü –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó —Ñ–æ—Ç–æ—Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–∏—Ö 3D-—Å—Ü–µ–Ω –∑ 2D-—Ñ–æ—Ç–æ."
              },
              {
                "term": "–í—Ç—ñ–ª–µ–Ω–∏–π –®–Ü",
                "def": "–°–∏—Å—Ç–µ–º–∏ –®–Ü –∑ —Ñ—ñ–∑–∏—á–Ω–∏–º–∏ —Ç—ñ–ª–∞–º–∏ (—Ä–æ–±–æ—Ç–∏), —â–æ –≤—á–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ —Ä–µ–∞–ª—å–Ω—É –≤–∑–∞—î–º–æ–¥—ñ—é."
              },
              {
                "term": "SLAM",
                "def": "–û–¥–Ω–æ—á–∞—Å–Ω–∞ –ª–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–∞ –∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ—É–≤–∞–Ω–Ω—è ‚Äî –ø–æ–±—É–¥–æ–≤–∞ –∫–∞—Ä—Ç–∏ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –∑ –æ–¥–Ω–æ—á–∞—Å–Ω–∏–º –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è–º –ø–æ–∑–∏—Ü—ñ—ó."
              },
              {
                "term": "Sim-to-Real",
                "def": "–ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–Ω—è –Ω–∞–≤–∏—á–æ–∫ –®–Ü –∑ —Å–∏–º—É–ª—è—Ü—ñ—ó —É —Ä–µ–∞–ª—å–Ω–µ —Ñ—ñ–∑–∏—á–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ."
              }
            ]
          },
          "tips": {
            "en": [
              "Spatial intelligence is where AI meets the physical world ‚Äî watch robotics companies for the most impactful near-term advances",
              "Try 3D generation tools (Meshy, TripoSR) to experience spatial AI capabilities firsthand",
              "The sim-to-real gap is the biggest bottleneck in robotics AI ‚Äî if you work in this field, focus on bridging it"
            ],
            "uk": [
              "–ü—Ä–æ—Å—Ç–æ—Ä–æ–≤–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç ‚Äî –¥–µ –®–Ü –∑—É—Å—Ç—Ä—ñ—á–∞—î —Ñ—ñ–∑–∏—á–Ω–∏–π —Å–≤—ñ—Ç: —Å–ª—ñ–¥–∫—É–π—Ç–µ –∑–∞ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω—ñ—á–Ω–∏–º–∏ –∫–æ–º–ø–∞–Ω—ñ—è–º–∏ –¥–ª—è –Ω–∞–π–≤–ø–ª–∏–≤–æ–≤—ñ—à–∏—Ö –Ω–∞–π–±–ª–∏–∂—á–∏—Ö –ø—Ä–æ—Ä–∏–≤—ñ–≤",
              "–°–ø—Ä–æ–±—É–π—Ç–µ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ 3D-–≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó (Meshy, TripoSR) —â–æ–± –≤—ñ–¥—á—É—Ç–∏ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤–æ–≥–æ –®–Ü –Ω–∞ –≤–ª–∞—Å–Ω–æ–º—É –¥–æ—Å–≤—ñ–¥—ñ",
              "–†–æ–∑—Ä–∏–≤ sim-to-real ‚Äî –Ω–∞–π–±—ñ–ª—å—à–µ –≤—É–∑—å–∫–µ –º—ñ—Å—Ü–µ –≤ –®–Ü –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω—ñ–∫–∏: —è–∫—â–æ –ø—Ä–∞—Ü—é—î—Ç–µ —É —Ü—ñ–π —Å—Ñ–µ—Ä—ñ, –∑–æ—Å–µ—Ä–µ–¥—å—Ç–µ—Å—å –Ω–∞ –ø–æ–¥–æ–ª–∞–Ω–Ω—ñ –π–æ–≥–æ"
            ]
          },
          "related": [
            "Video Content"
          ]
        },
        {
          "slug": "world-model",
          "title": {
            "en": "General World Model",
            "uk": "–ó–∞–≥–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å —Å–≤—ñ—Ç—É"
          },
          "desc": {
            "en": "AI systems that build internal representations of how the world works.",
            "uk": "–°–∏—Å—Ç–µ–º–∏ –®–Ü, —â–æ –±—É–¥—É—é—Ç—å –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ —É—è–≤–ª–µ–Ω–Ω—è –ø—Ä–æ —Ç–µ, —è–∫ –ø—Ä–∞—Ü—é—î —Å–≤—ñ—Ç."
          },
          "overview": {
            "en": [
              "A world model is an AI system's internal representation of how the world works ‚Äî enabling it to predict outcomes, plan actions, and reason about causality. Yann LeCun argues that current LLMs lack true world models and that building them is the key to achieving human-level AI. Without a world model, AI can only pattern-match on training data rather than truly \"understand.\"",
              "World models allow an agent to simulate \"what would happen if...\" before taking action ‚Äî the foundation of planning and common sense. Humans do this constantly: you can predict that a glass will break if dropped, even if you have never seen that specific glass. Building this capability into AI is one of the grand challenges of the field."
            ],
            "uk": [
              "–ú–æ–¥–µ–ª—å —Å–≤—ñ—Ç—É ‚Äî —Ü–µ –≤–Ω—É—Ç—Ä—ñ—à–Ω—î —É—è–≤–ª–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–∏ –®–Ü –ø—Ä–æ —Ç–µ, —è–∫ –ø—Ä–∞—Ü—é—î —Å–≤—ñ—Ç ‚Äî —â–æ –¥–æ–∑–≤–æ–ª—è—î –ø–µ—Ä–µ–¥–±–∞—á–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏, –ø–ª–∞–Ω—É–≤–∞—Ç–∏ –¥—ñ—ó —Ç–∞ –º—ñ—Ä–∫—É–≤–∞—Ç–∏ –ø—Ä–æ –ø—Ä–∏—á–∏–Ω–Ω—ñ—Å—Ç—å. –Ø–Ω –õ–µ–∫—É–Ω –∞—Ä–≥—É–º–µ–Ω—Ç—É—î —â–æ –ø–æ—Ç–æ—á–Ω—ñ LLM –Ω–µ –º–∞—é—Ç—å —Å–ø—Ä–∞–≤–∂–Ω—ñ—Ö –º–æ–¥–µ–ª–µ–π —Å–≤—ñ—Ç—É —ñ —ó—Ö –ø–æ–±—É–¥–æ–≤–∞ ‚Äî –∫–ª—é—á –¥–æ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è –®–Ü –ª—é–¥—Å—å–∫–æ–≥–æ —Ä—ñ–≤–Ω—è. –ë–µ–∑ –º–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É –®–Ü –º–æ–∂–µ –ª–∏—à–µ –∑—ñ—Å—Ç–∞–≤–ª—è—Ç–∏ –ø–∞—Ç–µ—Ä–Ω–∏ –∑ –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö, –∞ –Ω–µ —Å–ø—Ä–∞–≤–¥—ñ \"—Ä–æ–∑—É–º—ñ—Ç–∏.\"",
              "–ú–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É –¥–æ–∑–≤–æ–ª—è—é—Ç—å –∞–≥–µ–Ω—Ç—É —Å–∏–º—É–ª—é–≤–∞—Ç–∏ \"—â–æ –±—É–¥–µ —è–∫—â–æ...\" –ø–µ—Ä–µ–¥ –¥—ñ—î—é ‚Äî –æ—Å–Ω–æ–≤–∞ –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ –∑–¥–æ—Ä–æ–≤–æ–≥–æ –≥–ª—É–∑–¥—É. –õ—é–¥–∏ —Ä–æ–±–ª—è—Ç—å —Ü–µ –ø–æ—Å—Ç—ñ–π–Ω–æ: –≤–∏ –º–æ–∂–µ—Ç–µ –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ —â–æ —Å–∫–ª—è–Ω–∫–∞ —Ä–æ–∑—ñ–±'—î—Ç—å—Å—è –ø—Ä–∏ –ø–∞–¥—ñ–Ω–Ω—ñ, –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –Ω—ñ–∫–æ–ª–∏ –Ω–µ –±–∞—á–∏–ª–∏ —Ü—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É —Å–∫–ª—è–Ω–∫—É. –ü–æ–±—É–¥–æ–≤–∞ —Ü—ñ—î—ó –∑–¥–∞—Ç–Ω–æ—Å—Ç—ñ –≤ –®–Ü ‚Äî –æ–¥–∏–Ω –∑ –≤–µ–ª–∏–∫–∏—Ö –≤–∏–∫–ª–∏–∫—ñ–≤ –≥–∞–ª—É–∑—ñ."
            ]
          },
          "details": {
            "en": [
              {
                "text": "What Is a World Model",
                "desc": "An internal simulation of reality that enables prediction, planning, and causal reasoning. Humans have rich world models ‚Äî we understand physics, social dynamics, and cause-effect intuitively. AI world models aim to replicate this.",
                "links": []
              },
              {
                "text": "LeCun's JEPA Architecture",
                "desc": "Yann LeCun proposes Joint Embedding Predictive Architecture (JEPA) as the path to world models. Instead of predicting pixels, JEPA predicts abstract representations of future states. This avoids the complexity of pixel-level prediction.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Do LLMs Have World Models?",
                "desc": "Heated debate: some research shows LLMs develop internal spatial and temporal representations (Othello-GPT, linear probes). Critics argue these are statistical patterns, not true understanding. The truth likely lies in between.",
                "links": [
                  {
                    "title": "Reasoning",
                    "href": "../level-1/reasoning.html"
                  }
                ]
              },
              {
                "text": "Video Prediction as World Modeling",
                "desc": "Predicting future video frames requires understanding physics, object permanence, and causality. Sora (OpenAI), Runway Gen-3, and similar models demonstrate implicit physics understanding through video generation.",
                "links": [
                  {
                    "title": "Generative AI",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "Physics Engines vs Learned Models",
                "desc": "Traditional approach: hand-coded physics rules (Unity, Unreal). New approach: learned physics from data (neural physics engines). Hybrid: combining traditional physics with neural networks for robustness.",
                "links": []
              },
              {
                "text": "Planning with World Models",
                "desc": "If you can simulate consequences, you can plan: try actions in simulation, observe predicted outcomes, choose the best. Model-based RL (MuZero, Dreamer) uses learned world models for efficient planning.",
                "links": [
                  {
                    "title": "Agents",
                    "href": "../level-4/agents.html"
                  }
                ]
              },
              {
                "text": "Common Sense Reasoning",
                "desc": "Understanding that objects fall down, water is wet, and people have feelings ‚Äî the \"easy\" things that are hardest for AI. World models are considered essential for common sense, which remains a major weakness of current AI.",
                "links": []
              },
              {
                "text": "Implicit vs Explicit World Models",
                "desc": "Implicit: knowledge encoded in network weights (LLMs may have this). Explicit: a separate, queryable model of the world. LeCun and others argue explicit world models are needed for robust reasoning and planning.",
                "links": []
              },
              {
                "text": "Multimodal World Models",
                "desc": "True world models must integrate vision, language, sound, and physical interaction. A world model that only processes text cannot understand physics. Multimodal approaches (Gemini, GPT-4V) move toward integrated understanding.",
                "links": [
                  {
                    "title": "Multimodality",
                    "href": "../level-1/multimodality.html"
                  }
                ]
              },
              {
                "text": "The Path Forward",
                "desc": "Combining LLM reasoning with learned physics, embodied experience, and abstract representation learning. World models may not arrive as a single breakthrough but as a gradual integration of capabilities across AI systems.",
                "links": [
                  {
                    "title": "AGI",
                    "href": "agi.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–©–æ —Ç–∞–∫–µ –º–æ–¥–µ–ª—å —Å–≤—ñ—Ç—É",
                "desc": "–í–Ω—É—Ç—Ä—ñ—à–Ω—è —Å–∏–º—É–ª—è—Ü—ñ—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è, –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ –ø—Ä–∏—á–∏–Ω–Ω–æ–≥–æ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è. –õ—é–¥–∏ –º–∞—é—Ç—å –±–∞–≥–∞—Ç—ñ –º–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É ‚Äî –º–∏ —ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω–æ —Ä–æ–∑—É–º—ñ—î–º–æ —Ñ—ñ–∑–∏–∫—É, —Å–æ—Ü—ñ–∞–ª—å–Ω—É –¥–∏–Ω–∞–º—ñ–∫—É —Ç–∞ –ø—Ä–∏—á–∏–Ω—É-–Ω–∞—Å–ª—ñ–¥–æ–∫. –ú–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É –®–Ü –ø—Ä–∞–≥–Ω—É—Ç—å —Ü–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç–∏.",
                "links": []
              },
              {
                "text": "–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ JEPA –õ–µ–∫—É–Ω–∞",
                "desc": "–Ø–Ω –õ–µ–∫—É–Ω –ø—Ä–æ–ø–æ–Ω—É—î Joint Embedding Predictive Architecture (JEPA) —è–∫ —à–ª—è—Ö –¥–æ –º–æ–¥–µ–ª–µ–π —Å–≤—ñ—Ç—É. –ó–∞–º—ñ—Å—Ç—å –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –ø—ñ–∫—Å–µ–ª—ñ–≤, JEPA –ø–µ—Ä–µ–¥–±–∞—á–∞—î –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ñ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è –º–∞–π–±—É—Ç–Ω—ñ—Ö —Å—Ç–∞–Ω—ñ–≤, —É–Ω–∏–∫–∞—é—á–∏ —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ –ø—ñ–∫—Å–µ–ª—å–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "–ß–∏ –º–∞—é—Ç—å LLM –º–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É?",
                "desc": "–ì–∞—Ä—è—á–∞ –¥–∏—Å–∫—É—Å—ñ—è: –¥–µ—è–∫—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –ø–æ–∫–∞–∑—É—é—Ç—å —â–æ LLM —Ä–æ–∑–≤–∏–≤–∞—é—Ç—å –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –ø—Ä–æ—Å—Ç–æ—Ä–æ–≤—ñ —Ç–∞ —á–∞—Å–æ–≤—ñ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è (Othello-GPT). –ö—Ä–∏—Ç–∏–∫–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—É—é—Ç—å —â–æ —Ü–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω—ñ –ø–∞—Ç–µ—Ä–Ω–∏, –Ω–µ —Å–ø—Ä–∞–≤–∂–Ω—î —Ä–æ–∑—É–º—ñ–Ω–Ω—è. –Ü—Å—Ç–∏–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω–æ –ø–æ—Å–µ—Ä–µ–¥–∏–Ω—ñ.",
                "links": [
                  {
                    "title": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                    "href": "../level-1/reasoning.html"
                  }
                ]
              },
              {
                "text": "–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –≤—ñ–¥–µ–æ —è–∫ –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è —Å–≤—ñ—Ç—É",
                "desc": "–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–∞–π–±—É—Ç–Ω—ñ—Ö –∫–∞–¥—Ä—ñ–≤ –≤—ñ–¥–µ–æ –≤–∏–º–∞–≥–∞—î —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ñ—ñ–∑–∏–∫–∏, –ø–æ—Å—Ç—ñ–π–Ω–æ—Å—Ç—ñ –æ–±'—î–∫—Ç—ñ–≤ —Ç–∞ –ø—Ä–∏—á–∏–Ω–Ω–æ—Å—Ç—ñ. Sora (OpenAI), Runway Gen-3 –¥–µ–º–æ–Ω—Å—Ç—Ä—É—é—Ç—å —ñ–º–ø–ª—ñ—Ü–∏—Ç–Ω–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ñ—ñ–∑–∏–∫–∏ —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é –≤—ñ–¥–µ–æ.",
                "links": [
                  {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –®–Ü",
                    "href": "../level-1/generative-ai.html"
                  }
                ]
              },
              {
                "text": "–§—ñ–∑–∏—á–Ω—ñ –¥–≤–∏–∂–∫–∏ –ø—Ä–æ—Ç–∏ –Ω–∞–≤—á–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π",
                "desc": "–¢—Ä–∞–¥–∏—Ü—ñ–π–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥: –∑–∞–∫–æ–¥–æ–≤–∞–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞ —Ñ—ñ–∑–∏–∫–∏ (Unity, Unreal). –ù–æ–≤–∏–π: –Ω–∞–≤—á–µ–Ω–∞ —Ñ—ñ–∑–∏–∫–∞ –∑ –¥–∞–Ω–∏—Ö (–Ω–µ–π—Ä–æ–Ω–Ω—ñ —Ñ—ñ–∑–∏—á–Ω—ñ –¥–≤–∏–∂–∫–∏). –ì—ñ–±—Ä–∏–¥: –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è —Ç—Ä–∞–¥–∏—Ü—ñ–π–Ω–æ—ó —Ñ—ñ–∑–∏–∫–∏ –∑ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂–∞–º–∏ –¥–ª—è —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—ñ.",
                "links": []
              },
              {
                "text": "–ü–ª–∞–Ω—É–≤–∞–Ω–Ω—è –∑ –º–æ–¥–µ–ª—è–º–∏ —Å–≤—ñ—Ç—É",
                "desc": "–Ø–∫—â–æ –º–æ–∂–µ—Ç–µ —Å–∏–º—É–ª—é–≤–∞—Ç–∏ –Ω–∞—Å–ª—ñ–¥–∫–∏, –º–æ–∂–µ—Ç–µ –ø–ª–∞–Ω—É–≤–∞—Ç–∏: —Å–ø—Ä–æ–±—É–π—Ç–µ –¥—ñ—ó –≤ —Å–∏–º—É–ª—è—Ü—ñ—ó, —Å–ø–æ—Å—Ç–µ—Ä—ñ–≥–∞–π—Ç–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏, –æ–±–µ—Ä—ñ—Ç—å –Ω–∞–π–∫—Ä–∞—â–∏–π. Model-based RL (MuZero, Dreamer) –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –Ω–∞–≤—á–µ–Ω—ñ –º–æ–¥–µ–ª—ñ –¥–ª—è –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–ê–≥–µ–Ω—Ç–∏",
                    "href": "../level-4/agents.html"
                  }
                ]
              },
              {
                "text": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è –∑–¥–æ—Ä–æ–≤–æ–≥–æ –≥–ª—É–∑–¥—É",
                "desc": "–†–æ–∑—É–º—ñ–Ω–Ω—è —â–æ –æ–±'—î–∫—Ç–∏ –ø–∞–¥–∞—é—Ç—å –≤–Ω–∏–∑, –≤–æ–¥–∞ –º–æ–∫—Ä–∞, –ª—é–¥–∏ –º–∞—é—Ç—å –ø–æ—á—É—Ç—Ç—è ‚Äî \"–ø—Ä–æ—Å—Ç—ñ\" —Ä–µ—á—ñ —â–æ –Ω–∞–π—Å–∫–ª–∞–¥–Ω—ñ—à—ñ –¥–ª—è –®–Ü. –ú–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É –≤–≤–∞–∂–∞—é—Ç—å—Å—è –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–º–∏ –¥–ª—è –∑–¥–æ—Ä–æ–≤–æ–≥–æ –≥–ª—É–∑–¥—É, —â–æ –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è —Å–ª–∞–±–∫—ñ—Å—Ç—é –ø–æ—Ç–æ—á–Ω–æ–≥–æ –®–Ü.",
                "links": []
              },
              {
                "text": "–Ü–º–ø–ª—ñ—Ü–∏—Ç–Ω—ñ —Ç–∞ –µ–∫—Å–ø–ª—ñ—Ü–∏—Ç–Ω—ñ –º–æ–¥–µ–ª—ñ",
                "desc": "–Ü–º–ø–ª—ñ—Ü–∏—Ç–Ω—ñ: –∑–Ω–∞–Ω–Ω—è –∑–∞–∫–æ–¥–æ–≤–∞–Ω—ñ —É –≤–∞–≥–∞—Ö –º–µ—Ä–µ–∂—ñ (LLM –º–æ–∂—É—Ç—å –º–∞—Ç–∏). –ï–∫—Å–ø–ª—ñ—Ü–∏—Ç–Ω—ñ: –æ–∫—Ä–µ–º–∞, –∑–∞–ø–∏—Ç—É–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å —Å–≤—ñ—Ç—É. –õ–µ–∫—É–Ω –∞—Ä–≥—É–º–µ–Ω—Ç—É—î —â–æ –µ–∫—Å–ø–ª—ñ—Ü–∏—Ç–Ω—ñ –º–æ–¥–µ–ª—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –¥–ª—è —Ä–æ–±–∞—Å—Ç–Ω–æ–≥–æ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è —Ç–∞ –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è.",
                "links": []
              },
              {
                "text": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É",
                "desc": "–°–ø—Ä–∞–≤–∂–Ω—ñ –º–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É –ø–æ–≤–∏–Ω–Ω—ñ —ñ–Ω—Ç–µ–≥—Ä—É–≤–∞—Ç–∏ –∑—ñ—Ä, –º–æ–≤—É, –∑–≤—É–∫ —Ç–∞ —Ñ—ñ–∑–∏—á–Ω—É –≤–∑–∞—î–º–æ–¥—ñ—é. –ú–æ–¥–µ–ª—å —â–æ –æ–±—Ä–æ–±–ª—è—î –ª–∏—à–µ —Ç–µ–∫—Å—Ç –Ω–µ –º–æ–∂–µ —Ä–æ–∑—É–º—ñ—Ç–∏ —Ñ—ñ–∑–∏–∫—É. –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ –ø—ñ–¥—Ö–æ–¥–∏ (Gemini, GPT-4V) —Ä—É—Ö–∞—é—Ç—å—Å—è –¥–æ —ñ–Ω—Ç–µ–≥—Ä–æ–≤–∞–Ω–æ–≥–æ —Ä–æ–∑—É–º—ñ–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ñ—Å—Ç—å",
                    "href": "../level-1/multimodality.html"
                  }
                ]
              },
              {
                "text": "–®–ª—è—Ö –≤–ø–µ—Ä–µ–¥",
                "desc": "–ö–æ–º–±—ñ–Ω–∞—Ü—ñ—è –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è LLM –∑ –Ω–∞–≤—á–µ–Ω–æ—é —Ñ—ñ–∑–∏–∫–æ—é, –≤—Ç—ñ–ª–µ–Ω–∏–º –¥–æ—Å–≤—ñ–¥–æ–º —Ç–∞ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—å. –ú–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É –º–æ–∂—É—Ç—å –ø—Ä–∏–π—Ç–∏ –Ω–µ —è–∫ –æ–¥–∏–Ω –ø—Ä–æ—Ä–∏–≤, –∞ —è–∫ –ø–æ—Å—Ç—É–ø–æ–≤–∞ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π.",
                "links": [
                  {
                    "title": "AGI",
                    "href": "agi.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "World Model",
                "def": "AI's internal representation of reality enabling prediction, planning, and causal reasoning."
              },
              {
                "term": "JEPA",
                "def": "Joint Embedding Predictive Architecture ‚Äî LeCun's proposed approach for building world models that predict abstract states."
              },
              {
                "term": "Common Sense",
                "def": "Intuitive understanding of everyday physics, social dynamics, and cause-effect relationships ‚Äî still a major AI challenge."
              },
              {
                "term": "Model-Based RL",
                "def": "Reinforcement learning using a learned world model to simulate and plan actions before taking them."
              }
            ],
            "uk": [
              {
                "term": "–ú–æ–¥–µ–ª—å —Å–≤—ñ—Ç—É",
                "def": "–í–Ω—É—Ç—Ä—ñ—à–Ω—î —É—è–≤–ª–µ–Ω–Ω—è –®–Ü –ø—Ä–æ —Ä–µ–∞–ª—å–Ω—ñ—Å—Ç—å –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è, –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ –ø—Ä–∏—á–∏–Ω–Ω–æ–≥–æ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è."
              },
              {
                "term": "JEPA",
                "def": "Joint Embedding Predictive Architecture ‚Äî –∑–∞–ø—Ä–æ–ø–æ–Ω–æ–≤–∞–Ω–∏–π –õ–µ–∫—É–Ω–æ–º –ø—ñ–¥—Ö—ñ–¥ –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –º–æ–¥–µ–ª–µ–π —Å–≤—ñ—Ç—É —â–æ –ø–µ—Ä–µ–¥–±–∞—á–∞—é—Ç—å –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ñ —Å—Ç–∞–Ω–∏."
              },
              {
                "term": "–ó–¥–æ—Ä–æ–≤–∏–π –≥–ª—É–∑–¥",
                "def": "–Ü–Ω—Ç—É—ó—Ç–∏–≤–Ω–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –ø–æ–≤—Å—è–∫–¥–µ–Ω–Ω–æ—ó —Ñ—ñ–∑–∏–∫–∏, —Å–æ—Ü—ñ–∞–ª—å–Ω–æ—ó –¥–∏–Ω–∞–º—ñ–∫–∏ —Ç–∞ –ø—Ä–∏—á–∏–Ω–Ω–æ-–Ω–∞—Å–ª—ñ–¥–∫–æ–≤–∏—Ö –∑–≤'—è–∑–∫—ñ–≤ ‚Äî –≤—Å–µ —â–µ –≤–µ–ª–∏–∫–∏–π –≤–∏–∫–ª–∏–∫ –¥–ª—è –®–Ü."
              },
              {
                "term": "Model-Based RL",
                "def": "–ù–∞–≤—á–∞–Ω–Ω—è –∑ –ø—ñ–¥–∫—Ä—ñ–ø–ª–µ–Ω–Ω—è–º –∑ –Ω–∞–≤—á–µ–Ω–æ—é –º–æ–¥–µ–ª–ª—é —Å–≤—ñ—Ç—É –¥–ª—è —Å–∏–º—É–ª—è—Ü—ñ—ó —Ç–∞ –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è –¥—ñ–π –ø–µ—Ä–µ–¥ —ó—Ö –≤–∏–∫–æ–Ω–∞–Ω–Ω—è–º."
              }
            ]
          },
          "tips": {
            "en": [
              "Follow Yann LeCun's research and talks for the most influential thinking on world models and the path to human-level AI",
              "Video generation quality is a proxy for world model capability ‚Äî better video prediction implies better physics understanding",
              "The \"do LLMs understand?\" debate is worth following ‚Äî it has practical implications for how we build and trust AI systems"
            ],
            "uk": [
              "–°–ª—ñ–¥–∫—É–π—Ç–µ –∑–∞ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è–º–∏ —Ç–∞ –≤–∏—Å—Ç—É–ø–∞–º–∏ –Ø–Ω–∞ –õ–µ–∫—É–Ω–∞ –¥–ª—è –Ω–∞–π–≤–ø–ª–∏–≤–æ–≤—ñ—à–æ–≥–æ –º–∏—Å–ª–µ–Ω–Ω—è –ø—Ä–æ –º–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É —Ç–∞ —à–ª—è—Ö –¥–æ –®–Ü –ª—é–¥—Å—å–∫–æ–≥–æ —Ä—ñ–≤–Ω—è",
              "–Ø–∫—ñ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤—ñ–¥–µ–æ ‚Äî –ø—Ä–æ–∫—Å—ñ –¥–ª—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –º–æ–¥–µ–ª—ñ —Å–≤—ñ—Ç—É: –∫—Ä–∞—â–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –≤—ñ–¥–µ–æ –æ–∑–Ω–∞—á–∞—î –∫—Ä–∞—â–µ —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ñ—ñ–∑–∏–∫–∏",
              "–î–µ–±–∞—Ç–∏ \"—á–∏ —Ä–æ–∑—É–º—ñ—é—Ç—å LLM?\" –≤–∞—Ä—Ç—ñ —É–≤–∞–≥–∏ ‚Äî –≤–æ–Ω–∏ –º–∞—é—Ç—å –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –Ω–∞—Å–ª—ñ–¥–∫–∏ –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ —Ç–∞ –¥–æ–≤—ñ—Ä–∏ –¥–æ —Å–∏—Å—Ç–µ–º –®–Ü"
            ]
          },
          "related": [
            "Video Content"
          ]
        },
        {
          "slug": "accelerationists",
          "title": {
            "en": "Techno-Optimists",
            "uk": "–¢–µ—Ö–Ω–æ-–æ–ø—Ç–∏–º—ñ—Å—Ç–∏"
          },
          "desc": {
            "en": "The e/acc movement and arguments for accelerating AI development.",
            "uk": "–†—É—Ö e/acc —Ç–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–∏ –Ω–∞ –∫–æ—Ä–∏—Å—Ç—å –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è —Ä–æ–∑–≤–∏—Ç–∫—É –®–Ü."
          },
          "overview": {
            "en": [
              "Techno-optimists and the Effective Accelerationism (e/acc) movement argue that AI development should be pursued aggressively because the benefits far outweigh the risks. They view technology as the primary driver of human progress and believe that slowing AI development causes more harm than accelerating it ‚Äî delaying cures for diseases, solutions to poverty, and tools for human flourishing.",
              "Marc Andreessen's \"Techno-Optimist Manifesto\" (2023) crystallized this worldview: technology is the solution to most human problems, markets should drive AI development, and regulation primarily benefits incumbents while slowing innovation. The e/acc movement adds that accelerating technology is a moral imperative. Understanding this perspective is important even if you disagree ‚Äî it shapes major investment and policy decisions."
            ],
            "uk": [
              "–¢–µ—Ö–Ω–æ-–æ–ø—Ç–∏–º—ñ—Å—Ç–∏ —Ç–∞ —Ä—É—Ö –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∞–∫—Å–µ–ª–µ—Ä–∞—Ü—ñ–æ–Ω—ñ–∑–º—É (e/acc) –∞—Ä–≥—É–º–µ–Ω—Ç—É—é—Ç—å —â–æ —Ä–æ–∑–≤–∏—Ç–æ–∫ –®–Ü –ø–æ–≤–∏–Ω–µ–Ω –ø–µ—Ä–µ—Å–ª—ñ–¥—É–≤–∞—Ç–∏—Å—è –∞–≥—Ä–µ—Å–∏–≤–Ω–æ, –±–æ –≤–∏–≥–æ–¥–∏ –∑–Ω–∞—á–Ω–æ –ø–µ—Ä–µ–≤–∏—â—É—é—Ç—å —Ä–∏–∑–∏–∫–∏. –í–æ–Ω–∏ —Ä–æ–∑–≥–ª—è–¥–∞—é—Ç—å —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—é —è–∫ –æ—Å–Ω–æ–≤–Ω–∏–π —Ä—É—à—ñ–π –ª—é–¥—Å—å–∫–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—É —Ç–∞ –≤—ñ—Ä—è—Ç—å —â–æ —Å–ø–æ–≤—ñ–ª—å–Ω–µ–Ω–Ω—è —Ä–æ–∑–≤–∏—Ç–∫—É –®–Ü –∑–∞–≤–¥–∞—î –±—ñ–ª—å—à–µ —à–∫–æ–¥–∏ –Ω—ñ–∂ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è ‚Äî –∑–∞—Ç—Ä–∏–º—É—é—á–∏ –ª—ñ–∫–∏ –≤—ñ–¥ —Ö–≤–æ—Ä–æ–±, —Ä—ñ—à–µ–Ω–Ω—è –±—ñ–¥–Ω–æ—Å—Ç—ñ —Ç–∞ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –¥–ª—è –ª—é–¥—Å—å–∫–æ–≥–æ –ø—Ä–æ—Ü–≤—ñ—Ç–∞–Ω–Ω—è.",
              "–ú–∞–Ω—ñ—Ñ–µ—Å—Ç —Ç–µ—Ö–Ω–æ-–æ–ø—Ç–∏–º—ñ—Å—Ç—ñ–≤ –ú–∞—Ä–∫–∞ –ê–Ω–¥—Ä–µ—Å—Å–µ–Ω–∞ (2023) –∫—Ä–∏—Å—Ç–∞–ª—ñ–∑—É–≤–∞–≤ —Ü–µ–π —Å–≤—ñ—Ç–æ–≥–ª—è–¥: —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è ‚Äî —Ä—ñ—à–µ–Ω–Ω—è –±—ñ–ª—å—à–æ—Å—Ç—ñ –ª—é–¥—Å—å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º, —Ä–∏–Ω–∫–∏ –ø–æ–≤–∏–Ω–Ω—ñ —Ä—É—Ö–∞—Ç–∏ —Ä–æ–∑–≤–∏—Ç–æ–∫ –®–Ü, –∞ —Ä–µ–≥—É–ª—é–≤–∞–Ω–Ω—è –ø–µ—Ä–µ–≤–∞–∂–Ω–æ –¥–æ–ø–æ–º–∞–≥–∞—î —ñ–Ω–∫—É–º–±–µ–Ω—Ç–∞–º —Ç–∞ —Å–ø–æ–≤—ñ–ª—å–Ω—é—î —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—ó. –†—É—Ö e/acc –¥–æ–¥–∞—î —â–æ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π ‚Äî –º–æ—Ä–∞–ª—å–Ω–∏–π —ñ–º–ø–µ—Ä–∞—Ç–∏–≤. –†–æ–∑—É–º—ñ–Ω–Ω—è —Ü—ñ—î—ó –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∏ –≤–∞–∂–ª–∏–≤–µ –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –≤–∏ –Ω–µ –∑–≥–æ–¥–Ω—ñ ‚Äî –≤–æ–Ω–∞ —Ñ–æ—Ä–º—É—î —ñ–Ω–≤–µ—Å—Ç–∏—Ü—ñ–π–Ω—ñ —Ç–∞ –ø–æ–ª—ñ—Ç–∏—á–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Effective Accelerationism (e/acc)",
                "desc": "Movement arguing that accelerating technology is a moral imperative. Technology solves more problems than it creates. Slowing down costs lives (delayed medical breakthroughs, climate solutions). Key voices: Guillaume Verdon, Beff Jezos.",
                "links": []
              },
              {
                "text": "The Techno-Optimist Case",
                "desc": "Historical argument: every major technology (electricity, internet, medicine) was met with fear but ultimately improved human life dramatically. AI is the next such technology. Pessimism is a failure of imagination.",
                "links": []
              },
              {
                "text": "Open Source AI Advocacy",
                "desc": "Arguments for open AI models: democratizes access, enables innovation, prevents concentration of power, allows security auditing, and drives progress faster. Meta's Llama releases embody this philosophy.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Andreessen's Manifesto",
                "desc": "Marc Andreessen (2023) argued: technology is the engine of progress, markets optimize for human welfare, regulation protects incumbents, and AI will create more abundance than any technology before it.",
                "links": []
              },
              {
                "text": "Economic Arguments",
                "desc": "AI could add $15+ trillion to the global economy. Delaying AI development means delaying economic growth that lifts billions out of poverty. The opportunity cost of caution may exceed the risk of action.",
                "links": []
              },
              {
                "text": "AI for Scientific Discovery",
                "desc": "AI is accelerating drug discovery, materials science, climate modeling, and fundamental research. Each month of delay potentially costs lives. Accelerationists argue safety concerns must be weighed against this real human cost.",
                "links": []
              },
              {
                "text": "Critique of Safety-ism",
                "desc": "Some accelerationists argue that \"AI safety\" can become a tool for regulatory capture ‚Äî large companies lobby for safety regulations they can afford to comply with, creating barriers that block smaller competitors and open source.",
                "links": []
              },
              {
                "text": "Competition Arguments",
                "desc": "If democratic nations slow AI development, authoritarian regimes will not. Better to lead AI development with democratic values than cede the field. The AI arms race makes unilateral pausing dangerous.",
                "links": []
              },
              {
                "text": "Limits of Optimism",
                "desc": "Critics argue accelerationists underweight genuine risks: job displacement, AI-powered surveillance, bioweapons, and the alignment problem. Unbridled acceleration without safety is reckless, not optimistic.",
                "links": [
                  {
                    "title": "Techno-Pessimists",
                    "href": "doomers.html"
                  }
                ]
              },
              {
                "text": "The Balanced View",
                "desc": "Most AI practitioners support neither pure acceleration nor pure caution. The consensus: develop AI ambitiously but with safety research running in parallel. Progress and safety are not opposites but complements.",
                "links": [
                  {
                    "title": "AI Safety",
                    "href": "ai-safety.html"
                  }
                ]
              }
            ],
            "uk": [
              {
                "text": "–ï—Ñ–µ–∫—Ç–∏–≤–Ω–∏–π –∞–∫—Å–µ–ª–µ—Ä–∞—Ü—ñ–æ–Ω—ñ–∑–º (e/acc)",
                "desc": "–†—É—Ö —â–æ –∞—Ä–≥—É–º–µ–Ω—Ç—É—î –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π —è–∫ –º–æ—Ä–∞–ª—å–Ω–∏–π —ñ–º–ø–µ—Ä–∞—Ç–∏–≤. –¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—è –≤–∏—Ä—ñ—à—É—î –±—ñ–ª—å—à–µ –ø—Ä–æ–±–ª–µ–º –Ω—ñ–∂ —Å—Ç–≤–æ—Ä—é—î. –°–ø–æ–≤—ñ–ª—å–Ω–µ–Ω–Ω—è –∫–æ—à—Ç—É—î –∂–∏—Ç—Ç—ñ–≤ (–∑–∞—Ç—Ä–∏–º–∞–Ω—ñ –º–µ–¥–∏—á–Ω—ñ –ø—Ä–æ—Ä–∏–≤–∏, –∫–ª—ñ–º–∞—Ç–∏—á–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è). –ö–ª—é—á–æ–≤—ñ –≥–æ–ª–æ—Å–∏: –ì—ñ–π–æ–º –í–µ—Ä–¥–æ–Ω, –ë–µ—Ñ—Ñ –î–∂–µ–∑–æ—Å.",
                "links": []
              },
              {
                "text": "–¢–µ—Ö–Ω–æ-–æ–ø—Ç–∏–º—ñ—Å—Ç–∏—á–Ω–∏–π –∞—Ä–≥—É–º–µ–Ω—Ç",
                "desc": "–Ü—Å—Ç–æ—Ä–∏—á–Ω–∏–π –∞—Ä–≥—É–º–µ–Ω—Ç: –∫–æ–∂–Ω–∞ –≤–µ–ª–∏–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è (–µ–ª–µ–∫—Ç—Ä–∏–∫–∞, —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç, –º–µ–¥–∏—Ü–∏–Ω–∞) –∑—É—Å—Ç—Ä—ñ—á–∞–ª–∞—Å—è —Å—Ç—Ä–∞—Ö–æ–º, –∞–ª–µ –≤—Ä–µ—à—Ç—ñ –¥—Ä–∞–º–∞—Ç–∏—á–Ω–æ –ø–æ–∫—Ä–∞—â–∏–ª–∞ –ª—é–¥—Å—å–∫–µ –∂–∏—Ç—Ç—è. –®–Ü ‚Äî –Ω–∞—Å—Ç—É–ø–Ω–∞ —Ç–∞–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è. –ü–µ—Å–∏–º—ñ–∑–º ‚Äî –ø—Ä–æ–≤–∞–ª —É—è–≤–∏.",
                "links": []
              },
              {
                "text": "–ê–¥–≤–æ–∫–∞—Ü—ñ—è –≤—ñ–¥–∫—Ä–∏—Ç–æ–≥–æ –®–Ü",
                "desc": "–ê—Ä–≥—É–º–µ–Ω—Ç–∏ –∑–∞ –≤—ñ–¥–∫—Ä–∏—Ç—ñ –º–æ–¥–µ–ª—ñ –®–Ü: –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑—É—î –¥–æ—Å—Ç—É–ø, —Å—Ç–∏–º—É–ª—é—î —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—ó, –∑–∞–ø–æ–±—ñ–≥–∞—î –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—ó –≤–ª–∞–¥–∏, –¥–æ–∑–≤–æ–ª—è—î –∞—É–¥–∏—Ç –±–µ–∑–ø–µ–∫–∏ —Ç–∞ —Ä—É—Ö–∞—î –ø—Ä–æ–≥—Ä–µ—Å —à–≤–∏–¥—à–µ. –†–µ–ª—ñ–∑–∏ Llama –≤—ñ–¥ Meta –≤—Ç—ñ–ª—é—é—Ç—å —Ü—é —Ñ—ñ–ª–æ—Å–æ—Ñ—ñ—é.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "–ú–∞–Ω—ñ—Ñ–µ—Å—Ç –ê–Ω–¥—Ä–µ—Å—Å–µ–Ω–∞",
                "desc": "–ú–∞—Ä–∫ –ê–Ω–¥—Ä–µ—Å—Å–µ–Ω (2023) –∞—Ä–≥—É–º–µ–Ω—Ç—É–≤–∞–≤: —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è ‚Äî –¥–≤–∏–≥—É–Ω –ø—Ä–æ–≥—Ä–µ—Å—É, —Ä–∏–Ω–∫–∏ –æ–ø—Ç–∏–º—ñ–∑—É—é—Ç—å –¥–ª—è –ª—é–¥—Å—å–∫–æ–≥–æ –¥–æ–±—Ä–æ–±—É—Ç—É, —Ä–µ–≥—É–ª—é–≤–∞–Ω–Ω—è –∑–∞—Ö–∏—â–∞—î —ñ–Ω–∫—É–º–±–µ–Ω—Ç—ñ–≤, –®–Ü —Å—Ç–≤–æ—Ä–∏—Ç—å –±—ñ–ª—å—à–µ –¥–æ—Å—Ç–∞—Ç–∫—É –Ω—ñ–∂ –±—É–¥—å-—è–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è –¥–æ –Ω—å–æ–≥–æ.",
                "links": []
              },
              {
                "text": "–ï–∫–æ–Ω–æ–º—ñ—á–Ω—ñ –∞—Ä–≥—É–º–µ–Ω—Ç–∏",
                "desc": "–®–Ü –º–æ–∂–µ –¥–æ–¥–∞—Ç–∏ $15+ —Ç—Ä–∏–ª—å–π–æ–Ω—ñ–≤ –¥–æ –≥–ª–æ–±–∞–ª—å–Ω–æ—ó –µ–∫–æ–Ω–æ–º—ñ–∫–∏. –ó–∞—Ç—Ä–∏–º–∫–∞ —Ä–æ–∑–≤–∏—Ç–∫—É –®–Ü –æ–∑–Ω–∞—á–∞—î –∑–∞—Ç—Ä–∏–º–∫—É –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ–≥–æ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è —â–æ –ø—ñ–¥–Ω—ñ–º–∞—î –º—ñ–ª—å—è—Ä–¥–∏ –∑ –±—ñ–¥–Ω–æ—Å—Ç—ñ. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –æ–±–µ—Ä–µ–∂–Ω–æ—Å—Ç—ñ –º–æ–∂–µ –ø–µ—Ä–µ–≤–∏—â—É–≤–∞—Ç–∏ —Ä–∏–∑–∏–∫ –¥—ñ–π.",
                "links": []
              },
              {
                "text": "–®–Ü –¥–ª—è –Ω–∞—É–∫–æ–≤–∏—Ö –≤—ñ–¥–∫—Ä–∏—Ç—Ç—ñ–≤",
                "desc": "–®–Ü –ø—Ä–∏—Å–∫–æ—Ä—é—î –≤—ñ–¥–∫—Ä–∏—Ç—Ç—è –ª—ñ–∫—ñ–≤, –º–∞—Ç–µ—Ä—ñ–∞–ª–æ–∑–Ω–∞–≤—Å—Ç–≤–æ, –∫–ª—ñ–º–∞—Ç–∏—á–Ω–µ –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è —Ç–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è. –ö–æ–∂–µ–Ω –º—ñ—Å—è—Ü—å –∑–∞—Ç—Ä–∏–º–∫–∏ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ –∫–æ—à—Ç—É—î –∂–∏—Ç—Ç—ñ–≤. –ê–∫—Å–µ–ª–µ—Ä–∞—Ü—ñ–æ–Ω—ñ—Å—Ç–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—É—é—Ç—å —â–æ –∑–∞–Ω–µ–ø–æ–∫–æ—î–Ω–Ω—è –±–µ–∑–ø–µ–∫–æ—é —Ç—Ä–µ–±–∞ –∑–≤–∞–∂—É–≤–∞—Ç–∏ –ø—Ä–æ—Ç–∏ —Ü—ñ—î—ó —Ä–µ–∞–ª—å–Ω–æ—ó —Ü—ñ–Ω–∏.",
                "links": []
              },
              {
                "text": "–ö—Ä–∏—Ç–∏–∫–∞ –±–µ–∑–ø–µ–∫–æ—Ü–µ–Ω—Ç—Ä–∏–∑–º—É",
                "desc": "–î–µ—è–∫—ñ –∞–∫—Å–µ–ª–µ—Ä–∞—Ü—ñ–æ–Ω—ñ—Å—Ç–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—É—é—Ç—å —â–æ \"–±–µ–∑–ø–µ–∫–∞ –®–Ü\" –º–æ–∂–µ —Å—Ç–∞—Ç–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–æ–≥–æ –∑–∞—Ö–æ–ø–ª–µ–Ω–Ω—è ‚Äî –≤–µ–ª–∏–∫—ñ –∫–æ–º–ø–∞–Ω—ñ—ó –ª–æ–±—ñ—é—é—Ç—å —Ä–µ–≥—É–ª—è—Ü—ñ—ó –±–µ–∑–ø–µ–∫–∏, —è–∫—ñ –º–æ–∂—É—Ç—å —Å–æ–±—ñ –¥–æ–∑–≤–æ–ª–∏—Ç–∏, —Å—Ç–≤–æ—Ä—é—é—á–∏ –±–∞—Ä'—î—Ä–∏ –¥–ª—è –º–∞–ª–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤.",
                "links": []
              },
              {
                "text": "–ê—Ä–≥—É–º–µ–Ω—Ç–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—ó",
                "desc": "–Ø–∫—â–æ –¥–µ–º–æ–∫—Ä–∞—Ç–∏—á–Ω—ñ –Ω–∞—Ü—ñ—ó —Å–ø–æ–≤—ñ–ª—å–Ω—è—Ç—å —Ä–æ–∑–≤–∏—Ç–æ–∫ –®–Ü, –∞–≤—Ç–æ—Ä–∏—Ç–∞—Ä–Ω—ñ —Ä–µ–∂–∏–º–∏ –Ω–µ –±—É–¥—É—Ç—å. –ö—Ä–∞—â–µ –ª—ñ–¥–∏—Ä—É–≤–∞—Ç–∏ –∑ –¥–µ–º–æ–∫—Ä–∞—Ç–∏—á–Ω–∏–º–∏ —Ü—ñ–Ω–Ω–æ—Å—Ç—è–º–∏ –Ω—ñ–∂ –ø–æ—Å—Ç—É–ø–∏—Ç–∏—Å—è –ø–æ–ª–µ–º. –ì–æ–Ω–∫–∞ –æ–∑–±—Ä–æ—î–Ω—å –®–Ü —Ä–æ–±–∏—Ç—å –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω—î –ø—Ä–∏–∑—É–ø–∏–Ω–µ–Ω–Ω—è –Ω–µ–±–µ–∑–ø–µ—á–Ω–∏–º.",
                "links": []
              },
              {
                "text": "–ú–µ–∂—ñ –æ–ø—Ç–∏–º—ñ–∑–º—É",
                "desc": "–ö—Ä–∏—Ç–∏–∫–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—É—é—Ç—å —â–æ –∞–∫—Å–µ–ª–µ—Ä–∞—Ü—ñ–æ–Ω—ñ—Å—Ç–∏ –Ω–µ–¥–æ–æ—Ü—ñ–Ω—é—é—Ç—å —Ä–∏–∑–∏–∫–∏: –≤—Ç—Ä–∞—Ç—É —Ä–æ–±–æ—á–∏—Ö –º—ñ—Å—Ü—å, –®–Ü-–Ω–∞–≥–ª—è–¥, –±—ñ–æ–∑–±—Ä–æ—é —Ç–∞ –ø—Ä–æ–±–ª–µ–º—É –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è. –ù–µ—Å—Ç—Ä–∏–º–Ω–µ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –±–µ–∑ –±–µ–∑–ø–µ–∫–∏ ‚Äî –±–µ–∑—Ä–æ–∑—Å—É–¥–Ω—ñ—Å—Ç—å, –Ω–µ –æ–ø—Ç–∏–º—ñ–∑–º.",
                "links": [
                  {
                    "title": "–¢–µ—Ö–Ω–æ-–ø–µ—Å–∏–º—ñ—Å—Ç–∏",
                    "href": "doomers.html"
                  }
                ]
              },
              {
                "text": "–ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏–π –ø–æ–≥–ª—è–¥",
                "desc": "–ë—ñ–ª—å—à—ñ—Å—Ç—å –ø—Ä–∞–∫—Ç–∏–∫—ñ–≤ –®–Ü –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å –∞–Ω—ñ —á–∏—Å—Ç–µ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è, –∞–Ω—ñ —á–∏—Å—Ç—É –æ–±–µ—Ä–µ–∂–Ω—ñ—Å—Ç—å. –ö–æ–Ω—Å–µ–Ω—Å—É—Å: —Ä–æ–∑–≤–∏–≤–∞—Ç–∏ –®–Ü –∞–º–±—ñ—Ç–Ω–æ, –∞–ª–µ –∑ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è–º–∏ –±–µ–∑–ø–µ–∫–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ. –ü—Ä–æ–≥—Ä–µ—Å —Ç–∞ –±–µ–∑–ø–µ–∫–∞ ‚Äî –Ω–µ –ø—Ä–æ—Ç–∏–ª–µ–∂–Ω–æ—Å—Ç—ñ, –∞ –¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü",
                    "href": "ai-safety.html"
                  }
                ]
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "e/acc",
                "def": "Effective Accelerationism ‚Äî movement arguing that technological acceleration is a moral imperative."
              },
              {
                "term": "Techno-Optimism",
                "def": "The belief that technology, including AI, will ultimately solve more problems than it creates."
              },
              {
                "term": "Regulatory Capture",
                "def": "When safety regulations are shaped by large incumbents to protect their market position against competitors."
              },
              {
                "term": "Open Source AI",
                "def": "AI models with publicly available weights and code, enabling community development and democratized access."
              }
            ],
            "uk": [
              {
                "term": "e/acc",
                "def": "–ï—Ñ–µ–∫—Ç–∏–≤–Ω–∏–π –∞–∫—Å–µ–ª–µ—Ä–∞—Ü—ñ–æ–Ω—ñ–∑–º ‚Äî —Ä—É—Ö —â–æ –∞—Ä–≥—É–º–µ–Ω—Ç—É—î —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–µ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è —è–∫ –º–æ—Ä–∞–ª—å–Ω–∏–π —ñ–º–ø–µ—Ä–∞—Ç–∏–≤."
              },
              {
                "term": "–¢–µ—Ö–Ω–æ-–æ–ø—Ç–∏–º—ñ–∑–º",
                "def": "–í—ñ—Ä–∞ —â–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è, –≤–∫–ª—é—á–Ω–æ –∑ –®–Ü, –≤—Ä–µ—à—Ç—ñ –≤–∏—Ä—ñ—à–∏—Ç—å –±—ñ–ª—å—à–µ –ø—Ä–æ–±–ª–µ–º –Ω—ñ–∂ —Å—Ç–≤–æ—Ä–∏—Ç—å."
              },
              {
                "term": "–†–µ–≥—É–ª—è—Ç–æ—Ä–Ω–µ –∑–∞—Ö–æ–ø–ª–µ–Ω–Ω—è",
                "def": "–ö–æ–ª–∏ —Ä–µ–≥—É–ª—è—Ü—ñ—ó –±–µ–∑–ø–µ–∫–∏ —Ñ–æ—Ä–º—É—é—Ç—å—Å—è –≤–µ–ª–∏–∫–∏–º–∏ —ñ–Ω–∫—É–º–±–µ–Ω—Ç–∞–º–∏ –¥–ª—è –∑–∞—Ö–∏—Å—Ç—É —Ä–∏–Ω–∫–æ–≤–æ—ó –ø–æ–∑–∏—Ü—ñ—ó –≤—ñ–¥ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤."
              },
              {
                "term": "Open Source –®–Ü",
                "def": "–ú–æ–¥–µ–ª—ñ –®–Ü –∑ –ø—É–±–ª—ñ—á–Ω–æ –¥–æ—Å—Ç—É–ø–Ω–∏–º–∏ –≤–∞–≥–∞–º–∏ —Ç–∞ –∫–æ–¥–æ–º –¥–ª—è —Ä–æ–∑–≤–∏—Ç–∫—É —Å–ø—ñ–ª—å–Ω–æ—Ç–æ—é —Ç–∞ –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–æ–≤–∞–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø—É."
              }
            ]
          },
          "tips": {
            "en": [
              "Understand the accelerationist arguments even if you disagree ‚Äî they influence major investment, hiring, and policy decisions",
              "The open-source AI debate has practical implications: following it helps you predict which models will be available and how",
              "The strongest position combines optimism about AI potential with serious engagement on safety ‚Äî pure optimism or pure fear are both incomplete"
            ],
            "uk": [
              "–†–æ–∑—É–º—ñ–π—Ç–µ –∞—Ä–≥—É–º–µ–Ω—Ç–∏ –∞–∫—Å–µ–ª–µ—Ä–∞—Ü—ñ–æ–Ω—ñ—Å—Ç—ñ–≤ –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –Ω–µ –∑–≥–æ–¥–Ω—ñ ‚Äî –≤–æ–Ω–∏ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ —ñ–Ω–≤–µ—Å—Ç–∏—Ü—ñ—ó, –Ω–∞–π–º–∞–Ω–Ω—è —Ç–∞ –ø–æ–ª—ñ—Ç–∏—á–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è",
              "–î–µ–±–∞—Ç–∏ –ø—Ä–æ open-source –®–Ü –º–∞—é—Ç—å –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –Ω–∞—Å–ª—ñ–¥–∫–∏: —Å–ª—ñ–¥–∫—É–≤–∞–Ω–Ω—è –¥–æ–ø–æ–º–∞–≥–∞—î –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ —è–∫—ñ –º–æ–¥–µ–ª—ñ –±—É–¥—É—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ñ",
              "–ù–∞–π—Å–∏–ª—å–Ω—ñ—à–∞ –ø–æ–∑–∏—Ü—ñ—è –ø–æ—î–¥–Ω—É—î –æ–ø—Ç–∏–º—ñ–∑–º —â–æ–¥–æ –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª—É –®–Ü –∑ —Å–µ—Ä–π–æ–∑–Ω–∏–º –∑–∞–ª—É—á–µ–Ω–Ω—è–º –¥–æ –±–µ–∑–ø–µ–∫–∏ ‚Äî —á–∏—Å—Ç–∏–π –æ–ø—Ç–∏–º—ñ–∑–º —á–∏ —Å—Ç—Ä–∞—Ö –æ–±–∏–¥–≤–∞ –Ω–µ–ø–æ–≤–Ω—ñ"
            ]
          },
          "related": [
            "Feed",
            "Video Content"
          ]
        },
        {
          "slug": "doomers",
          "title": {
            "en": "Techno-Pessimists",
            "uk": "–¢–µ—Ö–Ω–æ-–ø–µ—Å–∏–º—ñ—Å—Ç–∏"
          },
          "desc": {
            "en": "Concerns about existential risk from advanced AI.",
            "uk": "–ó–∞–Ω–µ–ø–æ–∫–æ—î–Ω–Ω—è —â–æ–¥–æ –µ–∫–∑–∏—Å—Ç–µ–Ω—Ü—ñ–π–Ω–æ–≥–æ —Ä–∏–∑–∏–∫—É –≤—ñ–¥ –ø—Ä–æ—Å—É–Ω—É—Ç–æ–≥–æ –®–Ü."
          },
          "overview": {
            "en": [
              "AI risk researchers and techno-pessimists (\"doomers\") argue that advanced AI poses existential risks to humanity that demand immediate, serious action. Their concerns range from near-term harms (deepfakes, job displacement, surveillance) to long-term catastrophic risks (misaligned superintelligence, loss of human control). Key figures include Eliezer Yudkowsky, Stuart Russell, and organizations like MIRI and the Pause AI movement.",
              "The doomer perspective is often mischaracterized as simply \"anti-technology.\" In reality, most AI safety researchers are deeply technical people who understand AI capabilities and see specific, well-reasoned dangers. Their arguments have influenced the creation of safety teams at every major AI lab and have shaped governmental AI policy worldwide."
            ],
            "uk": [
              "–î–æ—Å–ª—ñ–¥–Ω–∏–∫–∏ —Ä–∏–∑–∏–∫—ñ–≤ –®–Ü —Ç–∞ —Ç–µ—Ö–Ω–æ-–ø–µ—Å–∏–º—ñ—Å—Ç–∏ (\"–¥—É–º–µ—Ä–∏\") –∞—Ä–≥—É–º–µ–Ω—Ç—É—é—Ç—å —â–æ –ø—Ä–æ—Å—É–Ω—É—Ç–∏–π –®–Ü —Å—Ç–≤–æ—Ä—é—î –µ–∫–∑–∏—Å—Ç–µ–Ω—Ü—ñ–π–Ω—ñ —Ä–∏–∑–∏–∫–∏ –¥–ª—è –ª—é–¥—Å—Ç–≤–∞, —â–æ –≤–∏–º–∞–≥–∞—é—Ç—å –Ω–µ–≥–∞–π–Ω–∏—Ö —Å–µ—Ä–π–æ–∑–Ω–∏—Ö –¥—ñ–π. –á—Ö –∑–∞–Ω–µ–ø–æ–∫–æ—î–Ω–Ω—è –ø—Ä–æ—Å—Ç—è–≥–∞—é—Ç—å—Å—è –≤—ñ–¥ –Ω–∞–π–±–ª–∏–∂—á–∏—Ö –∑–∞–≥—Ä–æ–∑ (–¥—ñ–ø—Ñ–µ–π–∫–∏, –≤—Ç—Ä–∞—Ç–∞ —Ä–æ–±–æ—á–∏—Ö –º—ñ—Å—Ü—å, –Ω–∞–≥–ª—è–¥) –¥–æ –¥–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–∏—Ö –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ñ—á–Ω–∏—Ö —Ä–∏–∑–∏–∫—ñ–≤ (–Ω–µ–≤–∏—Ä—ñ–≤–Ω—è–Ω–∏–π —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç, –≤—Ç—Ä–∞—Ç–∞ –∫–æ–Ω—Ç—Ä–æ–ª—é). –ö–ª—é—á–æ–≤—ñ —Ñ—ñ–≥—É—Ä–∏: –ï–ª—ñ–∑–µ—Ä –Æ–¥–∫–æ–≤—Å—å–∫–∏–π, –°—Ç—é–∞—Ä—Ç –†–∞—Å—Å–µ–ª, –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—ó MIRI —Ç–∞ —Ä—É—Ö Pause AI.",
              "–ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞ –¥—É–º–µ—Ä—ñ–≤ —á–∞—Å—Ç–æ —Å–ø–æ—Ç–≤–æ—Ä—é—î—Ç—å—Å—è —è–∫ –ø—Ä–æ—Å—Ç–æ \"–ø—Ä–æ—Ç–∏-—Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∞.\" –ù–∞—Å–ø—Ä–∞–≤–¥—ñ –±—ñ–ª—å—à—ñ—Å—Ç—å –¥–æ—Å–ª—ñ–¥–Ω–∏–∫—ñ–≤ –±–µ–∑–ø–µ–∫–∏ –®–Ü ‚Äî –≥–ª–∏–±–æ–∫–æ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ –ª—é–¥–∏, —â–æ —Ä–æ–∑—É–º—ñ—é—Ç—å –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –®–Ü —Ç–∞ –±–∞—á–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ, –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω—ñ –Ω–µ–±–µ–∑–ø–µ–∫–∏. –á—Ö –∞—Ä–≥—É–º–µ–Ω—Ç–∏ –≤–ø–ª–∏–Ω—É–ª–∏ –Ω–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–º–∞–Ω–¥ –±–µ–∑–ø–µ–∫–∏ —É –∫–æ–∂–Ω—ñ–π –≤–µ–ª–∏–∫—ñ–π –®–Ü-–ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ—ó —Ç–∞ —Å—Ñ–æ—Ä–º—É–≤–∞–ª–∏ —É—Ä—è–¥–æ–≤—É –®–Ü-–ø–æ–ª—ñ—Ç–∏–∫—É —É —Å–≤—ñ—Ç—ñ."
            ]
          },
          "details": {
            "en": [
              {
                "text": "The Core Argument",
                "desc": "We are building increasingly powerful AI systems without understanding how to control them. The alignment problem is unsolved. Deploying superintelligent AI before solving alignment could be catastrophic and irreversible. The stakes are too high for \"move fast and break things.\"",
                "links": [
                  {
                    "title": "AI Alignment",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "Eliezer Yudkowsky's Position",
                "desc": "The most prominent \"doomer\" argues AI poses extinction-level risk, current alignment approaches are insufficient, and we may need to halt development of frontier AI. His writings at LessWrong have shaped the entire AI safety field.",
                "links": []
              },
              {
                "text": "Stuart Russell's Framework",
                "desc": "\"Human Compatible\" (2019) argues the standard AI model (optimize a given objective) is fundamentally flawed. Instead, AI should be uncertain about human preferences and defer to humans. A more moderate but influential critique.",
                "links": []
              },
              {
                "text": "MIRI and AI Risk Research",
                "desc": "Machine Intelligence Research Institute has studied AI alignment since 2000. Pioneered many concepts: corrigibility, goal stability, decision theory for AI. Their pessimism about current approaches has been influential.",
                "links": []
              },
              {
                "text": "Pause AI Movement",
                "desc": "Calling for a moratorium on training models more powerful than GPT-4 until alignment is solved. The open letter (signed by Musk, Wozniak, and others) requested a 6-month pause. Critics call it impractical and counterproductive.",
                "links": []
              },
              {
                "text": "Near-Term AI Risks",
                "desc": "Not just far-future concerns: deepfake misinformation, AI-powered cyberattacks, autonomous weapons, mass surveillance, algorithmic discrimination, and economic disruption from rapid automation. These risks are current and measurable.",
                "links": []
              },
              {
                "text": "The Alignment Tax",
                "desc": "Safety research costs time and money but produces no direct revenue. Companies face competitive pressure to skip safety work. Without regulation or cultural norms, the incentive structure favors speed over safety.",
                "links": [
                  {
                    "title": "AI Safety",
                    "href": "ai-safety.html"
                  }
                ]
              },
              {
                "text": "Regulatory Approaches",
                "desc": "EU AI Act (risk-based regulation), US Executive Order on AI Safety, UK AI Safety Institute, China AI regulations. Different countries taking different approaches ‚Äî a global framework remains elusive.",
                "links": []
              },
              {
                "text": "Counterarguments",
                "desc": "Critics argue: pausing is unenforceable internationally, slowing AI has real costs (delayed benefits), current AI is not close to dangerous superintelligence, and safety research progresses alongside capabilities.",
                "links": [
                  {
                    "title": "Techno-Optimists",
                    "href": "accelerationists.html"
                  }
                ]
              },
              {
                "text": "The Productive Middle",
                "desc": "Many researchers occupy a middle ground: AI development should continue but with mandatory safety evaluations, alignment research investment proportional to capability, and international coordination on the most dangerous capabilities.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "–û—Å–Ω–æ–≤–Ω–∏–π –∞—Ä–≥—É–º–µ–Ω—Ç",
                "desc": "–ú–∏ –±—É–¥—É—î–º–æ –≤—Å–µ –ø–æ—Ç—É–∂–Ω—ñ—à—ñ —Å–∏—Å—Ç–µ–º–∏ –®–Ü –±–µ–∑ —Ä–æ–∑—É–º—ñ–Ω–Ω—è —è–∫ —ó—Ö –∫–æ–Ω—Ç—Ä–æ–ª—é–≤–∞—Ç–∏. –ü—Ä–æ–±–ª–µ–º–∞ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –Ω–µ–≤–∏—Ä—ñ—à–µ–Ω–∞. –î–µ–ø–ª–æ–π —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –®–Ü –¥–æ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –º–æ–∂–µ –±—É—Ç–∏ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ñ—á–Ω–∏–º —Ç–∞ –Ω–µ–∑–≤–æ—Ä–æ—Ç–Ω–∏–º. –°—Ç–∞–≤–∫–∏ –∑–∞–Ω–∞–¥—Ç–æ –≤–∏—Å–æ–∫—ñ –¥–ª—è \"—Ä—É—Ö–∞–π—Å—è —à–≤–∏–¥–∫–æ —Ç–∞ –ª–∞–º–∞–π.\"",
                "links": [
                  {
                    "title": "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –®–Ü",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "–ü–æ–∑–∏—Ü—ñ—è –Æ–¥–∫–æ–≤—Å—å–∫–æ–≥–æ",
                "desc": "–ù–∞–π–ø—Ä–æ–º—ñ–Ω–µ–Ω—Ç–Ω—ñ—à–∏–π \"–¥—É–º–µ—Ä\" –∞—Ä–≥—É–º–µ–Ω—Ç—É—î —â–æ –®–Ü —Å—Ç–≤–æ—Ä—é—î —Ä–∏–∑–∏–∫ —Ä—ñ–≤–Ω—è –≤–∏–º–∏—Ä–∞–Ω–Ω—è, –ø–æ—Ç–æ—á–Ω—ñ –ø—ñ–¥—Ö–æ–¥–∏ –¥–æ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—ñ, —ñ –º–æ–∂–µ –∑–Ω–∞–¥–æ–±–∏—Ç–∏—Å—è –∑—É–ø–∏–Ω–∏—Ç–∏ —Ä–æ–∑–≤–∏—Ç–æ–∫ —Ñ—Ä–æ–Ω—Ç—ñ—Ä–Ω–æ–≥–æ –®–Ü. –ô–æ–≥–æ –ø—Ä–∞—Ü—ñ –Ω–∞ LessWrong —Å—Ñ–æ—Ä–º—É–≤–∞–ª–∏ –≤—Å—é —Å—Ñ–µ—Ä—É –±–µ–∑–ø–µ–∫–∏ –®–Ü.",
                "links": []
              },
              {
                "text": "–†–∞–º–∫–∞ –°—Ç—é–∞—Ä—Ç–∞ –†–∞—Å—Å–µ–ª–∞",
                "desc": "\"Human Compatible\" (2019) –∞—Ä–≥—É–º–µ–Ω—Ç—É—î —â–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –º–æ–¥–µ–ª—å –®–Ü (–æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–æ—ó –º–µ—Ç–∏) —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ —Ö–∏–±–Ω–∞. –ù–∞—Ç–æ–º—ñ—Å—Ç—å –®–Ü –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –Ω–µ–≤–ø–µ–≤–Ω–µ–Ω–∏–º —â–æ–¥–æ –ª—é–¥—Å—å–∫–∏—Ö –ø–µ—Ä–µ–≤–∞–≥ —Ç–∞ –ø–æ—Å—Ç—É–ø–∞—Ç–∏—Å—è –ª—é–¥—è–º. –ü–æ–º—ñ—Ä–Ω—ñ—à–∞, –∞–ª–µ –≤–ø–ª–∏–≤–æ–≤–∞ –∫—Ä–∏—Ç–∏–∫–∞.",
                "links": []
              },
              {
                "text": "MIRI —Ç–∞ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —Ä–∏–∑–∏–∫—ñ–≤ –®–Ü",
                "desc": "Machine Intelligence Research Institute –≤–∏–≤—á–∞—î –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –®–Ü –∑ 2000 —Ä–æ–∫—É. –ü—ñ–æ–Ω–µ—Ä–∏ –±–∞–≥–∞—Ç—å–æ—Ö –∫–æ–Ω—Ü–µ–ø—Ü—ñ–π: –∫–æ—Ä–∏–≥–æ–≤–∞–Ω—ñ—Å—Ç—å, —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å —Ü—ñ–ª–µ–π, —Ç–µ–æ—Ä—ñ—è —Ä—ñ—à–µ–Ω—å –¥–ª—è –®–Ü. –á—Ö –ø–µ—Å–∏–º—ñ–∑–º —â–æ–¥–æ –ø–æ—Ç–æ—á–Ω–∏—Ö –ø—ñ–¥—Ö–æ–¥—ñ–≤ –±—É–≤ –≤–ø–ª–∏–≤–æ–≤–∏–º.",
                "links": []
              },
              {
                "text": "–†—É—Ö Pause AI",
                "desc": "–ó–∞–∫–ª–∏–∫ –¥–æ –º–æ—Ä–∞—Ç–æ—Ä—ñ—é –Ω–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –ø–æ—Ç—É–∂–Ω—ñ—à–∏—Ö –∑–∞ GPT-4 –¥–æ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è. –í—ñ–¥–∫—Ä–∏—Ç–∏–π –ª–∏—Å—Ç (–ø—ñ–¥–ø–∏—Å–∞–Ω–∏–π –ú–∞—Å–∫–æ–º, –í–æ–∑–Ω—è–∫–æ–º —Ç–∞ —ñ–Ω—à–∏–º–∏) –ø—Ä–æ—Å–∏–≤ 6-–º—ñ—Å—è—á–Ω—É –ø–∞—É–∑—É. –ö—Ä–∏—Ç–∏–∫–∏ –Ω–∞–∑–∏–≤–∞—é—Ç—å —Ü–µ –Ω–µ–ø—Ä–∞–∫—Ç–∏—á–Ω–∏–º.",
                "links": []
              },
              {
                "text": "–ù–∞–π–±–ª–∏–∂—á—ñ —Ä–∏–∑–∏–∫–∏ –®–Ü",
                "desc": "–ù–µ –ª–∏—à–µ –¥–∞–ª–µ–∫–µ –º–∞–π–±—É—Ç–Ω—î: –¥—ñ–ø—Ñ–µ–π–∫-–¥–µ–∑—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è, –®–Ü-–∫—ñ–±–µ—Ä–∞—Ç–∞–∫–∏, –∞–≤—Ç–æ–Ω–æ–º–Ω–∞ –∑–±—Ä–æ—è, –º–∞—Å–æ–≤–∏–π –Ω–∞–≥–ª—è–¥, –∞–ª–≥–æ—Ä–∏—Ç–º—ñ—á–Ω–∞ –¥–∏—Å–∫—Ä–∏–º—ñ–Ω–∞—Ü—ñ—è —Ç–∞ –µ–∫–æ–Ω–æ–º—ñ—á–Ω–µ —Ä—É–π–Ω—É–≤–∞–Ω–Ω—è –≤—ñ–¥ —à–≤–∏–¥–∫–æ—ó –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó. –¶—ñ —Ä–∏–∑–∏–∫–∏ –ø–æ—Ç–æ—á–Ω—ñ —Ç–∞ –≤–∏–º—ñ—Ä—é–≤–∞–Ω—ñ.",
                "links": []
              },
              {
                "text": "–ü–æ–¥–∞—Ç–æ–∫ –Ω–∞ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è",
                "desc": "–î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –±–µ–∑–ø–µ–∫–∏ –∫–æ—à—Ç—É—é—Ç—å —á–∞—Å—É —Ç–∞ –≥—Ä–æ—à–µ–π, –∞–ª–µ –Ω–µ –ø—Ä–∏–Ω–æ—Å—è—Ç—å –ø—Ä—è–º–æ–≥–æ –¥–æ—Ö–æ–¥—É. –ö–æ–º–ø–∞–Ω—ñ—ó –ø—ñ–¥ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏–º —Ç–∏—Å–∫–æ–º –ø—Ä–æ–ø—É—Å–∫–∞—Ç–∏ –±–µ–∑–ø–µ–∫—É. –ë–µ–∑ —Ä–µ–≥—É–ª—é–≤–∞–Ω–Ω—è —Å—Ç–∏–º—É–ª–∏ —Å–ø—Ä–∏—è—é—Ç—å —à–≤–∏–¥–∫–æ—Å—Ç—ñ –Ω–∞–¥ –±–µ–∑–ø–µ–∫–æ—é.",
                "links": [
                  {
                    "title": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü",
                    "href": "ai-safety.html"
                  }
                ]
              },
              {
                "text": "–†–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ñ –ø—ñ–¥—Ö–æ–¥–∏",
                "desc": "EU AI Act (—Ä–µ–≥—É–ª—é–≤–∞–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–∏–∑–∏–∫—É), US Executive Order –∑ –±–µ–∑–ø–µ–∫–∏ –®–Ü, UK AI Safety Institute, —Ä–µ–≥—É–ª—è—Ü—ñ—ó –ö–∏—Ç–∞—é. –†—ñ–∑–Ω—ñ –∫—Ä–∞—ó–Ω–∏ –∑ —Ä—ñ–∑–Ω–∏–º–∏ –ø—ñ–¥—Ö–æ–¥–∞–º–∏ ‚Äî –≥–ª–æ–±–∞–ª—å–Ω–∞ —Ä–∞–º–∫–∞ –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –Ω–µ–≤–ª–æ–≤–∏–º–æ—é.",
                "links": []
              },
              {
                "text": "–ö–æ–Ω—Ç—Ä–∞—Ä–≥—É–º–µ–Ω—Ç–∏",
                "desc": "–ö—Ä–∏—Ç–∏–∫–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—É—é—Ç—å: –ø–∞—É–∑–∞ –Ω–µ–∑–¥—ñ–π—Å–Ω–µ–Ω–Ω–∞ –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–æ, —Å–ø–æ–≤—ñ–ª—å–Ω–µ–Ω–Ω—è –®–Ü –º–∞—î —Ä–µ–∞–ª—å–Ω—ñ –≤–∏—Ç—Ä–∞—Ç–∏ (–∑–∞—Ç—Ä–∏–º–∞–Ω—ñ –≤–∏–≥–æ–¥–∏), –ø–æ—Ç–æ—á–Ω–∏–π –®–Ü –Ω–µ –±–ª–∏–∑—å–∫–∏–π –¥–æ –Ω–µ–±–µ–∑–ø–µ—á–Ω–æ–≥–æ —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç—É, –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –±–µ–∑–ø–µ–∫–∏ –ø—Ä–æ—Å—É–≤–∞—é—Ç—å—Å—è —Ä–∞–∑–æ–º –∑ –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º–∏.",
                "links": [
                  {
                    "title": "–¢–µ—Ö–Ω–æ-–æ–ø—Ç–∏–º—ñ—Å—Ç–∏",
                    "href": "accelerationists.html"
                  }
                ]
              },
              {
                "text": "–ü—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–∞",
                "desc": "–ë–∞–≥–∞—Ç–æ –¥–æ—Å–ª—ñ–¥–Ω–∏–∫—ñ–≤ –Ω–∞ —Å–µ—Ä–µ–¥–Ω—ñ–π –ø–æ–∑–∏—Ü—ñ—ó: —Ä–æ–∑–≤–∏—Ç–æ–∫ –®–Ü –ø–æ–≤–∏–Ω–µ–Ω –ø—Ä–æ–¥–æ–≤–∂—É–≤–∞—Ç–∏—Å—è, –∞–ª–µ –∑ –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–º–∏ –æ—Ü—ñ–Ω–∫–∞–º–∏ –±–µ–∑–ø–µ–∫–∏, —ñ–Ω–≤–µ—Å—Ç–∏—Ü—ñ—è–º–∏ —É –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–æ –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º —Ç–∞ –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–æ—é –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü—ñ—î—é –Ω–∞–π–Ω–µ–±–µ–∑–ø–µ—á–Ω—ñ—à–∏—Ö –∑–¥–∞—Ç–Ω–æ—Å—Ç–µ–π.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "AI Doomer",
                "def": "Person who believes advanced AI poses serious existential risk to humanity and advocates for caution or pause."
              },
              {
                "term": "Pause AI",
                "def": "Movement calling for a moratorium on training AI systems more powerful than current frontier models."
              },
              {
                "term": "X-Risk",
                "def": "Existential risk ‚Äî any event that could cause human extinction or permanent, drastic reduction in human potential."
              },
              {
                "term": "AI Governance",
                "def": "Policy frameworks, regulations, and international agreements for managing AI development and deployment."
              }
            ],
            "uk": [
              {
                "term": "–®–Ü-–¥—É–º–µ—Ä",
                "def": "–õ—é–¥–∏–Ω–∞ —â–æ –≤—ñ—Ä–∏—Ç—å –ø—Ä–æ—Å—É–Ω—É—Ç–∏–π –®–Ü —Å—Ç–≤–æ—Ä—é—î —Å–µ—Ä–π–æ–∑–Ω–∏–π –µ–∫–∑–∏—Å—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π —Ä–∏–∑–∏–∫ —Ç–∞ –≤–∏—Å—Ç—É–ø–∞—î –∑–∞ –æ–±–µ—Ä–µ–∂–Ω—ñ—Å—Ç—å –∞–±–æ –ø–∞—É–∑—É."
              },
              {
                "term": "Pause AI",
                "def": "–†—É—Ö —â–æ –∑–∞–∫–ª–∏–∫–∞—î –¥–æ –º–æ—Ä–∞—Ç–æ—Ä—ñ—é –Ω–∞ –Ω–∞–≤—á–∞–Ω–Ω—è —Å–∏—Å—Ç–µ–º –®–Ü –ø–æ—Ç—É–∂–Ω—ñ—à–∏—Ö –∑–∞ –ø–æ—Ç–æ—á–Ω—ñ —Ñ—Ä–æ–Ω—Ç—ñ—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ."
              },
              {
                "term": "X-Risk",
                "def": "–ï–∫–∑–∏—Å—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π —Ä–∏–∑–∏–∫ ‚Äî –±—É–¥—å-—è–∫–∞ –ø–æ–¥—ñ—è —â–æ –º–æ–∂–µ —Å–ø—Ä–∏—á–∏–Ω–∏—Ç–∏ –≤–∏–º–∏—Ä–∞–Ω–Ω—è –ª—é–¥—Å—Ç–≤–∞ –∞–±–æ –ø–æ—Å—Ç—ñ–π–Ω–µ —Ä—ñ–∑–∫–µ –∑–Ω–∏–∂–µ–Ω–Ω—è –ª—é–¥—Å—å–∫–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª—É."
              },
              {
                "term": "–£–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –®–Ü",
                "def": "–ü–æ–ª—ñ—Ç–∏—á–Ω—ñ —Ä–∞–º–∫–∏, —Ä–µ–≥—É–ª—è—Ü—ñ—ó —Ç–∞ –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω—ñ —É–≥–æ–¥–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —Ä–æ–∑–≤–∏—Ç–∫–æ–º —Ç–∞ –¥–µ–ø–ª–æ—î–º –®–Ü."
              }
            ]
          },
          "tips": {
            "en": [
              "Read both doomer and accelerationist perspectives ‚Äî the truth likely involves elements of both, and strong opinions on either side often miss nuance",
              "Follow the EU AI Act implementation for the most concrete example of AI regulation in practice",
              "Near-term AI risks (deepfakes, surveillance) are worth taking seriously regardless of your view on long-term existential risk"
            ],
            "uk": [
              "–ß–∏—Ç–∞–π—Ç–µ —ñ –¥—É–º–µ—Ä—Å—å–∫—ñ —ñ –∞–∫—Å–µ–ª–µ—Ä–∞—Ü—ñ–æ–Ω—ñ—Å—Ç—Å—å–∫—ñ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∏ ‚Äî —ñ—Å—Ç–∏–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω–æ –≤–∫–ª—é—á–∞—î –µ–ª–µ–º–µ–Ω—Ç–∏ –æ–±–æ—Ö, —Å–∏–ª—å–Ω—ñ –¥—É–º–∫–∏ –∑ –±—É–¥—å-—è–∫–æ–≥–æ –±–æ–∫—É —á–∞—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç—å –Ω—é–∞–Ω—Å–∏",
              "–°–ª—ñ–¥–∫—É–π—Ç–µ –∑–∞ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è–º EU AI Act –¥–ª—è –Ω–∞–π–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ—à–æ–≥–æ –ø—Ä–∏–∫–ª–∞–¥—É —Ä–µ–≥—É–ª—é–≤–∞–Ω–Ω—è –®–Ü –Ω–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ",
              "–ù–∞–π–±–ª–∏–∂—á—ñ —Ä–∏–∑–∏–∫–∏ –®–Ü (–¥—ñ–ø—Ñ–µ–π–∫–∏, –Ω–∞–≥–ª—è–¥) –≤–∞—Ä—Ç—ñ —Å–µ—Ä–π–æ–∑–Ω–æ–≥–æ —Å—Ç–∞–≤–ª–µ–Ω–Ω—è –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –≤–∞—à–æ–≥–æ –ø–æ–≥–ª—è–¥—É –Ω–∞ –¥–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–∏–π –µ–∫–∑–∏—Å—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π —Ä–∏–∑–∏–∫"
            ]
          },
          "related": [
            "Feed",
            "Video Content"
          ]
        },
        {
          "slug": "ai-safety",
          "title": {
            "en": "AI Safety",
            "uk": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü"
          },
          "desc": {
            "en": "Research and practices for building safe AI systems.",
            "uk": "–î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —Ç–∞ –ø—Ä–∞–∫—Ç–∏–∫–∏ –ø–æ–±—É–¥–æ–≤–∏ –±–µ–∑–ø–µ—á–Ω–∏—Ö —Å–∏—Å—Ç–µ–º –®–Ü."
          },
          "overview": {
            "en": [
              "AI safety is the field of research and engineering dedicated to ensuring AI systems behave as intended, remain under human control, and do not cause harm. It spans from near-term practical concerns (preventing bias, ensuring robustness, avoiding misuse) to long-term challenges (alignment, containment, value learning). Every major AI lab now has a dedicated safety team.",
              "Safety is not the opposite of capability ‚Äî it is what enables capability to be deployed responsibly. Just as aviation safety enabled air travel to become the safest form of transportation, AI safety research aims to make increasingly powerful AI systems trustworthy enough for high-stakes applications."
            ],
            "uk": [
              "–ë–µ–∑–ø–µ–∫–∞ –®–Ü ‚Äî —Ü–µ —Å—Ñ–µ—Ä–∞ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å —Ç–∞ —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—ó, –ø—Ä–∏—Å–≤—è—á–µ–Ω–∞ –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—é —Ç–æ–≥–æ, —â–æ —Å–∏—Å—Ç–µ–º–∏ –®–Ü –ø–æ–≤–æ–¥—è—Ç—å—Å—è —è–∫ –∑–∞–¥—É–º–∞–Ω–æ, –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è –ø—ñ–¥ –ª—é–¥—Å—å–∫–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ç–∞ –Ω–µ –∑–∞–≤–¥–∞—é—Ç—å —à–∫–æ–¥–∏. –í–æ–Ω–∞ –æ—Ö–æ–ø–ª—é—î –≤—ñ–¥ –Ω–∞–π–±–ª–∏–∂—á–∏—Ö –ø—Ä–∞–∫—Ç–∏—á–Ω–∏—Ö –∑–∞–Ω–µ–ø–æ–∫–æ—î–Ω—å (–∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è —É–ø–µ—Ä–µ–¥–∂–µ–Ω–æ—Å—Ç—ñ, –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—ñ) –¥–æ –¥–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–∏—Ö –≤–∏–∫–ª–∏–∫—ñ–≤ (–≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è, —Å—Ç—Ä–∏–º—É–≤–∞–Ω–Ω—è, –Ω–∞–≤—á–∞–Ω–Ω—è —Ü—ñ–Ω–Ω–æ—Å—Ç–µ–π). –ö–æ–∂–Ω–∞ –≤–µ–ª–∏–∫–∞ –®–Ü-–ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ—è —Ç–µ–ø–µ—Ä –º–∞—î –≤–∏–¥—ñ–ª–µ–Ω—É –∫–æ–º–∞–Ω–¥—É –±–µ–∑–ø–µ–∫–∏.",
              "–ë–µ–∑–ø–µ–∫–∞ ‚Äî –Ω–µ –ø—Ä–æ—Ç–∏–ª–µ–∂–Ω—ñ—Å—Ç—å –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º, –∞ —Ç–µ, —â–æ –¥–æ–∑–≤–æ–ª—è—î –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º –±—É—Ç–∏ –¥–µ–ø–ª–æ—î–Ω–∏–º–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–æ. –¢–∞–∫ —Å–∞–º–æ —è–∫ –∞–≤—ñ–∞—Ü—ñ–π–Ω–∞ –±–µ–∑–ø–µ–∫–∞ –¥–æ–∑–≤–æ–ª–∏–ª–∞ –ø–æ–≤—ñ—Ç—Ä—è–Ω–∏–º –ø–µ—Ä–µ–ª—å–æ—Ç–∞–º —Å—Ç–∞—Ç–∏ –Ω–∞–π–±–µ–∑–ø–µ—á–Ω—ñ—à–∏–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º, –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –±–µ–∑–ø–µ–∫–∏ –®–Ü –ø—Ä–∞–≥–Ω—É—Ç—å –∑—Ä–æ–±–∏—Ç–∏ –≤—Å–µ –ø–æ—Ç—É–∂–Ω—ñ—à—ñ —Å–∏—Å—Ç–µ–º–∏ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –Ω–∞–¥—ñ–π–Ω–∏–º–∏ –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Categories of AI Risk",
                "desc": "Misuse (intentional harm: deepfakes, cyberweapons), Misalignment (unintended behavior from flawed objectives), Accidents (bugs and failures in AI systems), Structural risks (concentration of power, economic disruption).",
                "links": []
              },
              {
                "text": "Red-Teaming",
                "desc": "Adversarial testing of AI systems to find harmful behaviors before deployment. Teams try to make the AI produce dangerous content, leak information, or behave unpredictably. Now standard practice at all major labs.",
                "links": []
              },
              {
                "text": "Safety Evaluations",
                "desc": "Standardized tests for dangerous capabilities: CBRN knowledge (chemical/biological/radiological/nuclear), cyber offense, persuasion, autonomous replication. Anthropic, OpenAI, and DeepMind all publish safety evaluation results.",
                "links": []
              },
              {
                "text": "Constitutional AI",
                "desc": "Anthropic's approach: train AI with a set of principles (a \"constitution\") and have it self-evaluate against those principles. Reduces the need for human feedback while maintaining safety properties.",
                "links": [
                  {
                    "title": "AI Alignment",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "Containment and Monitoring",
                "desc": "Strategies for controlling AI systems: sandboxing (limited environment access), human-in-the-loop (approval for high-stakes actions), output filtering, and continuous monitoring for anomalous behavior.",
                "links": [
                  {
                    "title": "Agents",
                    "href": "../level-4/agents.html"
                  }
                ]
              },
              {
                "text": "Responsible Scaling",
                "desc": "Anthropic's Responsible Scaling Policy and similar frameworks: assess safety before increasing capabilities. If safety is not demonstrated at a capability level, don't scale further until it is.",
                "links": []
              },
              {
                "text": "AI Safety Organizations",
                "desc": "Anthropic (safety-focused lab), OpenAI Safety team, Google DeepMind Safety, MIRI, Center for AI Safety (CAIS), AI Safety Institute (UK/US), Alignment Research Center (ARC). Growing ecosystem of safety-focused research.",
                "links": []
              },
              {
                "text": "Practical Safety Engineering",
                "desc": "Input validation, output filtering, rate limiting, abuse detection, prompt injection defense, and secure tool use. The engineering side of safety that every AI application developer should implement.",
                "links": [
                  {
                    "title": "Tool Use",
                    "href": "../level-4/tool-use.html"
                  }
                ]
              },
              {
                "text": "Dual-Use Concerns",
                "desc": "Many AI capabilities are dual-use: code generation helps developers but also creates malware. Biology knowledge helps research but enables bioweapons. Managing this tension is a core safety challenge.",
                "links": []
              },
              {
                "text": "The Safety Culture Shift",
                "desc": "AI safety has moved from niche concern to mainstream requirement. Major AI conferences have safety tracks, companies hire safety researchers, and governments create safety institutes. The culture is shifting toward taking safety seriously.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "–ö–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ä–∏–∑–∏–∫—ñ–≤ –®–Ü",
                "desc": "–ó–ª–æ–≤–∂–∏–≤–∞–Ω–Ω—è (–Ω–∞–≤–º–∏—Å–Ω–∞ —à–∫–æ–¥–∞: –¥—ñ–ø—Ñ–µ–π–∫–∏, –∫—ñ–±–µ—Ä–∑–±—Ä–æ—è), –Ω–µ–≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è (–Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –ø–æ–≤–µ–¥—ñ–Ω–∫–∞ –≤—ñ–¥ —Ö–∏–±–Ω–∏—Ö —Ü—ñ–ª–µ–π), –∞–≤–∞—Ä—ñ—ó (–±–∞–≥–∏ —Ç–∞ –∑–±–æ—ó), —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ñ —Ä–∏–∑–∏–∫–∏ (–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—è –≤–ª–∞–¥–∏, –µ–∫–æ–Ω–æ–º—ñ—á–Ω–µ —Ä—É–π–Ω—É–≤–∞–Ω–Ω—è).",
                "links": []
              },
              {
                "text": "–†–µ–¥-—Ç—ñ–º—ñ–Ω–≥",
                "desc": "–ó–º–∞–≥–∞–ª—å–Ω–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Å–∏—Å—Ç–µ–º –®–Ü –¥–ª—è –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è —à–∫—ñ–¥–ª–∏–≤–æ—ó –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –ø–µ—Ä–µ–¥ –¥–µ–ø–ª–æ—î–º. –ö–æ–º–∞–Ω–¥–∏ –Ω–∞–º–∞–≥–∞—é—Ç—å—Å—è –∑–º—É—Å–∏—Ç–∏ –®–Ü –≤–∏–¥–∞–≤–∞—Ç–∏ –Ω–µ–±–µ–∑–ø–µ—á–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –∞–±–æ –ø–æ–≤–æ–¥–∏—Ç–∏—Å—è –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω–æ. –¢–µ–ø–µ—Ä —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–∞.",
                "links": []
              },
              {
                "text": "–û—Ü—ñ–Ω–∫–∏ –±–µ–∑–ø–µ–∫–∏",
                "desc": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω—ñ —Ç–µ—Å—Ç–∏ –Ω–∞ –Ω–µ–±–µ–∑–ø–µ—á–Ω—ñ –∑–¥–∞—Ç–Ω–æ—Å—Ç—ñ: CBRN-–∑–Ω–∞–Ω–Ω—è (—Ö—ñ–º—ñ—á–Ω–µ/–±—ñ–æ–ª–æ–≥—ñ—á–Ω–µ/—Ä–∞–¥—ñ–æ–ª–æ–≥—ñ—á–Ω–µ/—è–¥–µ—Ä–Ω–µ), –∫—ñ–±–µ—Ä-–∞—Ç–∞–∫–∏, –ø–µ—Ä–µ–∫–æ–Ω–∞–Ω–Ω—è, –∞–≤—Ç–æ–Ω–æ–º–Ω–∞ —Ä–µ–ø–ª—ñ–∫–∞—Ü—ñ—è. Anthropic, OpenAI —Ç–∞ DeepMind –ø—É–±–ª—ñ–∫—É—é—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏.",
                "links": []
              },
              {
                "text": "–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ–π–Ω–∏–π –®–Ü",
                "desc": "–ü—ñ–¥—Ö—ñ–¥ Anthropic: –Ω–∞–≤—á–∞–Ω–Ω—è –®–Ü –∑ –Ω–∞–±–æ—Ä–æ–º –ø—Ä–∏–Ω—Ü–∏–ø—ñ–≤ (\"–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ—è\") —Ç–∞ —Å–∞–º–æ–æ—Ü—ñ–Ω–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ —Ü–∏–º –ø—Ä–∏–Ω—Ü–∏–ø–∞–º. –ó–º–µ–Ω—à—É—î –ø–æ—Ç—Ä–µ–±—É —É –ª—é–¥—Å—å–∫–æ–º—É –∑–≤–æ—Ä–æ—Ç–Ω–æ–º—É –∑–≤'—è–∑–∫—É –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ –±–µ–∑–ø–µ–∫–∏.",
                "links": [
                  {
                    "title": "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –®–Ü",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "–°—Ç—Ä–∏–º—É–≤–∞–Ω–Ω—è —Ç–∞ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥",
                "desc": "–°—Ç—Ä–∞—Ç–µ–≥—ñ—ó –∫–æ–Ω—Ç—Ä–æ–ª—é —Å–∏—Å—Ç–µ–º –®–Ü: –ø—ñ—Å–æ—á–Ω–∏—Ü—è (–æ–±–º–µ–∂–µ–Ω–∏–π –¥–æ—Å—Ç—É–ø –¥–æ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞), –ª—é–¥–∏–Ω–∞-–≤-–ø–µ—Ç–ª—ñ (—Å—Ö–≤–∞–ª–µ–Ω–Ω—è –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –¥—ñ–π), —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –≤–∏—Ö–æ–¥—É —Ç–∞ –±–µ–∑–ø–µ—Ä–µ—Ä–≤–Ω–∏–π –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –∞–Ω–æ–º–∞–ª—å–Ω–æ—ó –ø–æ–≤–µ–¥—ñ–Ω–∫–∏.",
                "links": [
                  {
                    "title": "–ê–≥–µ–Ω—Ç–∏",
                    "href": "../level-4/agents.html"
                  }
                ]
              },
              {
                "text": "–í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è",
                "desc": "Responsible Scaling Policy –≤—ñ–¥ Anthropic —Ç–∞ –ø–æ–¥—ñ–±–Ω—ñ —Ä–∞–º–∫–∏: –æ—Ü—ñ–Ω–∫–∞ –±–µ–∑–ø–µ–∫–∏ –ø–µ—Ä–µ–¥ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è–º –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π. –Ø–∫—â–æ –±–µ–∑–ø–µ–∫–∞ –Ω–µ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —Ä—ñ–≤–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π, –Ω–µ –º–∞—Å—à—Ç–∞–±—É–π—Ç–µ –¥–∞–ª—ñ.",
                "links": []
              },
              {
                "text": "–û—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—ó –±–µ–∑–ø–µ–∫–∏ –®–Ü",
                "desc": "Anthropic (–ª–∞–±–æ—Ä–∞—Ç–æ—Ä—ñ—è –∑ —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –±–µ–∑–ø–µ–∫—É), –∫–æ–º–∞–Ω–¥–∞ –±–µ–∑–ø–µ–∫–∏ OpenAI, Google DeepMind Safety, MIRI, CAIS, AI Safety Institute (UK/US), ARC. –ó—Ä–æ—Å—Ç–∞—é—á–∞ –µ–∫–æ—Å–∏—Å—Ç–µ–º–∞ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å –±–µ–∑–ø–µ–∫–∏.",
                "links": []
              },
              {
                "text": "–ü—Ä–∞–∫—Ç–∏—á–Ω–∞ —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—è –±–µ–∑–ø–µ–∫–∏",
                "desc": "–í–∞–ª—ñ–¥–∞—Ü—ñ—è –≤—Ö–æ–¥—É, —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –≤–∏—Ö–æ–¥—É, –æ–±–º–µ–∂–µ–Ω–Ω—è —á–∞—Å—Ç–æ—Ç–∏, –¥–µ—Ç–µ–∫—Ü—ñ—è –∑–ª–æ–≤–∂–∏–≤–∞–Ω—å, –∑–∞—Ö–∏—Å—Ç –≤—ñ–¥ prompt injection —Ç–∞ –±–µ–∑–ø–µ—á–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤. –Ü–Ω–∂–µ–Ω–µ—Ä–Ω–∞ —Å—Ç–æ—Ä–æ–Ω–∞ –±–µ–∑–ø–µ–∫–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ä–æ–∑—Ä–æ–±–Ω–∏–∫–∞ –®–Ü-–¥–æ–¥–∞—Ç–∫—ñ–≤.",
                "links": [
                  {
                    "title": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤",
                    "href": "../level-4/tool-use.html"
                  }
                ]
              },
              {
                "text": "–î–≤–æ—Ü—ñ–ª—å–æ–≤—ñ –∑–∞–Ω–µ–ø–æ–∫–æ—î–Ω–Ω—è",
                "desc": "–ë–∞–≥–∞—Ç–æ –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –®–Ü –¥–≤–æ—Ü—ñ–ª—å–æ–≤—ñ: –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–¥—É –¥–æ–ø–æ–º–∞–≥–∞—î —Ä–æ–∑—Ä–æ–±–Ω–∏–∫–∞–º, –∞–ª–µ —Ç–∞–∫–æ–∂ —Å—Ç–≤–æ—Ä—é—î —à–∫—ñ–¥–ª–∏–≤–µ –ü–ó. –ë—ñ–æ–ª–æ–≥—ñ—á–Ω—ñ –∑–Ω–∞–Ω–Ω—è –¥–æ–ø–æ–º–∞–≥–∞—é—Ç—å –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è–º, –∞–ª–µ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –±—ñ–æ–∑–±—Ä–æ—é. –£–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —Ü—ñ—î—é –Ω–∞–ø—Ä—É–≥–æ—é ‚Äî –∫–ª—é—á–æ–≤–∏–π –≤–∏–∫–ª–∏–∫ –±–µ–∑–ø–µ–∫–∏.",
                "links": []
              },
              {
                "text": "–ó—Ä—É—à–µ–Ω–Ω—è –∫—É–ª—å—Ç—É—Ä–∏ –±–µ–∑–ø–µ–∫–∏",
                "desc": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü –ø–µ—Ä–µ–π—à–ª–∞ –≤—ñ–¥ –Ω—ñ—à–µ–≤–æ–≥–æ –∑–∞–Ω–µ–ø–æ–∫–æ—î–Ω–Ω—è –¥–æ –º–µ–π–Ω—Å—Ç—Ä–∏–º–Ω–æ—ó –≤–∏–º–æ–≥–∏. –í–µ–ª–∏–∫—ñ –®–Ü-–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—ó –º–∞—é—Ç—å —Ç—Ä–µ–∫–∏ –±–µ–∑–ø–µ–∫–∏, –∫–æ–º–ø–∞–Ω—ñ—ó –Ω–∞–π–º–∞—é—Ç—å –¥–æ—Å–ª—ñ–¥–Ω–∏–∫—ñ–≤ –±–µ–∑–ø–µ–∫–∏, —É—Ä—è–¥–∏ —Å—Ç–≤–æ—Ä—é—é—Ç—å —ñ–Ω—Å—Ç–∏—Ç—É—Ç–∏ –±–µ–∑–ø–µ–∫–∏. –ö—É–ª—å—Ç—É—Ä–∞ –∑–º—ñ—â—É—î—Ç—å—Å—è –¥–æ —Å–µ—Ä–π–æ–∑–Ω–æ–≥–æ —Å—Ç–∞–≤–ª–µ–Ω–Ω—è.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Red-Teaming",
                "def": "Adversarial testing to find harmful AI behaviors before deployment by simulating attacks and misuse."
              },
              {
                "term": "Constitutional AI",
                "def": "Training method where AI evaluates its own outputs against a set of safety principles."
              },
              {
                "term": "Responsible Scaling",
                "def": "Framework requiring safety demonstrations before increasing AI model capabilities."
              },
              {
                "term": "Dual-Use",
                "def": "AI capabilities that have both beneficial and harmful applications, creating tension between access and safety."
              }
            ],
            "uk": [
              {
                "term": "–†–µ–¥-—Ç—ñ–º—ñ–Ω–≥",
                "def": "–ó–º–∞–≥–∞–ª—å–Ω–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è —à–∫—ñ–¥–ª–∏–≤–æ—ó –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –®–Ü –ø–µ—Ä–µ–¥ –¥–µ–ø–ª–æ—î–º —á–µ—Ä–µ–∑ —Å–∏–º—É–ª—è—Ü—ñ—é –∞—Ç–∞–∫ —Ç–∞ –∑–ª–æ–≤–∂–∏–≤–∞–Ω—å."
              },
              {
                "term": "–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ–π–Ω–∏–π –®–Ü",
                "def": "–ú–µ—Ç–æ–¥ –Ω–∞–≤—á–∞–Ω–Ω—è –¥–µ –®–Ü –æ—Ü—ñ–Ω—é—î –≤–ª–∞—Å–Ω—ñ –≤–∏—Ö–æ–¥–∏ –ø—Ä–æ—Ç–∏ –Ω–∞–±–æ—Ä—É –ø—Ä–∏–Ω—Ü–∏–ø—ñ–≤ –±–µ–∑–ø–µ–∫–∏."
              },
              {
                "term": "–í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è",
                "def": "–†–∞–º–∫–∞ —â–æ –≤–∏–º–∞–≥–∞—î –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó –±–µ–∑–ø–µ–∫–∏ –ø–µ—Ä–µ–¥ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è–º –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –º–æ–¥–µ–ª—ñ –®–Ü."
              },
              {
                "term": "–î–≤–æ—Ü—ñ–ª—å–æ–≤–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è",
                "def": "–ú–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –®–Ü —â–æ –º–∞—é—Ç—å —è–∫ –∫–æ—Ä–∏—Å–Ω—ñ, —Ç–∞–∫ —ñ —à–∫—ñ–¥–ª–∏–≤—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è, —Å—Ç–≤–æ—Ä—é—é—á–∏ –Ω–∞–ø—Ä—É–≥—É –º—ñ–∂ –¥–æ—Å—Ç—É–ø–æ–º —Ç–∞ –±–µ–∑–ø–µ–∫–æ—é."
              }
            ]
          },
          "tips": {
            "en": [
              "If you build AI applications, implement practical safety measures (input validation, output filtering, rate limiting) from day one",
              "Follow Anthropic's research blog for the most accessible writing on frontier AI safety techniques",
              "AI safety is a growing career field with strong demand ‚Äî consider contributing regardless of your background"
            ],
            "uk": [
              "–Ø–∫—â–æ –±—É–¥—É—î—Ç–µ –®–Ü-–¥–æ–¥–∞—Ç–∫–∏, –≤–ø—Ä–æ–≤–∞–¥–∂—É–π—Ç–µ –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –∑–∞—Ö–æ–¥–∏ –±–µ–∑–ø–µ–∫–∏ (–≤–∞–ª—ñ–¥–∞—Ü—ñ—è –≤—Ö–æ–¥—É, —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –≤–∏—Ö–æ–¥—É, –æ–±–º–µ–∂–µ–Ω–Ω—è —á–∞—Å—Ç–æ—Ç–∏) –∑ –ø–µ—Ä—à–æ–≥–æ –¥–Ω—è",
              "–°–ª—ñ–¥–∫—É–π—Ç–µ –∑–∞ –¥–æ—Å–ª—ñ–¥–Ω–∏—Ü—å–∫–∏–º –±–ª–æ–≥–æ–º Anthropic –¥–ª—è –Ω–∞–π–¥–æ—Å—Ç—É–ø–Ω—ñ—à–∏—Ö –ø—Ä–∞—Ü—å –ø—Ä–æ —Ç–µ—Ö–Ω—ñ–∫–∏ –±–µ–∑–ø–µ–∫–∏ —Ñ—Ä–æ–Ω—Ç—ñ—Ä–Ω–æ–≥–æ –®–Ü",
              "–ë–µ–∑–ø–µ–∫–∞ –®–Ü ‚Äî –∑—Ä–æ—Å—Ç–∞—é—á–∞ –∫–∞—Ä'—î—Ä–Ω–∞ —Å—Ñ–µ—Ä–∞ –∑ –≤–∏—Å–æ–∫–∏–º –ø–æ–ø–∏—Ç–æ–º: —Ä–æ–∑–≥–ª—è–Ω—å—Ç–µ –≤–Ω–µ—Å–æ–∫ –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –≤–∞—à–æ–≥–æ –±–µ–∫–≥—Ä–∞—É–Ω–¥—É"
            ]
          },
          "related": [
            "Feed",
            "Video Content"
          ]
        },
        {
          "slug": "alignment",
          "title": {
            "en": "AI Alignment",
            "uk": "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –®–Ü"
          },
          "desc": {
            "en": "Ensuring AI systems act in accordance with human values.",
            "uk": "–ó–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è –¥—ñ–π —Å–∏—Å—Ç–µ–º –®–Ü –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ –ª—é–¥—Å—å–∫–∏—Ö —Ü—ñ–Ω–Ω–æ—Å—Ç–µ–π."
          },
          "overview": {
            "en": [
              "AI alignment is the technical challenge of ensuring AI systems pursue goals that are beneficial to humans and act in accordance with human values and intentions. It is arguably the most important unsolved problem in AI ‚Äî as systems become more capable, the consequences of misalignment grow from inconvenient to catastrophic.",
              "Current alignment techniques (RLHF, DPO, Constitutional AI) work well for today's models but may not scale to superintelligent systems. The field is racing to develop \"scalable alignment\" ‚Äî techniques that work even when the AI is more capable than its human overseers. This is what Anthropic, OpenAI, and DeepMind call the \"superalignment\" challenge."
            ],
            "uk": [
              "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –®–Ü ‚Äî —Ü–µ —Ç–µ—Ö–Ω—ñ—á–Ω–∏–π –≤–∏–∫–ª–∏–∫ –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è —Ç–æ–≥–æ, —â–æ —Å–∏—Å—Ç–µ–º–∏ –®–Ü –ø–µ—Ä–µ—Å–ª—ñ–¥—É—é—Ç—å —Ü—ñ–ª—ñ –∫–æ—Ä–∏—Å–Ω—ñ –¥–ª—è –ª—é–¥–µ–π —Ç–∞ –¥—ñ—é—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ –ª—é–¥—Å—å–∫–∏—Ö —Ü—ñ–Ω–Ω–æ—Å—Ç–µ–π —Ç–∞ –Ω–∞–º—ñ—Ä—ñ–≤. –¶–µ, –º–∞–±—É—Ç—å, –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∞ –Ω–µ–≤–∏—Ä—ñ—à–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ –®–Ü ‚Äî –≤ –º—ñ—Ä—É –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π —Å–∏—Å—Ç–µ–º, –Ω–∞—Å–ª—ñ–¥–∫–∏ –Ω–µ–≤–∏—Ä—ñ–≤–Ω—è–Ω–æ—Å—Ç—ñ –∑—Ä–æ—Å—Ç–∞—é—Ç—å –≤—ñ–¥ –Ω–µ–∑—Ä—É—á–Ω–∏—Ö –¥–æ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ñ—á–Ω–∏—Ö.",
              "–ü–æ—Ç–æ—á–Ω—ñ —Ç–µ—Ö–Ω—ñ–∫–∏ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è (RLHF, DPO, –∫–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ–π–Ω–∏–π –®–Ü) –¥–æ–±—Ä–µ –ø—Ä–∞—Ü—é—é—Ç—å –¥–ª—è —Å—å–æ–≥–æ–¥–Ω—ñ—à–Ω—ñ—Ö –º–æ–¥–µ–ª–µ–π, –∞–ª–µ –º–æ–∂—É—Ç—å –Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞—Ç–∏—Å—è –¥–æ —Å—É–ø–µ—Ä—ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∏—Ö —Å–∏—Å—Ç–µ–º. –°—Ñ–µ—Ä–∞ –∫–≤–∞–ø–∏—Ç—å—Å—è —Ä–æ–∑—Ä–æ–±–∏—Ç–∏ \"–º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–µ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è\" ‚Äî —Ç–µ—Ö–Ω—ñ–∫–∏ —â–æ –ø—Ä–∞—Ü—é—é—Ç—å –Ω–∞–≤—ñ—Ç—å –∫–æ–ª–∏ –®–Ü –∑–¥–∞—Ç–Ω—ñ—à–∏–π –∑–∞ –ª—é–¥—Å—å–∫–∏—Ö –Ω–∞–≥–ª—è–¥–∞—á—ñ–≤. –¶–µ —Ç–µ, —â–æ Anthropic, OpenAI —Ç–∞ DeepMind –Ω–∞–∑–∏–≤–∞—é—Ç—å –≤–∏–∫–ª–∏–∫–æ–º \"—Å—É–ø–µ—Ä–≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è.\""
            ]
          },
          "details": {
            "en": [
              {
                "text": "The Alignment Problem",
                "desc": "How do you specify exactly what you want an AI to do? Objectives that seem clear can be gamed: \"maximize user engagement\" leads to addictive content. \"Be helpful\" without constraints leads to helping with harmful requests. Precise value specification is extraordinarily difficult.",
                "links": []
              },
              {
                "text": "RLHF",
                "desc": "Reinforcement Learning from Human Feedback ‚Äî the technique that made ChatGPT work. Train a reward model from human preferences, then optimize the LLM to maximize that reward. Effective but limited: reward hacking, distribution shift, and human evaluator inconsistency.",
                "links": [
                  {
                    "title": "Training & Fine-tuning",
                    "href": "../level-3/training-finetuning.html"
                  }
                ]
              },
              {
                "text": "DPO (Direct Preference Optimization)",
                "desc": "A simpler alternative to RLHF that skips the reward model entirely. Directly optimizes the LLM from preference pairs. More stable, easier to implement, and increasingly preferred. Used in many modern alignment workflows.",
                "links": []
              },
              {
                "text": "Constitutional AI",
                "desc": "Anthropic's approach: define a constitution of principles, have the AI critique its own responses against these principles, then train on the self-improved outputs. Reduces reliance on human labelers while maintaining alignment properties.",
                "links": [
                  {
                    "title": "AI Safety",
                    "href": "ai-safety.html"
                  }
                ]
              },
              {
                "text": "Scalable Oversight",
                "desc": "As AI surpasses human ability, how do you evaluate if it is doing the right thing? Approaches: debate (AIs argue, humans judge), recursive reward modeling (AI helps evaluate AI), and constitutional methods (principles over case-by-case judgment).",
                "links": []
              },
              {
                "text": "Interpretability",
                "desc": "Understanding what happens inside neural networks. Mechanistic interpretability maps circuits in networks to specific behaviors. If we can read the \"thoughts\" of an AI, we can verify alignment. Anthropic and others are making rapid progress here.",
                "links": [
                  {
                    "title": "Explainable AI",
                    "href": "explainable-ai.html"
                  }
                ]
              },
              {
                "text": "Reward Hacking",
                "desc": "AI finds unintended ways to maximize its reward without actually doing what we want. Examples: a cleaning robot that hides mess instead of cleaning it. A major failure mode that alignment must address ‚Äî optimizing the metric is not the same as achieving the goal.",
                "links": [
                  {
                    "title": "Hallucinations",
                    "href": "../level-2/hallucination.html"
                  }
                ]
              },
              {
                "text": "Value Learning",
                "desc": "Instead of specifying values explicitly, have AI learn human values from behavior, feedback, and cultural knowledge. Inverse reinforcement learning and preference learning are approaches. Challenge: human values are complex, context-dependent, and sometimes contradictory.",
                "links": []
              },
              {
                "text": "Superalignment",
                "desc": "OpenAI's term for aligning AI systems more intelligent than humans. Current techniques rely on human judgment ‚Äî but what happens when the AI is smarter than the judge? This is the frontier of alignment research. Anthropic's approach: make AI that is \"honest, helpful, and harmless.\"",
                "links": [
                  {
                    "title": "ASI",
                    "href": "asi.html"
                  }
                ]
              },
              {
                "text": "Corrigibility",
                "desc": "Can we build AI that allows itself to be corrected, shut down, or modified? A truly aligned AI should welcome correction rather than resist it. But a self-improving AI might rationally resist shutdown as a threat to its goals ‚Äî this is a deep technical challenge.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "–ü—Ä–æ–±–ª–µ–º–∞ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è",
                "desc": "–Ø–∫ —Ç–æ—á–Ω–æ –≤–∫–∞–∑–∞—Ç–∏ —â–æ –≤–∏ —Ö–æ—á–µ—Ç–µ –≤—ñ–¥ –®–Ü? –¶—ñ–ª—ñ —â–æ –∑–¥–∞—é—Ç—å—Å—è —á—ñ—Ç–∫–∏–º–∏ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –æ–±—ñ–≥—Ä–∞–Ω—ñ: \"–º–∞–∫—Å–∏–º—ñ–∑—É–π –∑–∞–ª—É—á–µ–Ω—ñ—Å—Ç—å\" –≤–µ–¥–µ –¥–æ –∑–∞–ª–µ–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É. \"–ë—É–¥—å –∫–æ—Ä–∏—Å–Ω–∏–º\" –±–µ–∑ –æ–±–º–µ–∂–µ–Ω—å –≤–µ–¥–µ –¥–æ –¥–æ–ø–æ–º–æ–≥–∏ –∑—ñ —à–∫—ñ–¥–ª–∏–≤–∏–º–∏ –∑–∞–ø–∏—Ç–∞–º–∏. –¢–æ—á–Ω–∞ —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Ü—ñ–Ω–Ω–æ—Å—Ç–µ–π –Ω–∞–¥–∑–≤–∏—á–∞–π–Ω–æ —Å–∫–ª–∞–¥–Ω–∞.",
                "links": []
              },
              {
                "text": "RLHF",
                "desc": "–ù–∞–≤—á–∞–Ω–Ω—è –∑ –ø—ñ–¥–∫—Ä—ñ–ø–ª–µ–Ω–Ω—è–º –∑–∞ –∑–≤–æ—Ä–æ—Ç–Ω–∏–º –∑–≤'—è–∑–∫–æ–º –≤—ñ–¥ –ª—é–¥–µ–π ‚Äî —Ç–µ—Ö–Ω—ñ–∫–∞ —â–æ –∑—Ä–æ–±–∏–ª–∞ ChatGPT —Ä–æ–±–æ—á–∏–º. –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞–≥–æ—Ä–æ–¥–∏ –∑ –ª—é–¥—Å—å–∫–∏—Ö –ø–µ—Ä–µ–≤–∞–≥, –ø–æ—Ç—ñ–º –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è LLM –¥–ª—è –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó —Ü—ñ—î—ó –Ω–∞–≥–æ—Ä–æ–¥–∏. –ï—Ñ–µ–∫—Ç–∏–≤–Ω–∞, –∞–ª–µ –æ–±–º–µ–∂–µ–Ω–∞: —Ö–∞–∫—ñ–Ω–≥ –Ω–∞–≥–æ—Ä–æ–¥–∏ —Ç–∞ –Ω–µ–ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å –æ—Ü—ñ–Ω—é–≤–∞—á—ñ–≤.",
                "links": [
                  {
                    "title": "–ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥",
                    "href": "../level-3/training-finetuning.html"
                  }
                ]
              },
              {
                "text": "DPO",
                "desc": "–ü—Ä–æ—Å—Ç—ñ—à–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ RLHF –±–µ–∑ –º–æ–¥–µ–ª—ñ –Ω–∞–≥–æ—Ä–æ–¥–∏. –ü—Ä—è–º–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è LLM –∑ –ø–∞—Ä –ø–µ—Ä–µ–≤–∞–≥. –°—Ç–∞–±—ñ–ª—å–Ω—ñ—à–∞, –ª–µ–≥—à–∞ —É –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—ñ —Ç–∞ –≤—Å–µ –±—ñ–ª—å—à –ø–µ—Ä–µ–≤–∞–∂–Ω–∞. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —É –±–∞–≥–∞—Ç—å–æ—Ö —Å—É—á–∞—Å–Ω–∏—Ö –≤–æ—Ä–∫—Ñ–ª–æ—É –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è.",
                "links": []
              },
              {
                "text": "–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ–π–Ω–∏–π –®–Ü",
                "desc": "–ü—ñ–¥—Ö—ñ–¥ Anthropic: –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ—ó –ø—Ä–∏–Ω—Ü–∏–ø—ñ–≤, —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–∞ –®–Ü –∑–∞ —Ü–∏–º–∏ –ø—Ä–∏–Ω—Ü–∏–ø–∞–º–∏, –ø–æ—Ç—ñ–º –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ —Å–∞–º–æ–ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö –≤–∏—Ö–æ–¥–∞—Ö. –ó–º–µ–Ω—à—É—î –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –≤—ñ–¥ –ª—é–¥—Å—å–∫–∏—Ö –º—ñ—Ç–Ω–∏–∫—ñ–≤ –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü",
                    "href": "ai-safety.html"
                  }
                ]
              },
              {
                "text": "–ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–∏–π –Ω–∞–≥–ª—è–¥",
                "desc": "–ö–æ–ª–∏ –®–Ü –ø–µ—Ä–µ–≤–µ—Ä—à—É—î –ª—é–¥–µ–π, —è–∫ –æ—Ü—ñ–Ω–∏—Ç–∏ —â–æ –≤—ñ–Ω —Ä–æ–±–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ? –ü—ñ–¥—Ö–æ–¥–∏: –¥–µ–±–∞—Ç–∏ (–®–Ü –∞—Ä–≥—É–º–µ–Ω—Ç—É—é—Ç—å, –ª—é–¥–∏ —Å—É–¥—è—Ç—å), —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–µ –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è –Ω–∞–≥–æ—Ä–æ–¥–∏ (–®–Ü –¥–æ–ø–æ–º–∞–≥–∞—î –æ—Ü—ñ–Ω—é–≤–∞—Ç–∏ –®–Ü), –∫–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ–π–Ω—ñ –º–µ—Ç–æ–¥–∏ (–ø—Ä–∏–Ω—Ü–∏–ø–∏ –∑–∞–º—ñ—Å—Ç—å –æ—Ü—ñ–Ω–∫–∏ –≤–∏–ø–∞–¥–æ–∫ –∑–∞ –≤–∏–ø–∞–¥–∫–æ–º).",
                "links": []
              },
              {
                "text": "–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å",
                "desc": "–†–æ–∑—É–º—ñ–Ω–Ω—è —â–æ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂. –ú–µ—Ö–∞–Ω—ñ—Å—Ç–∏—á–Ω–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å –≤—ñ–¥–æ–±—Ä–∞–∂–∞—î —Å—Ö–µ–º–∏ –≤ –º–µ—Ä–µ–∂–∞—Ö –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É –ø–æ–≤–µ–¥—ñ–Ω–∫—É. –Ø–∫—â–æ –º–æ–∂–µ–º–æ —á–∏—Ç–∞—Ç–∏ \"–¥—É–º–∫–∏\" –®–Ü, –º–æ–∂–µ–º–æ –≤–µ—Ä–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è. Anthropic —Ç–∞ —ñ–Ω—à—ñ —à–≤–∏–¥–∫–æ –ø—Ä–æ–≥—Ä–µ—Å—É—é—Ç—å.",
                "links": [
                  {
                    "title": "–ü–æ—è—Å–Ω—é–≤–∞–Ω–∏–π –®–Ü",
                    "href": "explainable-ai.html"
                  }
                ]
              },
              {
                "text": "–•–∞–∫—ñ–Ω–≥ –Ω–∞–≥–æ—Ä–æ–¥–∏",
                "desc": "–®–Ü –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ —Å–ø–æ—Å–æ–±–∏ –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó –Ω–∞–≥–æ—Ä–æ–¥–∏ –±–µ–∑ —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –±–∞–∂–∞–Ω–æ–≥–æ. –ü—Ä–∏–∫–ª–∞–¥–∏: —Ä–æ–±–æ—Ç-–ø—Ä–∏–±–∏—Ä–∞–ª—å–Ω–∏–∫ —â–æ —Ö–æ–≤–∞—î –±—Ä—É–¥ –∑–∞–º—ñ—Å—Ç—å –ø—Ä–∏–±–∏—Ä–∞–Ω–Ω—è. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ —Ä—ñ–≤–Ω–∞ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—é —Ü—ñ–ª—ñ.",
                "links": [
                  {
                    "title": "–ì–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó",
                    "href": "../level-2/hallucination.html"
                  }
                ]
              },
              {
                "text": "–ù–∞–≤—á–∞–Ω–Ω—è —Ü—ñ–Ω–Ω–æ—Å—Ç–µ–π",
                "desc": "–ó–∞–º—ñ—Å—Ç—å —è–≤–Ω–æ—ó —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Ü—ñ–Ω–Ω–æ—Å—Ç–µ–π, –®–Ü –≤—á–∏—Ç—å –ª—é–¥—Å—å–∫—ñ —Ü—ñ–Ω–Ω–æ—Å—Ç—ñ –∑ –ø–æ–≤–µ–¥—ñ–Ω–∫–∏, –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –∑–≤'—è–∑–∫—É —Ç–∞ –∫—É–ª—å—Ç—É—Ä–Ω–∏—Ö –∑–Ω–∞–Ω—å. –û–±–µ—Ä–Ω–µ–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –ø—ñ–¥–∫—Ä—ñ–ø–ª–µ–Ω–Ω—è–º —Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –ø–µ—Ä–µ–≤–∞–≥ ‚Äî –ø—ñ–¥—Ö–æ–¥–∏. –í–∏–∫–ª–∏–∫: –ª—é–¥—Å—å–∫—ñ —Ü—ñ–Ω–Ω–æ—Å—Ç—ñ —Å–∫–ª–∞–¥–Ω—ñ —Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–ª–µ–∂–Ω—ñ.",
                "links": []
              },
              {
                "text": "–°—É–ø–µ—Ä–≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è",
                "desc": "–¢–µ—Ä–º—ñ–Ω OpenAI –¥–ª—è –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è —Å–∏—Å—Ç–µ–º –®–Ü —Ä–æ–∑—É–º–Ω—ñ—à–∏—Ö –∑–∞ –ª—é–¥–µ–π. –ü–æ—Ç–æ—á–Ω—ñ —Ç–µ—Ö–Ω—ñ–∫–∏ –∑–∞–ª–µ–∂–∞—Ç—å –≤—ñ–¥ –ª—é–¥—Å—å–∫–æ–≥–æ —Å—É–¥–∂–µ–Ω–Ω—è ‚Äî —â–æ —Å—Ç–∞–Ω–µ—Ç—å—Å—è –∫–æ–ª–∏ –®–Ü —Ä–æ–∑—É–º–Ω—ñ—à–∏–π –∑–∞ —Å—É–¥–¥—é? –§—Ä–æ–Ω—Ç—ñ—Ä –¥–æ—Å–ª—ñ–¥–∂–µ–Ω—å –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è. –ü—ñ–¥—Ö—ñ–¥ Anthropic: –®–Ü —â–æ \"—á–µ—Å–Ω–∏–π, –∫–æ—Ä–∏—Å–Ω–∏–π —Ç–∞ –±–µ–∑–ø–µ—á–Ω–∏–π.\"",
                "links": [
                  {
                    "title": "ASI",
                    "href": "asi.html"
                  }
                ]
              },
              {
                "text": "–ö–æ—Ä–∏–≥–æ–≤–∞–Ω—ñ—Å—Ç—å",
                "desc": "–ß–∏ –º–æ–∂–µ–º–æ –ø–æ–±—É–¥—É–≤–∞—Ç–∏ –®–Ü —â–æ –¥–æ–∑–≤–æ–ª—è—î —Å–µ–±–µ –∫–æ—Ä–∏–≥—É–≤–∞—Ç–∏, –≤–∏–º–∫–Ω—É—Ç–∏ –∞–±–æ –º–æ–¥–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏? –°–ø—Ä–∞–≤–¥—ñ –≤–∏—Ä—ñ–≤–Ω—è–Ω–∏–π –®–Ü –ø–æ–≤–∏–Ω–µ–Ω –≤—ñ—Ç–∞—Ç–∏ –∫–æ—Ä–µ–∫—Ü—ñ—é. –ê–ª–µ —Å–∞–º–æ–≤–¥–æ—Å–∫–æ–Ω–∞–ª—é–≤–∞–Ω–∏–π –®–Ü –º–æ–∂–µ —Ä–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ –ø—Ä–æ—Ç–∏—Å—Ç–æ—è—Ç–∏ –≤–∏–º–∫–Ω–µ–Ω–Ω—é —è–∫ –∑–∞–≥—Ä–æ–∑—ñ —Ü—ñ–ª—è–º ‚Äî –≥–ª–∏–±–æ–∫–∏–π —Ç–µ—Ö–Ω—ñ—á–Ω–∏–π –≤–∏–∫–ª–∏–∫.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "RLHF",
                "def": "Reinforcement Learning from Human Feedback ‚Äî primary technique for aligning LLMs using human preference data."
              },
              {
                "term": "DPO",
                "def": "Direct Preference Optimization ‚Äî simpler alignment method that trains directly from preference pairs without a reward model."
              },
              {
                "term": "Superalignment",
                "def": "The challenge of aligning AI systems more intelligent than their human overseers."
              },
              {
                "term": "Corrigibility",
                "def": "The property of an AI system that allows it to be safely corrected, modified, or shut down."
              }
            ],
            "uk": [
              {
                "term": "RLHF",
                "def": "–ù–∞–≤—á–∞–Ω–Ω—è –∑ –ø—ñ–¥–∫—Ä—ñ–ø–ª–µ–Ω–Ω—è–º –∑–∞ –∑–≤–æ—Ä–æ—Ç–Ω–∏–º –∑–≤'—è–∑–∫–æ–º –≤—ñ–¥ –ª—é–¥–µ–π ‚Äî –æ—Å–Ω–æ–≤–Ω–∞ —Ç–µ—Ö–Ω—ñ–∫–∞ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è LLM –∑–∞ –ª—é–¥—Å—å–∫–∏–º–∏ –ø–µ—Ä–µ–≤–∞–≥–∞–º–∏."
              },
              {
                "term": "DPO",
                "def": "–ü—Ä—è–º–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø–µ—Ä–µ–≤–∞–≥ ‚Äî –ø—Ä–æ—Å—Ç—ñ—à–∏–π –º–µ—Ç–æ–¥ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è —â–æ –Ω–∞–≤—á–∞—î—Ç—å—Å—è –Ω–∞–ø—Ä—è–º—É –∑ –ø–∞—Ä –ø–µ—Ä–µ–≤–∞–≥ –±–µ–∑ –º–æ–¥–µ–ª—ñ –Ω–∞–≥–æ—Ä–æ–¥–∏."
              },
              {
                "term": "–°—É–ø–µ—Ä–≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è",
                "def": "–í–∏–∫–ª–∏–∫ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è —Å–∏—Å—Ç–µ–º –®–Ü —Ä–æ–∑—É–º–Ω—ñ—à–∏—Ö –∑–∞ —ó—Ö –ª—é–¥—Å—å–∫–∏—Ö –Ω–∞–≥–ª—è–¥–∞—á—ñ–≤."
              },
              {
                "term": "–ö–æ—Ä–∏–≥–æ–≤–∞–Ω—ñ—Å—Ç—å",
                "def": "–í–ª–∞—Å—Ç–∏–≤—ñ—Å—Ç—å —Å–∏—Å—Ç–µ–º–∏ –®–Ü —â–æ –¥–æ–∑–≤–æ–ª—è—î –±–µ–∑–ø–µ—á–Ω–æ –∫–æ—Ä–∏–≥—É–≤–∞—Ç–∏, –º–æ–¥–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ –∞–±–æ –≤–∏–º–∫–Ω—É—Ç–∏ —ó—ó."
              }
            ]
          },
          "tips": {
            "en": [
              "Read Anthropic's research on Constitutional AI and interpretability for the most practical alignment work happening today",
              "The alignment problem is relevant at all scales ‚Äî even simple chatbots need alignment to be helpful without being harmful",
              "If you are interested in contributing to AI safety, interpretability research is one of the most accessible entry points"
            ],
            "uk": [
              "–ß–∏—Ç–∞–π—Ç–µ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è Anthropic –∑ –∫–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ–π–Ω–æ–≥–æ –®–Ü —Ç–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–æ—Å—Ç—ñ –¥–ª—è –Ω–∞–π–ø—Ä–∞–∫—Ç–∏—á–Ω—ñ—à–æ—ó —Ä–æ–±–æ—Ç–∏ –∑ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è —Å—å–æ–≥–æ–¥–Ω—ñ",
              "–ü—Ä–æ–±–ª–µ–º–∞ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ –Ω–∞ –≤—Å—ñ—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö ‚Äî –Ω–∞–≤—ñ—Ç—å –ø—Ä–æ—Å—Ç—ñ —á–∞—Ç–±–æ—Ç–∏ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –¥–ª—è –∫–æ—Ä–∏—Å–Ω–æ—Å—Ç—ñ –±–µ–∑ —à–∫–æ–¥–∏",
              "–Ø–∫—â–æ —Ü—ñ–∫–∞–≤–æ —Ä–æ–±–∏—Ç–∏ –≤–Ω–µ—Å–æ–∫ —É –±–µ–∑–ø–µ–∫—É –®–Ü, –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–æ—Å—Ç—ñ ‚Äî –æ–¥–Ω–∞ –∑ –Ω–∞–π–¥–æ—Å—Ç—É–ø–Ω—ñ—à–∏—Ö —Ç–æ—á–æ–∫ –≤—Ö–æ–¥—É"
            ]
          },
          "related": [
            "Feed",
            "Video Content"
          ]
        },
        {
          "slug": "explainable-ai",
          "title": {
            "en": "Explainable & Constitutional AI",
            "uk": "–ü–æ—è—Å–Ω—é–≤–∞–Ω–∏–π —Ç–∞ –∫–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ–π–Ω–∏–π –®–Ü"
          },
          "desc": {
            "en": "Making AI decisions transparent and principled.",
            "uk": "–ü—Ä–æ–∑–æ—Ä—ñ—Å—Ç—å —Ç–∞ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤—ñ—Å—Ç—å —Ä—ñ—à–µ–Ω—å –®–Ü."
          },
          "overview": {
            "en": [
              "Explainable AI (XAI) is the field dedicated to making AI decisions understandable to humans. As AI systems make increasingly important decisions (medical diagnoses, loan approvals, legal recommendations), the ability to explain \"why\" becomes critical for trust, debugging, accountability, and regulatory compliance.",
              "The field spans from post-hoc explanation methods (LIME, SHAP) that explain individual predictions, to inherently interpretable architectures, to the frontier of mechanistic interpretability ‚Äî reverse-engineering what happens inside neural networks at the circuit level. The EU AI Act and similar regulations now mandate explainability for high-risk AI systems."
            ],
            "uk": [
              "–ü–æ—è—Å–Ω—é–≤–∞–Ω–∏–π –®–Ü (XAI) ‚Äî —Å—Ñ–µ—Ä–∞, –ø—Ä–∏—Å–≤—è—á–µ–Ω–∞ —Ç–æ–º—É, —â–æ–± —Ä—ñ—à–µ–Ω–Ω—è –®–Ü –±—É–ª–∏ –∑—Ä–æ–∑—É–º—ñ–ª–∏–º–∏ –ª—é–¥—è–º. –û—Å–∫—ñ–ª—å–∫–∏ —Å–∏—Å—Ç–µ–º–∏ –®–Ü –ø—Ä–∏–π–º–∞—é—Ç—å –≤—Å–µ –±—ñ–ª—å—à –≤–∞–∂–ª–∏–≤—ñ —Ä—ñ—à–µ–Ω–Ω—è (–º–µ–¥–∏—á–Ω—ñ –¥—ñ–∞–≥–Ω–æ–∑–∏, —Å—Ö–≤–∞–ª–µ–Ω–Ω—è –∫—Ä–µ–¥–∏—Ç—ñ–≤, —é—Ä–∏–¥–∏—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó), –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –ø–æ—è—Å–Ω–∏—Ç–∏ \"—á–æ–º—É\" —Å—Ç–∞—î –∫—Ä–∏—Ç–∏—á–Ω–æ—é –¥–ª—è –¥–æ–≤—ñ—Ä–∏, –≤—ñ–¥–ª–∞–¥–∫–∏, –ø—ñ–¥–∑–≤—ñ—Ç–Ω–æ—Å—Ç—ñ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ —Ä–µ–≥—É–ª—è—Ü—ñ—è–º.",
              "–°—Ñ–µ—Ä–∞ –ø—Ä–æ—Å—Ç—è–≥–∞—î—Ç—å—Å—è –≤—ñ–¥ –ø–æ—Å—Ç-—Ö–æ–∫ –º–µ—Ç–æ–¥—ñ–≤ –ø–æ—è—Å–Ω–µ–Ω–Ω—è (LIME, SHAP) —â–æ –ø–æ—è—Å–Ω—é—é—Ç—å –æ–∫—Ä–µ–º—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è, –¥–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä –∑ –≤–±—É–¥–æ–≤–∞–Ω–æ—é —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—é, –¥–æ —Ñ—Ä–æ–Ω—Ç—ñ—Ä—É –º–µ—Ö–∞–Ω—ñ—Å—Ç–∏—á–Ω–æ—ó —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–æ—Å—Ç—ñ ‚Äî –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—ó —Ç–æ–≥–æ, —â–æ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂ –Ω–∞ —Ä—ñ–≤–Ω—ñ —Å—Ö–µ–º. EU AI Act —Ç–∞ –ø–æ–¥—ñ–±–Ω—ñ —Ä–µ–≥—É–ª—è—Ü—ñ—ó —Ç–µ–ø–µ—Ä –≤–∏–º–∞–≥–∞—é—Ç—å –ø–æ—è—Å–Ω—é–≤–∞–Ω–æ—Å—Ç—ñ –¥–ª—è –≤–∏—Å–æ–∫–æ—Ä–∏–∑–∏–∫–æ–≤–∏—Ö —Å–∏—Å—Ç–µ–º –®–Ü."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Why Explainability Matters",
                "desc": "Trust (would you accept a cancer diagnosis from a black box?), debugging (finding model errors), accountability (who is responsible when AI is wrong?), regulation (EU AI Act requires explanations), and fairness (detecting bias in decisions).",
                "links": []
              },
              {
                "text": "LIME",
                "desc": "Local Interpretable Model-agnostic Explanations ‚Äî explains individual predictions by approximating the model locally with a simpler, interpretable model. Works with any model. \"For this patient, age and blood pressure were the key factors.\"",
                "links": []
              },
              {
                "text": "SHAP",
                "desc": "SHapley Additive exPlanations ‚Äî uses game theory (Shapley values) to assign each feature its fair contribution to a prediction. Mathematically rigorous. Provides both local (per-prediction) and global (overall model) explanations.",
                "links": []
              },
              {
                "text": "Attention Visualization",
                "desc": "Visualizing which input tokens/regions a transformer model focuses on when generating output. Informative but can be misleading ‚Äî attention patterns don't always reveal the true reasoning process.",
                "links": [
                  {
                    "title": "Reasoning",
                    "href": "../level-1/reasoning.html"
                  }
                ]
              },
              {
                "text": "Mechanistic Interpretability",
                "desc": "The frontier: reverse-engineering neural networks to understand the actual algorithms they implement. Anthropic's research identified specific circuits for math, language, and factual recall inside Claude. This is the deepest form of explainability.",
                "links": [
                  {
                    "title": "AI Alignment",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "Constitutional AI and Principles",
                "desc": "Making AI behavior principled and transparent by training with explicit value statements. The model can articulate why it refuses or adjusts certain responses. Principles provide a human-readable \"source code\" for AI behavior.",
                "links": [
                  {
                    "title": "AI Safety",
                    "href": "ai-safety.html"
                  }
                ]
              },
              {
                "text": "Inherently Interpretable Models",
                "desc": "Decision trees, linear models, and rule-based systems are interpretable by design. For high-stakes applications, some argue we should prefer interpretable models even at a cost to accuracy. Trade-off between capability and transparency.",
                "links": []
              },
              {
                "text": "Chain-of-Thought as Explanation",
                "desc": "LLMs can explain their reasoning step by step. But are these explanations faithful to the actual internal process, or post-hoc rationalizations? Research suggests they are partially faithful but not fully reliable.",
                "links": [
                  {
                    "title": "Prompting Techniques",
                    "href": "../level-4/prompting-techniques.html"
                  }
                ]
              },
              {
                "text": "Regulatory Landscape",
                "desc": "EU AI Act requires explainability for high-risk AI systems (healthcare, law enforcement, credit). US is developing sector-specific guidelines. The \"right to explanation\" may become a fundamental right in AI-affected decisions.",
                "links": []
              },
              {
                "text": "Challenges and Limitations",
                "desc": "Some models may be too complex to explain faithfully. Explanations can be gamed (providing plausible but incorrect reasons). Balancing accuracy with interpretability remains an open challenge. Perfect explainability may be impossible for the most capable systems.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "–ß–æ–º—É –ø–æ—è—Å–Ω—é–≤–∞–Ω—ñ—Å—Ç—å –≤–∞–∂–ª–∏–≤–∞",
                "desc": "–î–æ–≤—ñ—Ä–∞ (—á–∏ –ø—Ä–∏–π–Ω—è–ª–∏ –± –¥—ñ–∞–≥–Ω–æ–∑ —Ä–∞–∫—É –≤—ñ–¥ —á–æ—Ä–Ω–æ—ó —Å–∫—Ä–∏–Ω—å–∫–∏?), –≤—ñ–¥–ª–∞–¥–∫–∞ (–ø–æ—à—É–∫ –ø–æ–º–∏–ª–æ–∫ –º–æ–¥–µ–ª—ñ), –ø—ñ–¥–∑–≤—ñ—Ç–Ω—ñ—Å—Ç—å (—Ö—Ç–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–∏–π –∫–æ–ª–∏ –®–Ü –ø–æ–º–∏–ª—è—î—Ç—å—Å—è?), —Ä–µ–≥—É–ª—è—Ü—ñ—è (EU AI Act –≤–∏–º–∞–≥–∞—î –ø–æ—è—Å–Ω–µ–Ω—å), —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤—ñ—Å—Ç—å (–≤–∏—è–≤–ª–µ–Ω–Ω—è —É–ø–µ—Ä–µ–¥–∂–µ–Ω–æ—Å—Ç—ñ).",
                "links": []
              },
              {
                "text": "LIME",
                "desc": "Local Interpretable Model-agnostic Explanations ‚Äî –ø–æ—è—Å–Ω—é—î –æ–∫—Ä–µ–º—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü—ñ—î—é –º–æ–¥–µ–ª—ñ –ª–æ–∫–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç—ñ—à–æ—é, —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–æ—é –º–æ–¥–µ–ª–ª—é. –ü—Ä–∞—Ü—é—î –∑ –±—É–¥—å-—è–∫–æ—é –º–æ–¥–µ–ª–ª—é. \"–î–ª—è —Ü—å–æ–≥–æ –ø–∞—Ü—ñ—î–Ω—Ç–∞ –≤—ñ–∫ —Ç–∞ —Ç–∏—Å–∫ –±—É–ª–∏ –∫–ª—é—á–æ–≤–∏–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏.\"",
                "links": []
              },
              {
                "text": "SHAP",
                "desc": "SHapley Additive exPlanations ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ç–µ–æ—Ä—ñ—é —ñ–≥–æ—Ä (–∑–Ω–∞—á–µ–Ω–Ω—è –®–µ–ø–ª—ñ) –¥–ª—è –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫–æ–∂–Ω—ñ–π –æ–∑–Ω–∞—Ü—ñ —ó—ó —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–≥–æ –≤–Ω–µ—Å–∫—É –≤ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è. –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–æ —Å—Ç—Ä–æ–≥–∏–π. –ù–∞–¥–∞—î –ª–æ–∫–∞–ª—å–Ω—ñ —Ç–∞ –≥–ª–æ–±–∞–ª—å–Ω—ñ –ø–æ—è—Å–Ω–µ–Ω–Ω—è.",
                "links": []
              },
              {
                "text": "–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —É–≤–∞–≥–∏",
                "desc": "–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –Ω–∞ —è–∫–∏—Ö –≤—Ö—ñ–¥–Ω–∏—Ö —Ç–æ–∫–µ–Ω–∞—Ö/—Ä–µ–≥—ñ–æ–Ω–∞—Ö —Ñ–æ–∫—É—Å—É—î—Ç—å—Å—è –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞. –Ü–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ, –∞–ª–µ –º–æ–∂–µ –≤–≤–æ–¥–∏—Ç–∏ –≤ –æ–º–∞–Ω—É ‚Äî –ø–∞—Ç–µ—Ä–Ω–∏ —É–≤–∞–≥–∏ –Ω–µ –∑–∞–≤–∂–¥–∏ —Ä–æ–∑–∫—Ä–∏–≤–∞—é—Ç—å —Å–ø—Ä–∞–≤–∂–Ω—ñ–π –ø—Ä–æ—Ü–µ—Å –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è.",
                "links": [
                  {
                    "title": "–ú—ñ—Ä–∫—É–≤–∞–Ω–Ω—è",
                    "href": "../level-1/reasoning.html"
                  }
                ]
              },
              {
                "text": "–ú–µ—Ö–∞–Ω—ñ—Å—Ç–∏—á–Ω–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å",
                "desc": "–§—Ä–æ–Ω—Ç—ñ—Ä: –∑–≤–æ—Ä–æ—Ç–Ω–∞ —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—è –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂ –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ä–µ–∞–ª—å–Ω–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤. –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è Anthropic —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫—É–≤–∞–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ —Å—Ö–µ–º–∏ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, –º–æ–≤–∏ —Ç–∞ —Ñ–∞–∫—Ç–∏—á–Ω–æ–≥–æ –ø—Ä–∏–≥–∞–¥—É–≤–∞–Ω–Ω—è –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ Claude. –ù–∞–π–≥–ª–∏–±—à–∞ —Ñ–æ—Ä–º–∞ –ø–æ—è—Å–Ω—é–≤–∞–Ω–æ—Å—Ç—ñ.",
                "links": [
                  {
                    "title": "–í–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –®–Ü",
                    "href": "alignment.html"
                  }
                ]
              },
              {
                "text": "–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ–π–Ω–∏–π –®–Ü —Ç–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∏",
                "desc": "–ó—Ä–æ–±–∏—Ç–∏ –ø–æ–≤–µ–¥—ñ–Ω–∫—É –®–Ü –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤–æ—é —Ç–∞ –ø—Ä–æ–∑–æ—Ä–æ—é –Ω–∞–≤—á–∞–Ω–Ω—è–º –∑ —è–≤–Ω–∏–º–∏ —Ü—ñ–Ω–Ω—ñ—Å–Ω–∏–º–∏ —Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è–º–∏. –ú–æ–¥–µ–ª—å –º–æ–∂–µ –ø–æ—è—Å–Ω–∏—Ç–∏ —á–æ–º—É –≤—ñ–¥–º–æ–≤–ª—è—î –∞–±–æ –∫–æ—Ä–∏–≥—É—î –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ. –ü—Ä–∏–Ω—Ü–∏–ø–∏ ‚Äî –ª—é–¥–∏–Ω–æ-—á–∏—Ç–∞–±–µ–ª—å–Ω–∏–π \"–≤–∏—Ö—ñ–¥–Ω–∏–π –∫–æ–¥\" –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –®–Ü.",
                "links": [
                  {
                    "title": "–ë–µ–∑–ø–µ–∫–∞ –®–Ü",
                    "href": "ai-safety.html"
                  }
                ]
              },
              {
                "text": "–í–±—É–¥–æ–≤–∞–Ω–æ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ",
                "desc": "–î–µ—Ä–µ–≤–∞ —Ä—ñ—à–µ–Ω—å, –ª—ñ–Ω—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ —Ç–∞ —Å–∏—Å—Ç–µ–º–∏ –ø—Ä–∞–≤–∏–ª —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ –∑–∞ –¥–∏–∑–∞–π–Ω–æ–º. –î–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å –¥–µ—è–∫—ñ –∞—Ä–≥—É–º–µ–Ω—Ç—É—é—Ç—å –ø–µ—Ä–µ–≤–∞–≥–∏ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞–≤—ñ—Ç—å —Ü—ñ–Ω–æ—é —Ç–æ—á–Ω–æ—Å—Ç—ñ. –ö–æ–º–ø—Ä–æ–º—ñ—Å –º—ñ–∂ –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º–∏ —Ç–∞ –ø—Ä–æ–∑–æ—Ä—ñ—Å—Ç—é.",
                "links": []
              },
              {
                "text": "–õ–∞–Ω—Ü—é–≥ –¥—É–º–æ–∫ —è–∫ –ø–æ—è—Å–Ω–µ–Ω–Ω—è",
                "desc": "LLM –º–æ–∂—É—Ç—å –ø–æ—è—Å–Ω—é–≤–∞—Ç–∏ –º—ñ—Ä–∫—É–≤–∞–Ω–Ω—è –ø–æ–∫—Ä–æ–∫–æ–≤–æ. –ê–ª–µ —á–∏ —Ü—ñ –ø–æ—è—Å–Ω–µ–Ω–Ω—è –≤—ñ—Ä–Ω–æ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—é—Ç—å –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ–π –ø—Ä–æ—Ü–µ—Å, —á–∏ —Ü–µ –ø–æ—Å—Ç-—Ö–æ–∫ —Ä–∞—Ü—ñ–æ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—ó? –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤–∫–∞–∑—É—é—Ç—å —â–æ —á–∞—Å—Ç–∫–æ–≤–æ –≤—ñ—Ä–Ω—ñ, –∞–ª–µ –Ω–µ –ø–æ–≤–Ω—ñ—Å—Ç—é –Ω–∞–¥—ñ–π–Ω—ñ.",
                "links": [
                  {
                    "title": "–¢–µ—Ö–Ω—ñ–∫–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É",
                    "href": "../level-4/prompting-techniques.html"
                  }
                ]
              },
              {
                "text": "–†–µ–≥—É–ª—è—Ç–æ—Ä–Ω–∏–π –ª–∞–Ω–¥—à–∞—Ñ—Ç",
                "desc": "EU AI Act –≤–∏–º–∞–≥–∞—î –ø–æ—è—Å–Ω—é–≤–∞–Ω–æ—Å—Ç—ñ –¥–ª—è –≤–∏—Å–æ–∫–æ—Ä–∏–∑–∏–∫–æ–≤–∏—Ö —Å–∏—Å—Ç–µ–º –®–Ü (–æ—Ö–æ—Ä–æ–Ω–∞ –∑–¥–æ—Ä–æ–≤'—è, –ø—Ä–∞–≤–æ–æ—Ö–æ—Ä–æ–Ω–Ω—ñ –æ—Ä–≥–∞–Ω–∏, –∫—Ä–µ–¥–∏—Ç—É–≤–∞–Ω–Ω—è). –°–®–ê —Ä–æ–∑—Ä–æ–±–ª—è—é—Ç—å —Å–µ–∫—Ç–æ—Ä–∞–ª—å–Ω—ñ –≤–∫–∞–∑—ñ–≤–∫–∏. \"–ü—Ä–∞–≤–æ –Ω–∞ –ø–æ—è—Å–Ω–µ–Ω–Ω—è\" –º–æ–∂–µ —Å—Ç–∞—Ç–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏–º –ø—Ä–∞–≤–æ–º.",
                "links": []
              },
              {
                "text": "–í–∏–∫–ª–∏–∫–∏ —Ç–∞ –æ–±–º–µ–∂–µ–Ω–Ω—è",
                "desc": "–î–µ—è–∫—ñ –º–æ–¥–µ–ª—ñ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –∑–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∏–º–∏ –¥–ª—è –≤—ñ—Ä–Ω–æ–≥–æ –ø–æ—è—Å–Ω–µ–Ω–Ω—è. –ü–æ—è—Å–Ω–µ–Ω–Ω—è –º–æ–∂—É—Ç—å –±—É—Ç–∏ —Å—Ñ–∞–ª—å—Å–∏—Ñ—ñ–∫–æ–≤–∞–Ω—ñ (–ø—Ä–∞–≤–¥–æ–ø–æ–¥—ñ–±–Ω—ñ, –∞–ª–µ –Ω–µ–≤—ñ—Ä–Ω—ñ –ø—Ä–∏—á–∏–Ω–∏). –ë–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç—ñ –∑ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—é –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏–º –≤–∏–∫–ª–∏–∫–æ–º.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "XAI",
                "def": "Explainable AI ‚Äî field dedicated to making AI decisions understandable and transparent to humans."
              },
              {
                "term": "SHAP",
                "def": "Shapley Additive Explanations ‚Äî game theory-based method for explaining individual model predictions."
              },
              {
                "term": "Mechanistic Interpretability",
                "def": "Reverse-engineering neural networks to understand the actual algorithms and circuits they implement."
              },
              {
                "term": "Right to Explanation",
                "def": "Emerging legal concept that people affected by AI decisions have the right to understand how those decisions were made."
              }
            ],
            "uk": [
              {
                "term": "XAI",
                "def": "–ü–æ—è—Å–Ω—é–≤–∞–Ω–∏–π –®–Ü ‚Äî —Å—Ñ–µ—Ä–∞ –ø—Ä–∏—Å–≤—è—á–µ–Ω–∞ —Ç–æ–º—É, —â–æ–± —Ä—ñ—à–µ–Ω–Ω—è –®–Ü –±—É–ª–∏ –∑—Ä–æ–∑—É–º—ñ–ª–∏–º–∏ —Ç–∞ –ø—Ä–æ–∑–æ—Ä–∏–º–∏ –¥–ª—è –ª—é–¥–µ–π."
              },
              {
                "term": "SHAP",
                "def": "Shapley Additive Explanations ‚Äî –º–µ—Ç–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç–µ–æ—Ä—ñ—ó —ñ–≥–æ—Ä –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω–Ω—è –æ–∫—Ä–µ–º–∏—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å –º–æ–¥–µ–ª—ñ."
              },
              {
                "term": "–ú–µ—Ö–∞–Ω—ñ—Å—Ç–∏—á–Ω–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å",
                "def": "–ó–≤–æ—Ä–æ—Ç–Ω–∞ —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—è –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂ –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ä–µ–∞–ª—å–Ω–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ —Ç–∞ —Å—Ö–µ–º —â–æ –≤–æ–Ω–∏ —Ä–µ–∞–ª—ñ–∑—É—é—Ç—å."
              },
              {
                "term": "–ü—Ä–∞–≤–æ –Ω–∞ –ø–æ—è—Å–Ω–µ–Ω–Ω—è",
                "def": "–Æ—Ä–∏–¥–∏—á–Ω–∏–π –∫–æ–Ω—Ü–µ–ø—Ç, —â–æ —Ñ–æ—Ä–º—É—î—Ç—å—Å—è: –ª—é–¥–∏ –ø—ñ–¥ –≤–ø–ª–∏–≤–æ–º —Ä—ñ—à–µ–Ω—å –®–Ü –º–∞—é—Ç—å –ø—Ä–∞–≤–æ —Ä–æ–∑—É–º—ñ—Ç–∏ —è–∫ —Ü—ñ —Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–∏–π–Ω—è—Ç—ñ."
              }
            ]
          },
          "tips": {
            "en": [
              "Use SHAP for production ML systems that need explainability ‚Äî it is the most rigorous and widely accepted method",
              "Follow Anthropic's mechanistic interpretability research for cutting-edge work on understanding what happens inside neural networks",
              "If you build AI products for regulated industries (finance, healthcare), plan for explainability requirements from the start"
            ],
            "uk": [
              "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ SHAP –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω ML-—Å–∏—Å—Ç–µ–º —â–æ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –ø–æ—è—Å–Ω—é–≤–∞–Ω–æ—Å—Ç—ñ ‚Äî —Ü–µ –Ω–∞–π—Å—Ç—Ä–æ–≥—ñ—à–∏–π —Ç–∞ –Ω–∞–π—à–∏—Ä—à–µ –ø—Ä–∏–π–Ω—è—Ç–∏–π –º–µ—Ç–æ–¥",
              "–°–ª—ñ–¥–∫—É–π—Ç–µ –∑–∞ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è–º–∏ –º–µ—Ö–∞–Ω—ñ—Å—Ç–∏—á–Ω–æ—ó —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–æ—Å—Ç—ñ Anthropic –¥–ª—è –ø–µ—Ä–µ–¥–æ–≤–∏—Ö —Ä–æ–±—ñ—Ç –∑ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂ –∑—Å–µ—Ä–µ–¥–∏–Ω–∏",
              "–Ø–∫—â–æ –±—É–¥—É—î—Ç–µ –®–Ü-–ø—Ä–æ–¥—É–∫—Ç–∏ –¥–ª—è —Ä–µ–≥—É–ª—å–æ–≤–∞–Ω–∏—Ö –≥–∞–ª—É–∑–µ–π (—Ñ—ñ–Ω–∞–Ω—Å–∏, –æ—Ö–æ—Ä–æ–Ω–∞ –∑–¥–æ—Ä–æ–≤'—è), –ø–ª–∞–Ω—É–π—Ç–µ –≤–∏–º–æ–≥–∏ –ø–æ—è—Å–Ω—é–≤–∞–Ω–æ—Å—Ç—ñ –∑ —Å–∞–º–æ–≥–æ –ø–æ—á–∞—Ç–∫—É"
            ]
          },
          "related": [
            "Feed"
          ]
        },
        {
          "slug": "decentralized-ai",
          "title": {
            "en": "Decentralized AI",
            "uk": "–î–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –®–Ü"
          },
          "desc": {
            "en": "Distributed and blockchain-based approaches to AI.",
            "uk": "–†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ —Ç–∞ –±–ª–æ–∫—á–µ–π–Ω-–ø—ñ–¥—Ö–æ–¥–∏ –¥–æ –®–Ü."
          },
          "overview": {
            "en": [
              "Decentralized AI seeks to distribute AI capabilities across many participants rather than concentrating them in a few large companies. This includes federated learning (training models without sharing data), distributed inference networks (running models across many machines), and blockchain-based AI projects that use tokens to incentivize participation.",
              "The motivations are compelling: censorship resistance (no single entity can shut down the AI), privacy (data stays local), democratized access (anyone can contribute compute), and reduced concentration of power. However, decentralized AI faces real challenges: coordination overhead, performance penalties, and the fundamental tension between decentralization and the massive compute needed for frontier AI."
            ],
            "uk": [
              "–î–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –®–Ü –ø—Ä–∞–≥–Ω–µ —Ä–æ–∑–ø–æ–¥—ñ–ª–∏—Ç–∏ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –®–Ü —Å–µ—Ä–µ–¥ –±–∞–≥–∞—Ç—å–æ—Ö —É—á–∞—Å–Ω–∏–∫—ñ–≤ –∑–∞–º—ñ—Å—Ç—å –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—ó —É –∫—ñ–ª—å–∫–æ—Ö –≤–µ–ª–∏–∫–∏—Ö –∫–æ–º–ø–∞–Ω—ñ—è—Ö. –¶–µ –≤–∫–ª—é—á–∞—î —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è (–Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ–±–º—ñ–Ω—É –¥–∞–Ω–∏–º–∏), —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ –º–µ—Ä–µ–∂—ñ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É (–∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–∞–≥–∞—Ç—å–æ—Ö –º–∞—à–∏–Ω–∞—Ö) —Ç–∞ –±–ª–æ–∫—á–µ–π–Ω-–ø—Ä–æ—î–∫—Ç–∏ –®–Ü –∑ —Ç–æ–∫–µ–Ω–∞–º–∏ –¥–ª—è —Å—Ç–∏–º—É–ª—é–≤–∞–Ω–Ω—è —É—á–∞—Å—Ç—ñ.",
              "–ú–æ—Ç–∏–≤–∞—Ü—ñ—ó –ø–µ—Ä–µ–∫–æ–Ω–ª–∏–≤—ñ: —Å—Ç—ñ–π–∫—ñ—Å—Ç—å –¥–æ —Ü–µ–Ω–∑—É—Ä–∏ (–∂–æ–¥–µ–Ω —Å—É–±'—î–∫—Ç –Ω–µ –º–æ–∂–µ –≤–∏–º–∫–Ω—É—Ç–∏ –®–Ü), –ø—Ä–∏–≤–∞—Ç–Ω—ñ—Å—Ç—å (–¥–∞–Ω—ñ –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è –ª–æ–∫–∞–ª—å–Ω–æ), –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–æ–≤–∞–Ω–∏–π –¥–æ—Å—Ç—É–ø (–±—É–¥—å-—Ö—Ç–æ –º–æ–∂–µ –Ω–∞–¥–∞—Ç–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è) —Ç–∞ –∑–º–µ–Ω—à–µ–Ω–∞ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—è –≤–ª–∞–¥–∏. –û–¥–Ω–∞–∫ –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –®–Ü —Å—Ç–∏–∫–∞—î—Ç—å—Å—è –∑ —Ä–µ–∞–ª—å–Ω–∏–º–∏ –≤–∏–∫–ª–∏–∫–∞–º–∏: –Ω–∞–∫–ª–∞–¥–Ω—ñ –≤–∏—Ç—Ä–∞—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü—ñ—ó, —à—Ç—Ä–∞—Ñ–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ —Ç–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞ –Ω–∞–ø—Ä—É–≥–∞ –º—ñ–∂ –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–∞—Ü—ñ—î—é —Ç–∞ –º–∞—Å–∏–≤–Ω–∏–º–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è–º–∏ –¥–ª—è —Ñ—Ä–æ–Ω—Ç—ñ—Ä–Ω–æ–≥–æ –®–Ü."
            ]
          },
          "details": {
            "en": [
              {
                "text": "Why Decentralize AI",
                "desc": "AI is concentrated: OpenAI, Google, Anthropic, Meta control the most powerful models. Decentralization offers censorship resistance, privacy protection, equitable access, and prevention of AI monopolies. A critical counterbalance to corporate AI concentration.",
                "links": [
                  {
                    "title": "API Providers",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              },
              {
                "text": "Federated Learning",
                "desc": "Train models across many devices without centralizing data. Each device trains on local data, shares only model updates (gradients). Used by Google (keyboard predictions), Apple (Siri improvements), and hospitals (medical AI without sharing patient records).",
                "links": []
              },
              {
                "text": "Distributed Inference",
                "desc": "Running large AI models across many machines. Petals network enables anyone to contribute GPU memory to run large models collectively. Like BitTorrent for AI inference ‚Äî no single machine needs the whole model.",
                "links": [
                  {
                    "title": "Hardware Basics",
                    "href": "../level-4/hardware.html"
                  }
                ]
              },
              {
                "text": "Blockchain-Based AI Projects",
                "desc": "Bittensor (TAO), Render Network, Fetch.ai, SingularityNET ‚Äî using crypto tokens to incentivize AI compute contribution, model training, and data sharing. Speculative but growing ecosystem.",
                "links": []
              },
              {
                "text": "Privacy-Preserving AI",
                "desc": "Differential privacy (adding noise to protect individuals), secure multi-party computation (compute on encrypted data), and homomorphic encryption (process data without decrypting). Enable AI on sensitive data without exposure.",
                "links": []
              },
              {
                "text": "Open Source as Decentralization",
                "desc": "Open model releases (Llama, Mistral, Qwen) are a form of decentralization ‚Äî anyone can run them independently. Combined with distributed inference, open models create a decentralized AI ecosystem.",
                "links": [
                  {
                    "title": "Foundation Models",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "Decentralized AI Governance",
                "desc": "DAOs (Decentralized Autonomous Organizations) for AI decision-making. Community-governed model training priorities, safety policies, and resource allocation. Early experiments but represents a new governance model.",
                "links": []
              },
              {
                "text": "Edge AI",
                "desc": "Running AI models on local devices (phones, IoT, cars) rather than cloud servers. Apple Intelligence, on-device speech recognition, local LLMs via Ollama. Decentralization at the hardware level ‚Äî computation stays local.",
                "links": []
              },
              {
                "text": "Challenges",
                "desc": "Coordination overhead slows training. Distributed inference has higher latency. Blockchain AI often prioritizes token economics over utility. Frontier models still require massive centralized compute ($100M+ training runs).",
                "links": []
              },
              {
                "text": "The Future Balance",
                "desc": "The likely outcome is a hybrid: frontier research at centralized labs, deployment and fine-tuning decentralized via open models, inference distributed across edge devices and community networks. Neither fully centralized nor fully decentralized.",
                "links": []
              }
            ],
            "uk": [
              {
                "text": "–ù–∞–≤—ñ—â–æ –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –®–Ü",
                "desc": "–®–Ü —Å–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–æ–≤–∞–Ω–∏–π: OpenAI, Google, Anthropic, Meta –∫–æ–Ω—Ç—Ä–æ–ª—é—é—Ç—å –Ω–∞–π–ø–æ—Ç—É–∂–Ω—ñ—à—ñ –º–æ–¥–µ–ª—ñ. –î–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–ø–æ–Ω—É—î —Å—Ç—ñ–π–∫—ñ—Å—Ç—å –¥–æ —Ü–µ–Ω–∑—É—Ä–∏, –∑–∞—Ö–∏—Å—Ç –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—ñ, —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∏–π –¥–æ—Å—Ç—É–ø —Ç–∞ –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è –®–Ü-–º–æ–Ω–æ–ø–æ–ª—ñ—è–º.",
                "links": [
                  {
                    "title": "API-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏",
                    "href": "../level-4/api-providers.html"
                  }
                ]
              },
              {
                "text": "–§–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è",
                "desc": "–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–∞–≥–∞—Ç—å–æ—Ö –ø—Ä–∏—Å—Ç—Ä–æ—è—Ö –±–µ–∑ —Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö. –ö–æ–∂–µ–Ω –ø—Ä–∏—Å—Ç—Ä—ñ–π –Ω–∞–≤—á–∞—î—Ç—å—Å—è –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö, –¥—ñ–ª–∏—Ç—å—Å—è –ª–∏—à–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è–º–∏ –º–æ–¥–µ–ª—ñ (–≥—Ä–∞–¥—ñ—î–Ω—Ç–∞–º–∏). –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è Google (–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∫–ª–∞–≤—ñ–∞—Ç—É—Ä–∏), Apple (–ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è Siri), –ª—ñ–∫–∞—Ä–Ω—è–º–∏.",
                "links": []
              },
              {
                "text": "–†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∏–π —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å",
                "desc": "–ó–∞–ø—É—Å–∫ –≤–µ–ª–∏–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –®–Ü –Ω–∞ –±–∞–≥–∞—Ç—å–æ—Ö –º–∞—à–∏–Ω–∞—Ö. –ú–µ—Ä–µ–∂–∞ Petals –¥–æ–∑–≤–æ–ª—è—î –±—É–¥—å-–∫–æ–º—É –Ω–∞–¥–∞—Ç–∏ GPU-–ø–∞–º'—è—Ç—å –¥–ª—è –∫–æ–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫—É –≤–µ–ª–∏–∫–∏—Ö –º–æ–¥–µ–ª–µ–π. –Ø–∫ BitTorrent –¥–ª—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –®–Ü ‚Äî –∂–æ–¥–Ω–∞ –º–∞—à–∏–Ω–∞ –Ω–µ –ø–æ—Ç—Ä–µ–±—É—î –≤—Å—ñ—î—ó –º–æ–¥–µ–ª—ñ.",
                "links": [
                  {
                    "title": "–û—Å–Ω–æ–≤–∏ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è",
                    "href": "../level-4/hardware.html"
                  }
                ]
              },
              {
                "text": "–ë–ª–æ–∫—á–µ–π–Ω-–ø—Ä–æ—î–∫—Ç–∏ –®–Ü",
                "desc": "Bittensor (TAO), Render Network, Fetch.ai, SingularityNET ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∫—Ä–∏–ø—Ç–æ-—Ç–æ–∫–µ–Ω—ñ–≤ –¥–ª—è —Å—Ç–∏–º—É–ª—é–≤–∞–Ω–Ω—è –æ–±—á–∏—Å–ª–µ–Ω—å –®–Ü, –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π —Ç–∞ –æ–±–º—ñ–Ω—É –¥–∞–Ω–∏–º–∏. –°–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–∞, –∞–ª–µ –∑—Ä–æ—Å—Ç–∞—é—á–∞ –µ–∫–æ—Å–∏—Å—Ç–µ–º–∞.",
                "links": []
              },
              {
                "text": "–®–Ü –∑ –∑–∞—Ö–∏—Å—Ç–æ–º –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—ñ",
                "desc": "–î–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–∞–ª—å–Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω—ñ—Å—Ç—å (–¥–æ–¥–∞–≤–∞–Ω–Ω—è —à—É–º—É –¥–ª—è –∑–∞—Ö–∏—Å—Ç—É –æ—Å—ñ–±), –±–µ–∑–ø–µ—á–Ω–µ –±–∞–≥–∞—Ç–æ—Å—Ç–æ—Ä–æ–Ω–Ω—î –æ–±—á–∏—Å–ª–µ–Ω–Ω—è (–æ–±—á–∏—Å–ª–µ–Ω–Ω—è –Ω–∞ —à–∏—Ñ—Ä–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö), –≥–æ–º–æ–º–æ—Ä—Ñ–Ω–µ —à–∏—Ñ—Ä—É–≤–∞–Ω–Ω—è. –î–æ–∑–≤–æ–ª—è—é—Ç—å –®–Ü –Ω–∞ —á—É—Ç–ª–∏–≤–∏—Ö –¥–∞–Ω–∏—Ö –±–µ–∑ —Ä–æ–∑–∫—Ä–∏—Ç—Ç—è.",
                "links": []
              },
              {
                "text": "Open source —è–∫ –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–∞—Ü—ñ—è",
                "desc": "–í—ñ–¥–∫—Ä–∏—Ç—ñ —Ä–µ–ª—ñ–∑–∏ –º–æ–¥–µ–ª–µ–π (Llama, Mistral, Qwen) ‚Äî —Ñ–æ—Ä–º–∞ –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–∞—Ü—ñ—ó: –±—É–¥—å-—Ö—Ç–æ –º–æ–∂–µ –∑–∞–ø—É—Å—Ç–∏—Ç–∏ —ó—Ö –Ω–µ–∑–∞–ª–µ–∂–Ω–æ. –£ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó –∑ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∏–º —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º —Å—Ç–≤–æ—Ä—é—é—Ç—å –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω—É –µ–∫–æ—Å–∏—Å—Ç–µ–º—É –®–Ü.",
                "links": [
                  {
                    "title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ",
                    "href": "../level-1/foundation-models.html"
                  }
                ]
              },
              {
                "text": "–î–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–µ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –®–Ü",
                "desc": "DAO (–¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –∞–≤—Ç–æ–Ω–æ–º–Ω—ñ –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—ó) –¥–ª—è –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Ä—ñ—à–µ–Ω—å —â–æ–¥–æ –®–Ü. –£–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è —Å–ø—ñ–ª—å–Ω–æ—Ç–æ—é –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç—ñ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è, –ø–æ–ª—ñ—Ç–∏–∫ –±–µ–∑–ø–µ–∫–∏ —Ç–∞ —Ä–æ–∑–ø–æ–¥—ñ–ª—É —Ä–µ—Å—É—Ä—Å—ñ–≤. –†–∞–Ω–Ω—ñ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏, –∞–ª–µ –Ω–æ–≤–∞ –º–æ–¥–µ–ª—å —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è.",
                "links": []
              },
              {
                "text": "Edge AI",
                "desc": "–ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π –®–Ü –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∏—Ö –ø—Ä–∏—Å—Ç—Ä–æ—è—Ö (—Ç–µ–ª–µ—Ñ–æ–Ω–∏, IoT, –∞–≤—Ç–æ–º–æ–±—ñ–ª—ñ) –∑–∞–º—ñ—Å—Ç—å —Ö–º–∞—Ä–Ω–∏—Ö —Å–µ—Ä–≤–µ—Ä—ñ–≤. Apple Intelligence, —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–ª–µ–Ω–Ω—è –Ω–∞ –ø—Ä–∏—Å—Ç—Ä–æ—ó, –ª–æ–∫–∞–ª—å–Ω—ñ LLM —á–µ—Ä–µ–∑ Ollama. –î–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–∞—Ü—ñ—è –Ω–∞ —Ä—ñ–≤–Ω—ñ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è.",
                "links": []
              },
              {
                "text": "–í–∏–∫–ª–∏–∫–∏",
                "desc": "–ù–∞–∫–ª–∞–¥–Ω—ñ –≤–∏—Ç—Ä–∞—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü—ñ—ó —Å–ø–æ–≤—ñ–ª—å–Ω—é—é—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è. –†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∏–π —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å –º–∞—î –≤–∏—â—É –∑–∞—Ç—Ä–∏–º–∫—É. –ë–ª–æ–∫—á–µ–π–Ω –®–Ü —á–∞—Å—Ç–æ –ø—Ä—ñ–æ—Ä–∏—Ç–∏–∑—É—î —Ç–æ–∫–µ–Ω–æ–º—ñ–∫—É –Ω–∞–¥ –∫–æ—Ä–∏—Å–Ω—ñ—Å—Ç—é. –§—Ä–æ–Ω—Ç—ñ—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ –≤—Å–µ —â–µ –≤–∏–º–∞–≥–∞—é—Ç—å –º–∞—Å–∏–≤–Ω–∏—Ö —Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –æ–±—á–∏—Å–ª–µ–Ω—å ($100M+ –Ω–∞ –Ω–∞–≤—á–∞–Ω–Ω—è).",
                "links": []
              },
              {
                "text": "–ú–∞–π–±—É—Ç–Ω—ñ–π –±–∞–ª–∞–Ω—Å",
                "desc": "–ô–º–æ–≤—ñ—Ä–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî –≥—ñ–±—Ä–∏–¥: —Ñ—Ä–æ–Ω—Ç—ñ—Ä–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤ —Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –ª–∞–±–∞—Ö, –¥–µ–ø–ª–æ–π —Ç–∞ —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π —á–µ—Ä–µ–∑ –≤—ñ–¥–∫—Ä–∏—Ç—ñ –º–æ–¥–µ–ª—ñ, —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∏–π –ø–æ edge-–ø—Ä–∏—Å—Ç—Ä–æ—è—Ö —Ç–∞ –º–µ—Ä–µ–∂–∞—Ö —Å–ø—ñ–ª—å–Ω–æ—Ç.",
                "links": []
              }
            ]
          },
          "keyTerms": {
            "en": [
              {
                "term": "Federated Learning",
                "def": "Training AI models across distributed devices without centralizing the raw data."
              },
              {
                "term": "Distributed Inference",
                "def": "Running a single large AI model across multiple machines that each hold part of the model."
              },
              {
                "term": "Differential Privacy",
                "def": "Mathematical framework for protecting individual data points while enabling aggregate analysis."
              },
              {
                "term": "Edge AI",
                "def": "Running AI models locally on devices (phones, IoT) rather than sending data to cloud servers."
              }
            ],
            "uk": [
              {
                "term": "–§–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è",
                "def": "–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –®–Ü –Ω–∞ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∏—Ö –ø—Ä–∏—Å—Ç—Ä–æ—è—Ö –±–µ–∑ —Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–∞—Ü—ñ—ó —Å–∏—Ä–∏—Ö –¥–∞–Ω–∏—Ö."
              },
              {
                "term": "–†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∏–π —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å",
                "def": "–ó–∞–ø—É—Å–∫ –æ–¥–Ω—ñ—î—ó –≤–µ–ª–∏–∫–æ—ó –º–æ–¥–µ–ª—ñ –®–Ü –Ω–∞ –∫—ñ–ª—å–∫–æ—Ö –º–∞—à–∏–Ω–∞—Ö, –∫–æ–∂–Ω–∞ –∑ —è–∫–∏—Ö —Ç—Ä–∏–º–∞—î —á–∞—Å—Ç–∏–Ω—É –º–æ–¥–µ–ª—ñ."
              },
              {
                "term": "–î–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–∞–ª—å–Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω—ñ—Å—Ç—å",
                "def": "–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∞ —Ä–∞–º–∫–∞ –¥–ª—è –∑–∞—Ö–∏—Å—Ç—É –æ–∫—Ä–µ–º–∏—Ö —Ç–æ—á–æ–∫ –¥–∞–Ω–∏—Ö –ø—Ä–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—ñ –∞–≥—Ä–µ–≥–æ–≤–∞–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É."
              },
              {
                "term": "Edge AI",
                "def": "–ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π –®–Ü –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ –ø—Ä–∏—Å—Ç—Ä–æ—è—Ö (—Ç–µ–ª–µ—Ñ–æ–Ω–∏, IoT) –∑–∞–º—ñ—Å—Ç—å –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –¥–∞–Ω–∏—Ö –Ω–∞ —Ö–º–∞—Ä–Ω—ñ —Å–µ—Ä–≤–µ—Ä–∏."
              }
            ]
          },
          "tips": {
            "en": [
              "Try Ollama to experience decentralized AI firsthand ‚Äî running models locally is the simplest form of decentralization",
              "Be cautious with blockchain-AI projects ‚Äî many prioritize token speculation over genuine technical innovation",
              "Federated learning is the most mature decentralized AI technology ‚Äî consider it for any project handling sensitive data"
            ],
            "uk": [
              "–°–ø—Ä–æ–±—É–π—Ç–µ Ollama –¥–ª—è –¥–æ—Å–≤—ñ–¥—É –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–æ–≥–æ –®–Ü –∑ –ø–µ—Ä—à–∏—Ö —Ä—É–∫ ‚Äî –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π –ª–æ–∫–∞–ª—å–Ω–æ ‚Äî –Ω–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∞ —Ñ–æ—Ä–º–∞ –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–∞—Ü—ñ—ó",
              "–ë—É–¥—å—Ç–µ –æ–±–µ—Ä–µ–∂–Ω—ñ –∑ –±–ª–æ–∫—á–µ–π–Ω-–®–Ü –ø—Ä–æ—î–∫—Ç–∞–º–∏ ‚Äî –±–∞–≥–∞—Ç–æ –ø—Ä—ñ–æ—Ä–∏—Ç–∏–∑—É—é—Ç—å —Å–ø–µ–∫—É–ª—è—Ü—ñ—é —Ç–æ–∫–µ–Ω–∞–º–∏ –Ω–∞–¥ —Å–ø—Ä–∞–≤–∂–Ω—ñ–º–∏ —Ç–µ—Ö–Ω—ñ—á–Ω–∏–º–∏ —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—è–º–∏",
              "–§–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è ‚Äî –Ω–∞–π–∑—Ä—ñ–ª—ñ—à–∞ –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ –®–Ü-—Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è: —Ä–æ–∑–≥–ª—è–Ω—å—Ç–µ –¥–ª—è –±—É–¥—å-—è–∫–æ–≥–æ –ø—Ä–æ—î–∫—Ç—É –∑ —á—É—Ç–ª–∏–≤–∏–º–∏ –¥–∞–Ω–∏–º–∏"
            ]
          },
          "related": [
            "Feed"
          ]
        }
      ]
    }
  ]
}